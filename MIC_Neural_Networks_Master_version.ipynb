{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "%matplotlib inline\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scripts stored the outputs as dictionaries.\n",
    "all_results = []\n",
    "for f in os.listdir('.'):\n",
    "    if '.data' in f:\n",
    "        with open(f, 'r') as g:\n",
    "            all_results.append(ast.literal_eval(g.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize units of MIC\n",
    "def standardize_to_uM(concentration, unit, sequence):\n",
    "    concentration = concentration.replace(' ', '')\n",
    "    try:\n",
    "        concentration = float(concentration)\n",
    "    except:\n",
    "        return None\n",
    "    if unit == 'uM' or unit == u'\\xb5M' or unit == u'uM)':\n",
    "        return concentration\n",
    "    elif unit == 'ug/ml' or unit == u'\\xb5g/ml' or unit == u'ug/ml)':\n",
    "        try:\n",
    "            molWt = ProteinAnalysis(sequence).molecular_weight()\n",
    "        except ValueError:\n",
    "            return None\n",
    "        return concentration * 1000/molWt\n",
    "    elif unit == 'nmol/g' or unit == 'pmol/mg':\n",
    "        #1g, at density of 1g/mL, is 1mL, so nmol/g is nmol/mL = umol/L = uM yay!\n",
    "        return concentration\n",
    "    else:\n",
    "        # print 'Unit not recognized: ' + unit\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter an element of a result dictionary into df-ready row\n",
    "def convert_result_to_rows(sequence, result):\n",
    "    rows = []\n",
    "    if 'bacteria' not in result:\n",
    "        return rows\n",
    "    for bacterium, strain in result['bacteria']:\n",
    "        \n",
    "        rows.append({\n",
    "            'bacterium': bacterium,\n",
    "            'strain': strain,\n",
    "            'sequence': sequence.upper(),\n",
    "            'url_source': result['url_sources'][0],\n",
    "            'value': standardize_to_uM(\n",
    "                result['bacteria'][(bacterium, strain)]['value'],\n",
    "                result['bacteria'][(bacterium, strain)]['unit'],\n",
    "                sequence\n",
    "            ),\n",
    "            'modifications': result['modifications'] if 'modifications' in result else [],\n",
    "            'unit': 'uM'\n",
    "        })\n",
    "        if rows[-1]['value']:\n",
    "            rows[-1]['value'] = np.log10(rows[-1]['value'])\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the rows into an array\n",
    "rows = []\n",
    "for result_set in all_results:\n",
    "    for sequence in result_set:\n",
    "        for row in convert_result_to_rows(sequence, result_set[sequence]):\n",
    "            rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the df\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dataframe length before removing bad chars:', 62494)\n",
      "('Dataframe length after removing bad chars:', 57697)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe length before removing bad chars:\", len(df))\n",
    "# Remove sequences with amino acids that aren't well-defined\n",
    "def strip_sequences_with_char(df, bad_char):\n",
    "    return df[~df.sequence.str.contains(bad_char)]\n",
    "\n",
    "for bad_char in ['U', 'X', 'Z']:\n",
    "    df = strip_sequences_with_char(df, bad_char)\n",
    "print(\"Dataframe length after removing bad chars:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll want to strip off any sequences with modifications that could be hard to replicate\n",
    "# Their effects are too complex for the model\n",
    "def is_modified(modifications_list):\n",
    "    return len(modifications_list) > 0\n",
    "\n",
    "df['is_modified'] = df.modifications.apply(is_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, C-Terminal Amidation is common enough that we make an exception\n",
    "def has_non_cterminal_modification(modifications_list):\n",
    "    return any(['C-Term' not in modification for modification in modifications_list])\n",
    "\n",
    "df['has_non_cterminal_modification'] = df.modifications.apply(has_non_cterminal_modification)\n",
    "\n",
    "df['has_cterminal_modification'] = df.is_modified & ~df.has_non_cterminal_modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean sequences by removing newlines and one improper sequence\n",
    "df.sequence = df.sequence.str.strip()\n",
    "df = df.loc[df.sequence != '/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude sequences with modifications\n",
    "# Exclude rows from YADAMP and CAMP for having no modification data\n",
    "#     Unless that sequence is in another DB\n",
    "\n",
    "df = df.loc[df.has_non_cterminal_modification == False]\n",
    "\n",
    "no_modification_data_sources = ['camp3', 'yadamp']\n",
    "\n",
    "def datasource_has_modifications(cell):\n",
    "    # Everything except CAMP and YADAMP has modification data\n",
    "    return not any([s in cell for s in no_modification_data_sources])\n",
    "\n",
    "df['_datasource_has_modifications'] = df['url_source'].apply(datasource_has_modifications)\n",
    "\n",
    "sequences_containing_modifications = set(df.loc[df._datasource_has_modifications == True, 'sequence'])\n",
    "def sequence_has_modification_data(cell):\n",
    "    # If the sequence is labeled modifictationless in another database it's OK\n",
    "    return cell in sequences_containing_modifications\n",
    "\n",
    "df['_sequence_has_modifications'] = df['sequence'].apply(sequence_has_modification_data)\n",
    "\n",
    "df['modification_verified'] = df['_sequence_has_modifications'] | df['_datasource_has_modifications']\n",
    "\n",
    "df = df.loc[df.modification_verified == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARACTER_DICT = set([character for sequence in df.sequence for character in sequence])\n",
    "MAX_SEQUENCE_LENGTH = int(df.sequence.str.len().describe(percentiles=[0.95])['95%'])\n",
    "\n",
    "# Each amino acid its own group\n",
    "character_to_index = {\n",
    "    (character): i\n",
    "    for i, character in enumerate(CHARACTER_DICT)\n",
    "}\n",
    "\n",
    "# Group them together heavily\n",
    "\"\"\"character_to_index = {\n",
    "    ('R', 'K', 'H'): 0,\n",
    "    ('D', 'E'): 1,\n",
    "    ('S', 'T', 'N', 'Q', 'C'): 2,\n",
    "    ('A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W', 'P', 'G'): 3,\n",
    "}\n",
    "\n",
    "# Group them together more sparingly\n",
    "character_to_index = {\n",
    "    ('R'): 0,\n",
    "    ('H'): 1,\n",
    "    ('K'): 2,\n",
    "    ('D', 'E'): 3,\n",
    "    ('S', 'T', 'N', 'Q', 'C'): 4,\n",
    "    ('G', 'P'): 5,\n",
    "    ('A', 'V', 'I', 'L', 'M'): 6,\n",
    "    ('F', 'Y', 'W'): 7,\n",
    "}\"\"\"\n",
    "\n",
    "index2character = {\n",
    "    value: key\n",
    "    for key, value in character_to_index.items()\n",
    "}\n",
    "\n",
    "def sequence_to_vector(sequence, cterminal_amidation):\n",
    "# It looks like this truncates any sequence after max_sequence_length (which is length of 95th percentile longest peptide)\n",
    "# I just add cterminal amidation as the amino acid after the last real amino acid (if the amino acid gets truncated\n",
    "# then the cterminal amidation also gets cut off)\n",
    "    default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "    for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "        default[i][character_to_index[character]] = 1\n",
    "    if len(sequence)<MAX_SEQUENCE_LENGTH:\n",
    "        default[len(sequence)][-1]=cterminal_amidation\n",
    "    return default\n",
    "\n",
    "def old_sequence_to_vector(sequence, cterminal_amidation):\n",
    "    default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "    for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "        default[i][character_to_index[character]] = 1\n",
    "        default[i][-1] = cterminal_amidation\n",
    "    return default\n",
    "\n",
    "def find_character(character2index, character):\n",
    "    for key in character2index:\n",
    "        if character in key:\n",
    "            return character2index[key]\n",
    "    return -2\n",
    "\n",
    "def row_to_vector(row, shuffle_sequence=False):\n",
    "    sequence = list(row['sequence'])\n",
    "    if shuffle_sequence:\n",
    "        random.shuffle(sequence)\n",
    "    cterminal_amidation = row['has_cterminal_modification']\n",
    "    return sequence_to_vector(sequence,cterminal_amidation)\n",
    "\n",
    "def old_row_to_vector(row, shuffle_sequence=False):\n",
    "    sequence = list(row['sequence'])\n",
    "    if shuffle_sequence:\n",
    "        random.shuffle(sequence)\n",
    "    cterminal_amidation = row['has_cterminal_modification']\n",
    "    default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "\n",
    "    for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "        default[i][find_character(character_to_index, character)] = 1\n",
    "        default[i][-1] = cterminal_amidation\n",
    "\n",
    "    return default\n",
    "\n",
    "def vector_to_amp(vector):\n",
    "    sequence = ''\n",
    "    has_cterm = False\n",
    "    for v in vector:\n",
    "        nonzeros = np.argwhere(v[:len(character_to_index)])\n",
    "        if len(nonzeros) > 1:\n",
    "            print(\"?????\")\n",
    "        elif len(nonzeros) == 0:\n",
    "            sequence += '_'\n",
    "        else:\n",
    "            sequence += index2character[np.argwhere(v)[0][0]]  # First one\n",
    "        if v[-1]>0:\n",
    "            has_cterm=True\n",
    "    return {\n",
    "        'sequence': sequence,\n",
    "        'cterminal_amidation': has_cterm\n",
    "    }\n",
    "\n",
    "def bacterium_to_sample_weight(bacterium, intended_bacterium='E. coli'):\n",
    "    if intended_bacterium in bacterium:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containing_bacterium(bacterium, df):\n",
    "    return df.loc[df.bacterium.str.contains(bacterium)]\n",
    "\n",
    "def average_over_databases(bacterium_df):\n",
    "    return bacterium_df.groupby('sequence')['value'].mean().dropna()\n",
    "\n",
    "staph = containing_bacterium('S. aureus', df)\n",
    "staph = average_over_databases(staph)\n",
    "\n",
    "ecoli = df.loc[df.bacterium.str.contains('E. coli')].groupby('sequence')['value'].mean().dropna()\n",
    "pseudomonas = df.loc[df.bacterium.str.contains('P. aeruginosa')].groupby('sequence')['value'].mean().dropna()\n",
    "streptococcus = df.loc[df.bacterium.str.contains('S. mutans')].groupby('sequence')['value'].mean().dropna()\n",
    "bacillus = df.loc[df.bacterium.str.contains('B. subtilis')].groupby('sequence')['value'].mean().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ecoli</th>\n",
       "      <th>pseudomonas</th>\n",
       "      <th>streptococcus</th>\n",
       "      <th>staph</th>\n",
       "      <th>bacillus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ecoli</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pseudomonas</th>\n",
       "      <td>0.78</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>streptococcus</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staph</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacillus</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ecoli  pseudomonas  streptococcus  staph  bacillus\n",
       "ecoli           1.00         0.78           0.71   0.67      0.69\n",
       "pseudomonas     0.78         1.00           0.52   0.65      0.60\n",
       "streptococcus   0.71         0.52           1.00   0.83      0.82\n",
       "staph           0.67         0.65           0.83   1.00      0.67\n",
       "bacillus        0.69         0.60           0.82   0.67      1.00"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the correlation between bacteria\n",
    "# Note that Gram-Positivity seems to have a strong effect on correlation\n",
    "# E. coli and pseudomonas are highly correlated\n",
    "# While neither is correlated with streptococcus, staph or bacillus\n",
    "# Meanwhile, staph and streptococcus are strongly correlated as expected\n",
    "# As are bacillus and streptococcus\n",
    "# The lack of correlation between bacillus and staph is a mystery to me\n",
    "many_bacteria = pd.concat([ecoli, pseudomonas, streptococcus, staph, bacillus], axis=1).reset_index()\n",
    "many_bacteria.columns = ['index', 'ecoli', 'pseudomonas', 'streptococcus', 'staph', 'bacillus']\n",
    "many_bacteria.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zswitten/.pyenv/versions/2.7.12/lib/python2.7/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Since ecoli data and pseudomonas data are highly correlated, we will use some pseudomonas data in the model\n",
    "# (With a lower sample weight)\n",
    "ecoli_pseudomonas = many_bacteria.dropna(subset=('ecoli', 'pseudomonas'))\n",
    "x = np.array(ecoli_pseudomonas['pseudomonas']).reshape(-1, 1)\n",
    "y = np.array(ecoli_pseudomonas['ecoli']).reshape(-1, 1)\n",
    "pseudomonas_to_ecoli_model = LinearRegression().fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bacterium_df(bacterium, df):\n",
    "    bdf = df.loc[(df.bacterium.str.contains(bacterium))].groupby(['sequence', 'bacterium'])\n",
    "    return bdf.mean().reset_index().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_bad_amino_acids(df, bad_amino_acids=('U', 'X', 'Z')):\n",
    "    for b in bad_amino_acids:\n",
    "        df = df.loc[~df.sequence.str.contains(b)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli_df = get_bacterium_df('E. coli', df)\n",
    "ecoli_df = strip_bad_amino_acids(ecoli_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x114d01610>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqVMGgCR16v8Az1VCVFIwO5QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MIC on E. coli is normally distributed-ish\n",
    "ecoli_df.value.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the input vectors for our model\n",
    "# Each vector is two dimensional\n",
    "# The first dimension represents the number of characters in the sequence (46 characters)\n",
    "# Each character is a vector of length equal to the number of groupings of amino acids\n",
    "# This grouping can be 1-1 (each amino acid gets its own group), or coarser\n",
    "SHUFFLE_SEQUENCE = False\n",
    "cterminal_amidation = np.array(ecoli_df.has_cterminal_modification)\n",
    "\n",
    "vectors = []\n",
    "for row in ecoli_df.iterrows():\n",
    "    vectors.append(row_to_vector(row[1], shuffle_sequence=SHUFFLE_SEQUENCE))\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "labels = np.array(ecoli_df.value)\n",
    "sample_weights = np.full(len(labels), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sequence(alphabet, length_of_sequence_min=25, length_of_sequence_max=40):\n",
    "        sequence = ''\n",
    "        for _ in range(random.choice(range(length_of_sequence_min, length_of_sequence_max))):\n",
    "            sequence += random.choice(list(alphabet))\n",
    "        has_cterminal_modification = random.choice([0, 1])\n",
    "\n",
    "        return {\n",
    "            'sequence': sequence,\n",
    "            'has_cterminal_modification': has_cterminal_modification\n",
    "        }\n",
    "\n",
    "def add_random_negative_examples(vectors, labels, sample_weights, ratio, max_mic = None):\n",
    "    if not max_mic:\n",
    "        max_mic = max(labels)\n",
    "    # We will add randomly chosen sequences as negative examples\n",
    "    # We will double the length of our training set\n",
    "\n",
    "    len_vectors = ratio * len(vectors)\n",
    "    negative_rows = []\n",
    "    for i in range(len_vectors):\n",
    "        negative_rows.append(row_to_vector(generate_random_sequence(list(CHARACTER_DICT))))\n",
    "    negative_vectors = np.array(negative_rows)\n",
    "    vectors = np.concatenate((vectors, negative_vectors))\n",
    "    negative_labels = np.full(len_vectors, max_mic)\n",
    "    labels = np.concatenate((labels, negative_labels))\n",
    "    # Weight all samples equally\n",
    "    sample_weights = np.concatenate((sample_weights, np.full(len_vectors, 1)))\n",
    "    return vectors, labels, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MIC = 4\n",
    "vectors, labels, sample_weights = add_random_negative_examples(vectors, labels, sample_weights, ratio=4, max_mic=MAX_MIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape after adding negative examples\n",
      "(22540, 46, 21)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector shape after adding negative examples\")\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simulated ecoli data from pseudomonas data\n",
    "bacteria_name = 'P. aeruginosa'\n",
    "pa_df = df.loc[(df.bacterium.str.contains(bacteria_name))].groupby(['sequence', 'bacterium'])\n",
    "pa_df = pa_df.mean().reset_index().dropna()\n",
    "\n",
    "pa_cterminal_amidation = np.array(pa_df.has_cterminal_modification)\n",
    "\n",
    "pa_vectors = []\n",
    "for row in pa_df.iterrows():\n",
    "    pa_vectors.append(row_to_vector(row[1], shuffle_sequence=SHUFFLE_SEQUENCE))\n",
    "\n",
    "pa_vectors = np.array(pa_vectors)\n",
    "\n",
    "pa_labels = np.array(pa_df.value.apply(pseudomonas_to_ecoli_model.predict))  # Interpolate using the linear model\n",
    "pa_sample_weights = np.array([0.5] * len(pa_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline error:\n",
      "1.4327302826406154\n",
      "Baseline error on measured examples only\n",
      "0.60182344038256\n"
     ]
    }
   ],
   "source": [
    "average = np.mean(labels)\n",
    "squared_errors = sum([(label - average) ** 2 for label in labels])\n",
    "baseline_error = squared_errors/len(labels)\n",
    "print(\"Baseline error:\")\n",
    "print(baseline_error)\n",
    "measured_labels = [l for l in labels if l < MAX_MIC]\n",
    "average = np.mean(measured_labels)\n",
    "squared_errors = sum([(label - average) ** 2 for label in measured_labels])\n",
    "baseline_error = squared_errors/len(measured_labels)\n",
    "print(\"Baseline error on measured examples only\")\n",
    "print(baseline_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "linear_estimator = LinearRegression()\n",
    "#cross_val_score(linear_estimator, vectors.reshape(vectors.shape[0], -1)[:10], labels[:10])\n",
    "i = 200\n",
    "#cross_val_score(linear_estimator, np.full(i, 1).reshape(-1, 1), random_labels, scoring='neg_mean_squared_error')\n",
    "#linear_estimator.fit(vectors.reshape(vectors.shape[0], -1)[:95], labels[:95])\n",
    "linear_estimator.fit(np.full(shape=(len(labels)), fill_value=1).reshape(-1, 1), labels)\n",
    "#cross_val_score(linear_estimator, vectors.reshape(vectors.shape[0], -1), labels, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, LSTM, Conv2D, Conv1D, MaxPooling1D, MaxPooling2D, Flatten, ZeroPadding1D\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_splits(\n",
    "        vectors, labels,\n",
    "        extra_training_vectors=[], extra_training_labels=[], extra_sample_weights=[],\n",
    "        cutoff=0.85\n",
    "):\n",
    "    cutoff = int(cutoff * len(labels))\n",
    "    idx = range(len(vectors))\n",
    "    random.shuffle(idx)\n",
    "    reordered_vectors = vectors[idx]\n",
    "    reordered_labels = labels[idx]\n",
    "    reordered_sample_weights = sample_weights[idx]\n",
    "    if len(extra_training_vectors) > 0:\n",
    "        train_x = np.concatenate((reordered_vectors[:cutoff], extra_training_vectors))\n",
    "        train_y = np.concatenate((reordered_labels[:cutoff], extra_training_labels))\n",
    "        train_sample_weights = np.concatenate((reordered_sample_weights[:cutoff], pa_sample_weights))\n",
    "    else:\n",
    "        train_x = reordered_vectors[:cutoff]\n",
    "        train_y = reordered_labels[:cutoff]\n",
    "        train_sample_weights = reordered_sample_weights[:cutoff]\n",
    "    test_x = reordered_vectors[cutoff:]\n",
    "    test_y = reordered_labels[cutoff:]\n",
    "    return train_x, train_y, test_x, test_y, train_sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional NN\n",
    "def conv_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(ZeroPadding1D(\n",
    "        5, input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(Conv1D(\n",
    "        64,\n",
    "        kernel_size = 5,\n",
    "        strides = 1,\n",
    "        activation = 'relu',\n",
    "        #input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21652/21652 [==============================] - 4s 171us/step - loss: 0.6349\n",
      "Epoch 2/100\n",
      "21652/21652 [==============================] - 3s 147us/step - loss: 0.2963\n",
      "Epoch 3/100\n",
      "21652/21652 [==============================] - 3s 147us/step - loss: 0.2254\n",
      "Epoch 4/100\n",
      "21652/21652 [==============================] - 3s 160us/step - loss: 0.1930\n",
      "Epoch 5/100\n",
      "21652/21652 [==============================] - 3s 149us/step - loss: 0.1612\n",
      "Epoch 6/100\n",
      "21652/21652 [==============================] - 3s 149us/step - loss: 0.1408\n",
      "Epoch 7/100\n",
      "21652/21652 [==============================] - 3s 150us/step - loss: 0.1268\n",
      "Epoch 8/100\n",
      "21652/21652 [==============================] - 3s 145us/step - loss: 0.1098\n",
      "Epoch 9/100\n",
      "21652/21652 [==============================] - 3s 144us/step - loss: 0.1016\n",
      "Epoch 10/100\n",
      "21652/21652 [==============================] - 3s 138us/step - loss: 0.0987\n",
      "Epoch 11/100\n",
      "21652/21652 [==============================] - 3s 139us/step - loss: 0.0888\n",
      "Epoch 12/100\n",
      "21652/21652 [==============================] - 3s 147us/step - loss: 0.0855\n",
      "Epoch 13/100\n",
      "21652/21652 [==============================] - 3s 147us/step - loss: 0.0873\n",
      "Epoch 14/100\n",
      "21652/21652 [==============================] - 3s 148us/step - loss: 0.0851\n",
      "Epoch 15/100\n",
      "21652/21652 [==============================] - 3s 143us/step - loss: 0.0804\n",
      "Epoch 16/100\n",
      "21652/21652 [==============================] - 3s 152us/step - loss: 0.0745\n",
      "Epoch 17/100\n",
      "21652/21652 [==============================] - 3s 140us/step - loss: 0.0689\n",
      "Epoch 18/100\n",
      "21652/21652 [==============================] - 3s 151us/step - loss: 0.0721\n",
      "Epoch 19/100\n",
      "21652/21652 [==============================] - 3s 146us/step - loss: 0.0647\n",
      "Epoch 20/100\n",
      "21652/21652 [==============================] - 3s 148us/step - loss: 0.0641\n",
      "Epoch 21/100\n",
      "21652/21652 [==============================] - 3s 150us/step - loss: 0.0667\n",
      "Epoch 22/100\n",
      "21652/21652 [==============================] - 3s 149us/step - loss: 0.0593\n",
      "Epoch 23/100\n",
      "21652/21652 [==============================] - 3s 149us/step - loss: 0.0678\n",
      "Epoch 24/100\n",
      "21652/21652 [==============================] - 3s 150us/step - loss: 0.0615\n",
      "Epoch 25/100\n",
      "21652/21652 [==============================] - 3s 147us/step - loss: 0.0616\n",
      "Epoch 26/100\n",
      "21652/21652 [==============================] - 3s 148us/step - loss: 0.0574\n",
      "Epoch 27/100\n",
      "21652/21652 [==============================] - 3s 147us/step - loss: 0.0546\n",
      "Epoch 28/100\n",
      "21652/21652 [==============================] - 3s 146us/step - loss: 0.0608\n",
      "Epoch 29/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0640\n",
      "Epoch 30/100\n",
      "21652/21652 [==============================] - 3s 149us/step - loss: 0.0548\n",
      "Epoch 31/100\n",
      "21652/21652 [==============================] - 3s 146us/step - loss: 0.0553\n",
      "Epoch 32/100\n",
      "21652/21652 [==============================] - 3s 148us/step - loss: 0.0559\n",
      "Epoch 33/100\n",
      "21652/21652 [==============================] - 3s 145us/step - loss: 0.0534\n",
      "Epoch 34/100\n",
      "21652/21652 [==============================] - 3s 142us/step - loss: 0.0502\n",
      "Epoch 35/100\n",
      "21652/21652 [==============================] - 3s 138us/step - loss: 0.0485\n",
      "Epoch 36/100\n",
      "21652/21652 [==============================] - 3s 139us/step - loss: 0.0524\n",
      "Epoch 37/100\n",
      "21652/21652 [==============================] - 3s 153us/step - loss: 0.0555\n",
      "Epoch 38/100\n",
      "21652/21652 [==============================] - 3s 142us/step - loss: 0.0519\n",
      "Epoch 39/100\n",
      "21652/21652 [==============================] - 3s 156us/step - loss: 0.0507\n",
      "Epoch 40/100\n",
      "21652/21652 [==============================] - 3s 143us/step - loss: 0.0453\n",
      "Epoch 41/100\n",
      "21652/21652 [==============================] - 3s 147us/step - loss: 0.0440\n",
      "Epoch 42/100\n",
      "21652/21652 [==============================] - 3s 145us/step - loss: 0.0460\n",
      "Epoch 43/100\n",
      "21652/21652 [==============================] - 3s 144us/step - loss: 0.0432\n",
      "Epoch 44/100\n",
      "21652/21652 [==============================] - 3s 138us/step - loss: 0.0444\n",
      "Epoch 45/100\n",
      "21652/21652 [==============================] - 3s 148us/step - loss: 0.0438\n",
      "Epoch 46/100\n",
      "21652/21652 [==============================] - 3s 150us/step - loss: 0.0462\n",
      "Epoch 47/100\n",
      "21652/21652 [==============================] - 3s 146us/step - loss: 0.0508\n",
      "Epoch 48/100\n",
      "21652/21652 [==============================] - 3s 142us/step - loss: 0.0446\n",
      "Epoch 49/100\n",
      "21652/21652 [==============================] - 3s 144us/step - loss: 0.0420\n",
      "Epoch 50/100\n",
      "21652/21652 [==============================] - 3s 140us/step - loss: 0.0398\n",
      "Epoch 51/100\n",
      "21652/21652 [==============================] - 3s 144us/step - loss: 0.0425\n",
      "Epoch 52/100\n",
      "21652/21652 [==============================] - 3s 153us/step - loss: 0.0433\n",
      "Epoch 53/100\n",
      "21652/21652 [==============================] - 3s 143us/step - loss: 0.0438\n",
      "Epoch 54/100\n",
      "21652/21652 [==============================] - 3s 139us/step - loss: 0.0431\n",
      "Epoch 55/100\n",
      "21652/21652 [==============================] - 3s 141us/step - loss: 0.0400\n",
      "Epoch 56/100\n",
      "21652/21652 [==============================] - 3s 141us/step - loss: 0.0423\n",
      "Epoch 57/100\n",
      "21652/21652 [==============================] - 3s 140us/step - loss: 0.0397\n",
      "Epoch 58/100\n",
      "21652/21652 [==============================] - 3s 140us/step - loss: 0.0427\n",
      "Epoch 59/100\n",
      "21652/21652 [==============================] - 3s 140us/step - loss: 0.0416\n",
      "Epoch 60/100\n",
      "21652/21652 [==============================] - 3s 143us/step - loss: 0.0429\n",
      "Epoch 61/100\n",
      "21652/21652 [==============================] - 3s 138us/step - loss: 0.0473\n",
      "Epoch 62/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0366\n",
      "Epoch 63/100\n",
      "21652/21652 [==============================] - 3s 138us/step - loss: 0.0381\n",
      "Epoch 64/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0352\n",
      "Epoch 65/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0396\n",
      "Epoch 66/100\n",
      "21652/21652 [==============================] - 3s 138us/step - loss: 0.0398\n",
      "Epoch 67/100\n",
      "21652/21652 [==============================] - 3s 138us/step - loss: 0.0415\n",
      "Epoch 68/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0386\n",
      "Epoch 69/100\n",
      "21652/21652 [==============================] - 3s 138us/step - loss: 0.0376\n",
      "Epoch 70/100\n",
      "21652/21652 [==============================] - 3s 140us/step - loss: 0.0364\n",
      "Epoch 71/100\n",
      "21652/21652 [==============================] - 3s 142us/step - loss: 0.0386\n",
      "Epoch 72/100\n",
      "21652/21652 [==============================] - 3s 147us/step - loss: 0.0361\n",
      "Epoch 73/100\n",
      "21652/21652 [==============================] - 3s 138us/step - loss: 0.0338\n",
      "Epoch 74/100\n",
      "21652/21652 [==============================] - 3s 138us/step - loss: 0.0392\n",
      "Epoch 75/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0369\n",
      "Epoch 76/100\n",
      "21652/21652 [==============================] - 3s 138us/step - loss: 0.0365\n",
      "Epoch 77/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0356\n",
      "Epoch 78/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0362\n",
      "Epoch 79/100\n",
      "21652/21652 [==============================] - 3s 136us/step - loss: 0.0361\n",
      "Epoch 80/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0353\n",
      "Epoch 81/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0330\n",
      "Epoch 82/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0352\n",
      "Epoch 83/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0339\n",
      "Epoch 84/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0357\n",
      "Epoch 85/100\n",
      "21652/21652 [==============================] - 3s 141us/step - loss: 0.0350\n",
      "Epoch 86/100\n",
      "21652/21652 [==============================] - 3s 141us/step - loss: 0.0353\n",
      "Epoch 87/100\n",
      "21652/21652 [==============================] - 3s 150us/step - loss: 0.0347\n",
      "Epoch 88/100\n",
      "21652/21652 [==============================] - 3s 144us/step - loss: 0.0321\n",
      "Epoch 89/100\n",
      "21652/21652 [==============================] - 3s 139us/step - loss: 0.0355\n",
      "Epoch 90/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0356\n",
      "Epoch 91/100\n",
      "21652/21652 [==============================] - 3s 137us/step - loss: 0.0322\n",
      "Epoch 92/100\n",
      "21652/21652 [==============================] - 3s 149us/step - loss: 0.0349\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21652/21652 [==============================] - 3s 147us/step - loss: 0.0308\n",
      "Epoch 94/100\n",
      "21652/21652 [==============================] - 3s 143us/step - loss: 0.0325\n",
      "Epoch 95/100\n",
      "21652/21652 [==============================] - 3s 144us/step - loss: 0.0332\n",
      "Epoch 96/100\n",
      "21652/21652 [==============================] - 3s 147us/step - loss: 0.0330\n",
      "Epoch 97/100\n",
      "21652/21652 [==============================] - 3s 153us/step - loss: 0.0314\n",
      "Epoch 98/100\n",
      "21652/21652 [==============================] - 3s 139us/step - loss: 0.0331\n",
      "Epoch 99/100\n",
      "21652/21652 [==============================] - 3s 146us/step - loss: 0.0297\n",
      "Epoch 100/100\n",
      "21652/21652 [==============================] - 3s 145us/step - loss: 0.0326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a4a2e10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convmodel = conv_model()\n",
    "train_x, train_y, test_x, test_y, _ = generate_train_test_splits(vectors, labels, pa_vectors, pa_labels, pa_sample_weights)\n",
    "convmodel.fit(train_x, train_y, batch_size=40, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN test error, MSE of log MIC\n",
      "3381/3381 [==============================] - 1s 167us/step\n",
      "0.11308013951238956\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN test error, MSE of log MIC\")\n",
    "print(convmodel.evaluate(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN test error, MSE of log MIC\n",
      "2705/2705 [==============================] - 1s 193us/step\n",
      "0.10254753295077414\n"
     ]
    }
   ],
   "source": [
    "# THIS IS THE OLD ONE\n",
    "print(\"CNN test error, MSE of log MIC\")\n",
    "print(convmodel.evaluate(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN test error on measured examples only\n",
      "0.55005944\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN test error on measured examples only\")\n",
    "print(np.mean(\n",
    "    [(actual - predicted) ** 2 \n",
    "     for actual, predicted in zip(test_y, convmodel.predict(test_x)) if actual < MAX_MIC\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN test error on measured examples only\n",
      "0.38956055\n"
     ]
    }
   ],
   "source": [
    "# THIS IS THE OLD ONE\n",
    "print(\"CNN test error on measured examples only\")\n",
    "print(np.mean(\n",
    "    [(actual - predicted) ** 2 \n",
    "     for actual, predicted in zip(test_y, convmodel.predict(test_x)) if actual < MAX_MIC\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN test error on measured examples only (RANDOM SHUFFLED SEQUENCE)\n",
      "0.29721802\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN test error on measured examples only (RANDOM SHUFFLED SEQUENCE)\")\n",
    "print(np.mean(\n",
    "    [(actual - predicted) ** 2 \n",
    "     for actual, predicted in zip(test_y, convmodel.predict(test_x)) if actual < MAX_MIC\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scales={'Eisenberg':{'A':  0.25, 'R': -1.80, 'N': -0.64,'D': -0.72, 'C':  0.04, 'Q': -0.69,'E': -0.62, 'G':  0.16, 'H': -0.40,'I':  0.73, 'L':  0.53, 'K': -1.10,'M':  0.26, 'F':  0.61, 'P': -0.07,'S': -0.26, 'T': -0.18, 'W':  0.37,'Y':  0.02, 'V':  0.54},\n",
    "'Normalized_consensus':{'A':0.62,'C':0.29,'D':-0.9,'E':-0.74,'F':1.19,'G':0.48,'H':-0.4,'I':1.38,'K':-1.5,'L':1.06,'M':0.64,'N':-0.78,'P':0.12,'Q':-0.85,'R':-2.53,'S':-0.18,'T':-.05,'V':1.08,'W':0.81,'Y':0.26}}\n",
    "\n",
    "def hydrophobic_moment(sequence,scale='Normalized_consensus',angle=0,is_in_degrees=True,normalize=True):\n",
    "    # Angle should be 100 for alpha helix, 180 for beta sheet\n",
    "    hscale=scales[scale]\n",
    "    sin_sum = 0\n",
    "    cos_sum = 0\n",
    "    moment=0\n",
    "    for i in range(len(sequence)):\n",
    "        hp=hscale[sequence[i]]\n",
    "        angle_in_radians=i*angle\n",
    "        if is_in_degrees:\n",
    "            angle_in_radians = (i*angle)*math.pi/180.0\n",
    "        sin_sum += hp*math.sin(angle_in_radians)\n",
    "        cos_sum += hp*math.cos(angle_in_radians)\n",
    "    moment = math.sqrt(sin_sum**2+cos_sum**2)\n",
    "    if normalize:\n",
    "        moment = moment/len(sequence)\n",
    "    return moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli_amps_sequence = ecoli_df['sequence']\n",
    "amp_hydrophobic_moments = ecoli_amps_sequence.apply(hydrophobic_moment)\n",
    "\n",
    "shuffled_ecoli_amps = ecoli_amps_sequence.apply(lambda x: ''.join(random.sample(x, len(x))))\n",
    "# Randomly shuffled AMP sequences\n",
    "shuffled_amp_hydrophobic_moments = shuffled_ecoli_amps.apply(hydrophobic_moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_peptide(peptide, model):\n",
    "    sequence = peptide['sequence']\n",
    "    cterm = peptide['has_cterminal_modification']\n",
    "    return model.predict(row_to_vector(\n",
    "        {'sequence': sequence, 'has_cterminal_modification': int(cterm)}\n",
    "    ).reshape(-1, MAX_SEQUENCE_LENGTH, len(character_to_index) + 1))\n",
    "\n",
    "\n",
    "def find_nearby_sequences(sequence, old_sequences=None, character_dict=CHARACTER_DICT):\n",
    "    new_sequences = set()\n",
    "    if old_sequences == None:\n",
    "        old_sequences = set()\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "        for c1 in character_dict:\n",
    "            for j in range(i + 1, len(sequence)):\n",
    "                for c2 in character_dict:\n",
    "                    new_sequence = sequence[:i] + c1 + sequence[i+1:j] + c2 + sequence[j+1:]\n",
    "                    for cterm in (True, False):\n",
    "                        ns_dict = {'sequence': new_sequence, 'has_cterminal_modification': cterm}\n",
    "                        new_sequences.add(frozenset(ns_dict.items()))\n",
    "    return old_sequences | new_sequences\n",
    "\n",
    "def evaluate_peptides(peptides, model):\n",
    "    return model.predict(\n",
    "        np.array(\n",
    "            [row_to_vector(dict(p)) for p in peptides]\n",
    "        ).reshape(\n",
    "            -1, MAX_SEQUENCE_LENGTH, len(character_to_index) + 1\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearby_peptide_vectors(peptide_vector):\n",
    "    nearby_vectors = []\n",
    "    for i in range(len(peptide_vector)):\n",
    "#         print peptide_vector[i]\n",
    "        if np.sum(peptide_vector[i][:len(peptide_vector[i])-1])>0.5:\n",
    "            for j in range(len(peptide_vector[i]) - 1):  # - 1 because of amidation\n",
    "                v = np.zeros(len(peptide_vector[i]))\n",
    "                v[-1] = peptide_vector[0][-1]\n",
    "                v[j] = 1\n",
    "                new_vector = np.concatenate([\n",
    "                    peptide_vector[:i],\n",
    "                    v.reshape(-1, len(peptide_vector[i])),\n",
    "                    peptide_vector[i+1:]\n",
    "                ])\n",
    "                if is_acceptable(vector_to_amp(new_vector)['sequence']):\n",
    "                    nearby_vectors.append(new_vector)\n",
    "                else:\n",
    "                    print 'unacceptable! '+repr(vector_to_amp(new_vector)['sequence'])\n",
    "#                     cterm_flipped = deepcopy(new_vector)\n",
    "#                     reverse_cterm = (new_vector[0][-1] + 1) % 2\n",
    "#                     for c in cterm_flipped:\n",
    "#                         c[-1] = reverse_cterm\n",
    "#                     nearby_vectors.append(cterm_flipped)\n",
    "#         else:\n",
    "#             print 'Nope!'+repr(i)\n",
    "    return nearby_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio.SubsMat import MatrixInfo as matlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_acceptable(sequence_with_padding):\n",
    "    at_underscore=False\n",
    "    for char in sequence_with_padding:\n",
    "        if char == '_':\n",
    "            at_underscore=True\n",
    "        elif at_underscore:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(cdict=CHARACTER_DICT,min_seq_length=10):\n",
    "    s = generate_random_sequence(cdict,length_of_sequence_min=min_seq_length)\n",
    "    v = row_to_vector(s)\n",
    "    last=''\n",
    "    for i in range(100):\n",
    "        vs = nearby_peptide_vectors(v)\n",
    "        ps = convmodel.predict(np.array(vs))\n",
    "        best_i = min(range(len(ps)), key=lambda x: ps[x])\n",
    "        v = vs[best_i]\n",
    "        if vector_to_amp(v)['sequence']==last:\n",
    "            break\n",
    "        last=vector_to_amp(v)['sequence']\n",
    "    return vector_to_amp(v), ps[best_i][0]\n",
    "\n",
    "def generate_sequence_from_exp_sequences(sequence_pool,max_generated_length=25):\n",
    "    sequence=''\n",
    "    for ind in range(max_generated_length):\n",
    "#         print len(sequence_pool)\n",
    "        which_ind = random.randint(0,len(sequence_pool)-1)\n",
    "#         print 'which ind: '+repr(which_ind)\n",
    "        which = sequence_pool[which_ind]\n",
    "#         print which\n",
    "        if len(sequence)<len(which):\n",
    "#             print which[len(sequence)]\n",
    "            sequence=sequence+which[len(sequence)]\n",
    "        else:\n",
    "            break\n",
    "    has_cterminal_modification = random.choice([0, 1])>0.5\n",
    "#     print len(sequence)\n",
    "    return {\n",
    "        'sequence': sequence,\n",
    "        'has_cterminal_modification': has_cterminal_modification\n",
    "    }\n",
    "\n",
    "def generate_move(old_vector,min_length=10,max_length=25):\n",
    "    vector=deepcopy(old_vector)\n",
    "    asc=random.random()\n",
    "    peptide_length=0\n",
    "    for i in range(len(vector)):\n",
    "        if np.sum(vector[i][:len(vector[i])-1])>0.5:\n",
    "            peptide_length=i+1\n",
    "    if asc<0.025 and peptide_length>=min_length:\n",
    "#         Remove from the front 2.5% of the time\n",
    "        for i in range(len(vector)-1):\n",
    "            vector[i]=[k for k in vector[i+1]]\n",
    "        vector[len(vector)-1]=[0 for k in vector[0]]\n",
    "    elif asc < 0.05 and peptide_length>=min_length:\n",
    "#         Remove from the back 2.5% of the time\n",
    "        if peptide_length==len(vector):\n",
    "            vector[len(vector)-1]=[0]*len(vector[0])\n",
    "        else:\n",
    "            vector[peptide_length-1]=[k for k in vector[peptide_length]]\n",
    "            vector[peptide_length]=[0]*len(vector[0])\n",
    "    elif asc < .075 and peptide_length<max_length:\n",
    "#         Add to the front 2.5% of the time\n",
    "        which = random.randint(0,len(vector[0])-2)\n",
    "        blah=0\n",
    "        while which == character_to_index['C'] and blah<10:\n",
    "            which = random.randint(0,len(vector[0])-2)\n",
    "            blah += 1\n",
    "        for i in range(1,len(vector)):\n",
    "            vector[-i]=[k for k in vector[-i-1]]\n",
    "        vector[0]=[0 for k in vector[1]]\n",
    "        vector[0][which]=1\n",
    "    elif asc < .1 and peptide_length<max_length:\n",
    "#         Add to the back 2.5% of the time\n",
    "        which = random.randint(0,len(vector[0])-2)\n",
    "        blah=0\n",
    "        while which == character_to_index['C'] and blah<10:\n",
    "            which = random.randint(0,len(vector[0])-2)\n",
    "            blah += 1\n",
    "        vector[peptide_length][which]=1\n",
    "        if peptide_length<(len(vector)-1):\n",
    "            vector[peptide_length+1][len(vector[0])-1]=vector[peptide_length][len(vector[0])-1]\n",
    "        vector[peptide_length][-1]=0\n",
    "#     elif asc > 0.995 and peptide_length<len(vector):\n",
    "#     Toggle amidation 0.5% of the time\n",
    "#         vector[peptide_length][-1]=(vector[peptide_length][-1]+1)%2\n",
    "    else:\n",
    "#         Swap something in the middle\n",
    "        which_index=random.randint(0,peptide_length-1)\n",
    "        which_residue=random.randint(0,len(vector[0])-2)\n",
    "        blah=0\n",
    "        while which_residue == character_to_index['C'] and blah<10:\n",
    "            which_residue = random.randint(0,len(vector[0])-2)\n",
    "            blah += 1\n",
    "        try:\n",
    "#             print which_index\n",
    "            vector[which_index]=[0 for k in vector[0]]\n",
    "            vector[which_index][which_residue]=1\n",
    "        except:\n",
    "            print 'Trying to change index '+repr(which_index)+' but array is only of length '+repr(len(vector))\n",
    "    if not is_acceptable(vector_to_amp(vector)['sequence']):\n",
    "        print 'asc is: '+repr(asc)+' and sequence is '+repr(vector_to_amp(vector)['sequence'])\n",
    "        print 'which_residue: '+repr(which_residue)+', out of '+repr(len(vector[0])-1)\n",
    "        return old_vector\n",
    "    peptide_length=0\n",
    "    for i in range(len(vector)):\n",
    "        if np.sum(vector[i][:len(vector[i])-1])>0.5:\n",
    "            peptide_length=i+1\n",
    "    vector[peptide_length][-1]=1\n",
    "    return vector\n",
    "\n",
    "def accept_move(mic_old,mic_new,temp):\n",
    "    if mic_new<mic_old:\n",
    "        return True\n",
    "    return random.random()<np.exp((mic_old-mic_new)/temp)\n",
    "\n",
    "def generate_sequence_by_simulated_annealing(cdict=CHARACTER_DICT,min_seq_length=10,nsteps=100000,t0=1/np.log(2),tf=0.00001/np.log(2),all_seq=None):\n",
    "    if all_seq==None:\n",
    "        s = generate_random_sequence(cdict,length_of_sequence_min=min_seq_length)\n",
    "    else:\n",
    "        s = generate_sequence_from_exp_sequences(all_seq)\n",
    "    v = row_to_vector(s)\n",
    "    print 'Starting sequence: '+repr(s)\n",
    "    last=''\n",
    "    #So a transition that increases MIC by 1 will have acceptance probability of 0.5 in default\n",
    "    temp = t0\n",
    "    scale=np.power(tf/t0,1./nsteps)\n",
    "    for i in range(nsteps):\n",
    "        move = generate_move(v)\n",
    "        old_and_new = convmodel.predict(np.array([v,move]))\n",
    "        if accept_move(old_and_new[0][0],old_and_new[1][0],temp):\n",
    "            v = move\n",
    "        temp=temp*scale\n",
    "#     print vector_to_amp(v)\n",
    "#     print convmodel.predict(np.array([v]))[0][0]\n",
    "    return vector_to_amp(v), convmodel.predict(np.array([v]))[0][0]\n",
    "\n",
    "def generate_random_sequences(cdict=CHARACTER_DICT,min_seq_length=10):\n",
    "    s = generate_random_sequence(cdict,length_of_sequence_min=min_seq_length)\n",
    "    v = row_to_vector(s)\n",
    "    last=''\n",
    "    p=convmodel.predict(np.array([v]))\n",
    "    return vector_to_amp(v), p[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sequence: {'has_cterminal_modification': False, 'sequence': u'GIPGTIRGASK'}\n",
      "Starting sequence: {'has_cterminal_modification': False, 'sequence': u'SKWSTYMGG'}\n",
      "Starting sequence: {'has_cterminal_modification': False, 'sequence': u'IRNPRSAKQGKKW'}\n",
      "Starting sequence: {'has_cterminal_modification': True, 'sequence': u'RRRKVIRK'}\n",
      "Starting sequence: {'has_cterminal_modification': True, 'sequence': u'ILSKTRGVLFIKA'}\n",
      "Starting sequence: {'has_cterminal_modification': True, 'sequence': u'DVTYVVLGHARTI'}\n",
      "Starting sequence: {'has_cterminal_modification': True, 'sequence': u'QHERHWKKRFNSILFGT'}\n",
      "Starting sequence: {'has_cterminal_modification': True, 'sequence': u'AGFWLFAVRP'}\n",
      "Starting sequence: {'has_cterminal_modification': True, 'sequence': u'AKIRVWRRGLKKG'}\n",
      "Starting sequence: {'has_cterminal_modification': True, 'sequence': u'FLHTLFVWAWPKRAK'}\n",
      "[{'cterminal_amidation': True, 'sequence': u'KRRRRKQRKRRVI_________________________________'}, {'cterminal_amidation': True, 'sequence': u'PIRRRGYRRVYTRYI_______________________________'}, {'cterminal_amidation': True, 'sequence': u'SRIHRKPHYFPEYTQHIYRKRF________________________'}, {'cterminal_amidation': True, 'sequence': u'PKWKKWRITRRVI_________________________________'}, {'cterminal_amidation': True, 'sequence': u'EGFSVKHRTKGYTKRI______________________________'}, {'cterminal_amidation': True, 'sequence': u'FRMMRIRRRTGYRVI_______________________________'}, {'cterminal_amidation': True, 'sequence': u'RFKRLYRITRRVI_________________________________'}, {'cterminal_amidation': True, 'sequence': u'FKWPRYHKYNHRAVQPIRKR__________________________'}, {'cterminal_amidation': True, 'sequence': u'KISFKLPGMRGALADPIYFPP_________________________'}, {'cterminal_amidation': True, 'sequence': u'TEKWLHRLSHDPVDTPIRKT__________________________'}]\n",
      "[-0.21869785, -0.43427992, -1.3272889, -0.20490187, -0.48284125, -0.46515548, -0.1888529, -1.4552855, -1.0614609, -1.3129299]\n",
      "-0.7151694\n",
      "0.48774436\n"
     ]
    }
   ],
   "source": [
    "nseqs=10\n",
    "sequences=['']*nseqs\n",
    "pred_log_mics=[0]*nseqs\n",
    "for i in range(nseqs):\n",
    "    seq,plm=generate_sequence_by_simulated_annealing(all_seq=all_sequences,nsteps=100000)\n",
    "    sequences[i]=seq\n",
    "    pred_log_mics[i]=plm\n",
    "print sequences\n",
    "print pred_log_mics\n",
    "print np.mean(pred_log_mics)\n",
    "print np.std(pred_log_mics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRRRRKQRKRRVI\n",
      "-0.21869785\n",
      "KRRRRKQRKRRVI---------------------\n",
      " ||||||||||||\n",
      "-RRRRRRRRRRGIGKFLHSAKKFGKAFVGEIMNS\n",
      "  Score=57\n",
      "\n",
      "([0.17076746470377288, 0.1919030026788329, 0.036923076923076885], [13.999999999999966, 21.00000000000003, 7.899999999999988])\n",
      "\n",
      "\n",
      "PIRRRGYRRVYTRYI\n",
      "-0.43427992\n",
      "------PIRRRG----YRRVYTRYI\n",
      "       ||||||||||||\n",
      "RWCVYAYVRRRGVLVRYRRCW----\n",
      "  Score=46\n",
      "\n",
      "([0.26265339610088617, 0.3934377550953743, 0.4126666666666667], [29.800000000000153, 52.70000000000048, 64.30000000000062])\n",
      "\n",
      "\n",
      "SRIHRKPHYFPEYTQHIYRKRF\n",
      "-1.3272889\n",
      "SRIHRKPHYFPEYTQHIYRKRF----\n",
      "                 |||||\n",
      "---------------RLYRRRFVVGR\n",
      "  Score=35\n",
      "\n",
      "([0.3898483016594645, 0.08742896536226918, 0.010000000000000009], [88.29999999999926, 8.099999999999987, 2.600000000000001])\n",
      "\n",
      "\n",
      "PKWKKWRITRRVI\n",
      "-0.20490187\n",
      "PKWKKW-RITRRVI---\n",
      "  |||||||||||\n",
      "-WWKRWKRI-RRIFMMV\n",
      "  Score=49\n",
      "\n",
      "([0.4039710386899987, 0.46540474417056504, 0.1569230769230769], [55.60000000000052, 67.20000000000046, 26.500000000000107])\n",
      "\n",
      "\n",
      "EGFSVKHRTKGYTKRI\n",
      "-0.48284125\n",
      "-EGFSVKHRTKGYTKRI-----------\n",
      "   ||||||||||||||\n",
      "GCRFTVKP----YIKRIQLHYKGKMWCG\n",
      "  Score=38\n",
      "\n",
      "([0.28119185490422727, 0.09452119240281526, 0.25437499999999996], [54.0000000000005, 8.399999999999986, 58.20000000000056])\n",
      "\n",
      "\n",
      "FRMMRIRRRTGYRVI\n",
      "-0.46515548\n",
      "FRMMRI-RRRTGYRVI-------------\n",
      "  ||||||||||||||\n",
      "--MMRVMRRKT--KVIWEKKDFIGLYSID\n",
      "  Score=51\n",
      "\n",
      "([0.5140126828084575, 0.5574958382942853, 0.31200000000000006], [74.30000000000005, 78.19999999999983, 44.90000000000037])\n",
      "\n",
      "\n",
      "RFKRLYRITRRVI\n",
      "-0.1888529\n",
      "RFKRLYRITR-RVI---\n",
      "|||||||||||||\n",
      "RFRRLFRI-RVRVLKKI\n",
      "  Score=55\n",
      "\n",
      "([0.8361158279838996, 0.43079454311070536, 0.4269230769230769], [95.69999999999884, 51.60000000000046, 61.600000000000605])\n",
      "\n",
      "\n",
      "FKWPRYHKYNHRAVQPIRKR\n",
      "-1.4552855\n",
      "----FKWPRYHKYNHRAVQPIRKR\n",
      "    |||||\n",
      "FRRPFKWPRRFFKFF---------\n",
      "  Score=45\n",
      "\n",
      "([0.4024570530954696, 0.5480562380349132, 0.41849999999999987], [81.29999999999966, 96.89999999999877, 84.49999999999947])\n",
      "\n",
      "\n",
      "KISFKLPGMRGALADPIYFPP\n",
      "-1.0614609\n",
      "-----KISFKLPGMRGALADPIYFPP-----\n",
      "                   |||||||\n",
      "IKFEPPLPPKKAHKKFWEDDGIYYPPNHNFP\n",
      "  Score=38\n",
      "\n",
      "([0.09810328295114741, 0.29736078776875424, 0.11476190476190475], [16.399999999999963, 85.09999999999944, 36.300000000000246])\n",
      "\n",
      "\n",
      "TEKWLHRLSHDPVDTPIRKT\n",
      "-1.3129299\n",
      "---TEKWLHRLSHDPVDTPIRKT\n",
      "     ||||||\n",
      "LRRLRKWLRRLLKLL--------\n",
      "  Score=40\n",
      "\n",
      "([0.37389016046973844, 0.03808719963037097, 0.014000000000000049], [89.59999999999918, 2.1000000000000005, 3.800000000000002])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# best_indices=np.argpartition(pred_log_mics,4)[:4]\n",
    "# print best_indices\n",
    "# for index in best_indices:\n",
    "for index in range(len(sequences)):\n",
    "    seq=sequences[index]['sequence'][:sequences[index]['sequence'].find('_')]\n",
    "    print seq\n",
    "    print pred_log_mics[index]\n",
    "    show_best_n_alignments(seq,all_sequences,1)\n",
    "    print hmoment_analysis(seq)\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.717005  ]\n",
      " [1.3017523 ]\n",
      " [1.2388521 ]\n",
      " [0.66926587]\n",
      " [1.0179906 ]\n",
      " [1.0179906 ]\n",
      " [0.49489325]\n",
      " [1.0871339 ]\n",
      " [0.6007128 ]\n",
      " [0.7997312 ]\n",
      " [1.4129379 ]\n",
      " [0.85508037]]\n"
     ]
    }
   ],
   "source": [
    "good_seqs=['GRKTSRYRIRLIYRLRIRM','GRHLIWKLIKTYRPHPRL','MDPKQQLFGRKATPTHRVIIRYT','KLTKAGRLLSIRLKNQGRVV','WKHGKIGKWIKRYT']\n",
    "good_seqs.append('WKHGKIGKWIKRYT')\n",
    "good_seqs.append('KIKKWRRPLYSGRKKYQ')\n",
    "good_seqs.append('SRIHRKPHYFPEYTQHIYRKRF')\n",
    "good_seqs.append('RFKRLYRITRRVI')\n",
    "good_seqs.append('FKWPRYHKYNHRAVQPIRKR')\n",
    "good_seqs.append('TEKWLHRLSHDPVDTPIRKT')\n",
    "good_seqs.append('FRMMRIRRRTGYRVI')\n",
    "# Filters: predicted by https://www.dveltri.com/ascan/v2/ascan.html to be AMP\n",
    "# Predicted by a second model (not the one it was trained on) to have good AMP activity\n",
    "# High hydrophobic moment (compared to shuffled versions of itself)\n",
    "# Predicted by pepcalc to have good water solubility\n",
    "# Length < 25\n",
    "# Substitute serine (S) for cysteine because disulfide bonds are confusing\n",
    "# (Later runs just refuse to incorporate cysteine)\n",
    "# Sequence alignment shows it's not just copying something\n",
    "mics=convmodel.predict(np.array([sequence_to_vector(seq,True) for seq in good_seqs]))\n",
    "print mics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For FASTA search\n",
    ">m1\n",
    "GRKTSRYRIRLIYRLRIRM\n",
    ">m2\n",
    "GRHLIWKLIKTYRPHPRL\n",
    ">m3\n",
    "MDPKQQLFGRKATPTHRVIIRYT\n",
    ">m4\n",
    "KLTKAGRLLSIRLKNQGRVV\n",
    ">m5\n",
    "WKHGKIGKWIKRYT\n",
    ">m6\n",
    "WKHGKIGKWIKRYT\n",
    ">m7\n",
    "KIKKWRRPLYSGRKKYQ\n",
    ">m8\n",
    "SRIHRKPHYFPEYTQHIYRKRF\n",
    ">m9\n",
    "RFKRLYRITRRVI\n",
    ">m10\n",
    "FKWPRYHKYNHRAVQPIRKR\n",
    ">m11\n",
    "TEKWLHRLSHDPVDTPIRKT\n",
    ">m12\n",
    "FRMMRIRRRTGYRVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_best_n_alignments(test_sequence,sequence_db,nalign,score_matrix=matlist.pam30,gap_open_penalty=-9,gap_extension_penalty=-1):\n",
    "#     blosum62 is another option. Identity matrix is another. I think they're about the same.\n",
    "    alignment_scores=[0]*len(sequence_db)\n",
    "    for i in range(len(all_sequences)):\n",
    "        sequence=all_sequences[i]\n",
    "        alignment_scores[i]=pairwise2.align.localds(test_sequence,sequence,score_matrix,gap_open_penalty,gap_extension_penalty,score_only=True)\n",
    "    argm = np.argmax(alignment_scores)\n",
    "#     alignments=pairwise2.align.localds(test_sequence,all_sequences[argm],score_matrix,gap_open_penalty,gap_extension_penalty)\n",
    "    indices=np.argpartition(alignment_scores,-1*nalign)[(-1*nalign):]\n",
    "    for index in indices:\n",
    "        alignments=pairwise2.align.localds(test_sequence,all_sequences[index],score_matrix,gap_open_penalty,gap_extension_penalty)\n",
    "        print(pairwise2.format_alignment(*alignments[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRRWKLMKHVTETPNPR\n",
      "||||||||||\n",
      "GKWWSLLKHILK-----\n",
      "  Score=39\n",
      "\n",
      "GRRWKLMKHVTETPNPR-----\n",
      "   |||||||\n",
      "--KWKLKKHIGIGKHFLSAKKF\n",
      "  Score=43\n",
      "\n",
      "GRRW-KLMKHVTETPNPR\n",
      " |||||||\n",
      "RRRWWKLMM---------\n",
      "  Score=45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_sequences=[sequence for sequence in ecoli_df.sequence]\n",
    "show_best_n_alignments('GRRWKLMKHVTETPNPR',all_sequences,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmoment_analysis(test_sequence,angles=[100,140,180]):\n",
    "    hmoments=[0]*len(angles)\n",
    "    percentiles=[0]*len(angles)\n",
    "    for k in range(len(angles)):\n",
    "        test_angle=angles[k]\n",
    "        hmoments[k] = hydrophobic_moment(test_sequence,angle=test_angle)\n",
    "        other_h_moments=[0]*1000\n",
    "        shuffled=range(len(test_sequence))\n",
    "        perGreater=0\n",
    "        for i in range(1000):\n",
    "            np.random.shuffle(shuffled)\n",
    "            shuffled_seq=[test_sequence[j] for j in shuffled]\n",
    "            other_h_moments[i]=hydrophobic_moment(shuffled_seq,angle=test_angle)\n",
    "            if other_h_moments[i]<hydrophobic_moment(test_sequence,angle=test_angle):\n",
    "                perGreater+=.1\n",
    "        percentiles[k]=perGreater\n",
    "    return hmoments,percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.28744000139810794, 0.23115044867895798, 0.19357142857142856], [77.79999999999986, 63.000000000000625, 56.20000000000053])\n"
     ]
    }
   ],
   "source": [
    "print hmoment_analysis(u'YLFGLRVGKCWERNKALIAGTVLRISFH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1.])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
