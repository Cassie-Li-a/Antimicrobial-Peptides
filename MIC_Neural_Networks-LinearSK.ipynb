{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "%matplotlib inline\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scripts stored the outputs as dictionaries.\n",
    "all_results = []\n",
    "for f in os.listdir('.'):\n",
    "    if '.data' in f:\n",
    "        with open(f, 'r') as g:\n",
    "            all_results.append(ast.literal_eval(g.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize units of MIC\n",
    "def standardize_to_uM(concentration, unit, sequence):\n",
    "    concentration = concentration.replace(' ', '')\n",
    "    try:\n",
    "        concentration = float(concentration)\n",
    "    except:\n",
    "        return None\n",
    "    if unit == 'uM' or unit == u'\\xb5M' or unit == u'uM)':\n",
    "        return concentration\n",
    "    elif unit == 'ug/ml' or unit == u'\\xb5g/ml' or unit == u'ug/ml)':\n",
    "        try:\n",
    "            molWt = ProteinAnalysis(sequence).molecular_weight()\n",
    "        except ValueError:\n",
    "            return None\n",
    "        return concentration * 1000/molWt\n",
    "    elif unit == 'nmol/g' or unit == 'pmol/mg':\n",
    "        #1g, at density of 1g/mL, is 1mL, so nmol/g is nmol/mL = umol/L = uM yay!\n",
    "        return concentration\n",
    "    else:\n",
    "        # print 'Unit not recognized: ' + unit\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter an element of a result dictionary into df-ready row\n",
    "def convert_result_to_rows(sequence, result):\n",
    "    rows = []\n",
    "    if 'bacteria' not in result:\n",
    "        return rows\n",
    "    for bacterium, strain in result['bacteria']:\n",
    "        \n",
    "        rows.append({\n",
    "            'bacterium': bacterium,\n",
    "            'strain': strain,\n",
    "            'sequence': sequence.upper(),\n",
    "            'url_source': result['url_sources'][0],\n",
    "            'value': standardize_to_uM(\n",
    "                result['bacteria'][(bacterium, strain)]['value'],\n",
    "                result['bacteria'][(bacterium, strain)]['unit'],\n",
    "                sequence\n",
    "            ),\n",
    "            'modifications': result['modifications'] if 'modifications' in result else [],\n",
    "            'unit': 'uM'\n",
    "        })\n",
    "        if rows[-1]['value']:\n",
    "            rows[-1]['value'] = np.log10(rows[-1]['value'])\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the rows into an array\n",
    "rows = []\n",
    "for result_set in all_results:\n",
    "    for sequence in result_set:\n",
    "        for row in convert_result_to_rows(sequence, result_set[sequence]):\n",
    "            rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the df\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dataframe length before removing bad chars:', 62494)\n",
      "('Dataframe length after removing bad chars:', 57697)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe length before removing bad chars:\", len(df))\n",
    "# Remove sequences with amino acids that aren't well-defined\n",
    "def strip_sequences_with_char(df, bad_char):\n",
    "    return df[~df.sequence.str.contains(bad_char)]\n",
    "\n",
    "for bad_char in ['U', 'X', 'Z']:\n",
    "    df = strip_sequences_with_char(df, bad_char)\n",
    "print(\"Dataframe length after removing bad chars:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll want to strip off any sequences with modifications that could be hard to replicate\n",
    "# Their effects are too complex for the model\n",
    "def is_modified(modifications_list):\n",
    "    return len(modifications_list) > 0\n",
    "\n",
    "df['is_modified'] = df.modifications.apply(is_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, C-Terminal Amidation is common enough that we make an exception\n",
    "def has_non_cterminal_modification(modifications_list):\n",
    "    return any(['C-Term' not in modification for modification in modifications_list])\n",
    "\n",
    "df['has_non_cterminal_modification'] = df.modifications.apply(has_non_cterminal_modification)\n",
    "\n",
    "df['has_cterminal_modification'] = df.is_modified & ~df.has_non_cterminal_modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean sequences by removing newlines and one improper sequence\n",
    "df.sequence = df.sequence.str.strip()\n",
    "df = df.loc[df.sequence != '/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude sequences with modifications\n",
    "# Exclude rows from YADAMP and CAMP for having no modification data\n",
    "#     Unless that sequence is in another DB\n",
    "\n",
    "df = df.loc[df.has_non_cterminal_modification == False]\n",
    "\n",
    "no_modification_data_sources = ['camp3', 'yadamp']\n",
    "\n",
    "def datasource_has_modifications(cell):\n",
    "    # Everything except CAMP and YADAMP has modification data\n",
    "    return not any([s in cell for s in no_modification_data_sources])\n",
    "\n",
    "df['_datasource_has_modifications'] = df['url_source'].apply(datasource_has_modifications)\n",
    "\n",
    "sequences_containing_modifications = set(df.loc[df._datasource_has_modifications == True, 'sequence'])\n",
    "def sequence_has_modification_data(cell):\n",
    "    # If the sequence is labeled modifictationless in another database it's OK\n",
    "    return cell in sequences_containing_modifications\n",
    "\n",
    "df['_sequence_has_modifications'] = df['sequence'].apply(sequence_has_modification_data)\n",
    "\n",
    "df['modification_verified'] = df['_sequence_has_modifications'] | df['_datasource_has_modifications']\n",
    "\n",
    "df = df.loc[df.modification_verified == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARACTER_DICT = set([character for sequence in df.sequence for character in sequence])\n",
    "MAX_SEQUENCE_LENGTH = int(df.sequence.str.len().describe(percentiles=[0.95])['95%'])\n",
    "\n",
    "# Each amino acid its own group\n",
    "character_to_index = {\n",
    "    (character): i\n",
    "    for i, character in enumerate(CHARACTER_DICT)\n",
    "}\n",
    "\n",
    "# Group them together heavily\n",
    "\"\"\"character_to_index = {\n",
    "    ('R', 'K', 'H'): 0,\n",
    "    ('D', 'E'): 1,\n",
    "    ('S', 'T', 'N', 'Q', 'C'): 2,\n",
    "    ('A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W', 'P', 'G'): 3,\n",
    "}\n",
    "\n",
    "# Group them together more sparingly\n",
    "character_to_index = {\n",
    "    ('R'): 0,\n",
    "    ('H'): 1,\n",
    "    ('K'): 2,\n",
    "    ('D', 'E'): 3,\n",
    "    ('S', 'T', 'N', 'Q', 'C'): 4,\n",
    "    ('G', 'P'): 5,\n",
    "    ('A', 'V', 'I', 'L', 'M'): 6,\n",
    "    ('F', 'Y', 'W'): 7,\n",
    "}\"\"\"\n",
    "\n",
    "index2character = {\n",
    "    value: key\n",
    "    for key, value in character_to_index.items()\n",
    "}\n",
    "\n",
    "def sequence_to_vector(sequence, cterminal_amidation):\n",
    "    default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "    for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "        default[i][character_to_index[character]] = 1\n",
    "        default[i][-1] = cterminal_amidation\n",
    "    return default\n",
    "\n",
    "def find_character(character2index, character):\n",
    "    for key in character2index:\n",
    "        if character in key:\n",
    "            return character2index[key]\n",
    "    return -2\n",
    "\n",
    "\n",
    "def row_to_vector(row, shuffle_sequence=False):\n",
    "    sequence = list(row['sequence'])\n",
    "    if shuffle_sequence:\n",
    "        random.shuffle(sequence)\n",
    "    cterminal_amidation = row['has_cterminal_modification']\n",
    "    default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "\n",
    "    for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "        default[i][find_character(character_to_index, character)] = 1\n",
    "        default[i][-1] = cterminal_amidation\n",
    "\n",
    "    return default\n",
    "\n",
    "def vector_to_amp(vector):\n",
    "    sequence = ''\n",
    "    for v in vector:\n",
    "        nonzeros = np.argwhere(v[:len(character_to_index)])\n",
    "        if len(nonzeros) > 1:\n",
    "            print(\"?????\")\n",
    "        if len(nonzeros) == 0:\n",
    "            break\n",
    "        sequence += index2character[np.argwhere(v)[0][0]]  # First one\n",
    "    return {\n",
    "        'sequence': sequence,\n",
    "        'cterminal_amidation': bool(v[-1])\n",
    "    }\n",
    "\n",
    "def bacterium_to_sample_weight(bacterium, intended_bacterium='E. coli'):\n",
    "    if intended_bacterium in bacterium:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containing_bacterium(bacterium, df):\n",
    "    return df.loc[df.bacterium.str.contains(bacterium)]\n",
    "\n",
    "def average_over_databases(bacterium_df):\n",
    "    return bacterium_df.groupby('sequence')['value'].mean().dropna()\n",
    "\n",
    "staph = containing_bacterium('S. aureus', df)\n",
    "staph = average_over_databases(staph)\n",
    "\n",
    "ecoli = df.loc[df.bacterium.str.contains('E. coli')].groupby('sequence')['value'].mean().dropna()\n",
    "pseudomonas = df.loc[df.bacterium.str.contains('P. aeruginosa')].groupby('sequence')['value'].mean().dropna()\n",
    "streptococcus = df.loc[df.bacterium.str.contains('S. mutans')].groupby('sequence')['value'].mean().dropna()\n",
    "bacillus = df.loc[df.bacterium.str.contains('B. subtilis')].groupby('sequence')['value'].mean().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ecoli</th>\n",
       "      <th>pseudomonas</th>\n",
       "      <th>streptococcus</th>\n",
       "      <th>staph</th>\n",
       "      <th>bacillus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ecoli</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pseudomonas</th>\n",
       "      <td>0.78</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>streptococcus</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staph</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacillus</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ecoli  pseudomonas  streptococcus  staph  bacillus\n",
       "ecoli           1.00         0.78           0.71   0.67      0.69\n",
       "pseudomonas     0.78         1.00           0.52   0.65      0.60\n",
       "streptococcus   0.71         0.52           1.00   0.83      0.82\n",
       "staph           0.67         0.65           0.83   1.00      0.67\n",
       "bacillus        0.69         0.60           0.82   0.67      1.00"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the correlation between bacteria\n",
    "# Note that Gram-Positivity seems to have a strong effect on correlation\n",
    "# E. coli and pseudomonas are highly correlated\n",
    "# While neither is correlated with streptococcus, staph or bacillus\n",
    "# Meanwhile, staph and streptococcus are strongly correlated as expected\n",
    "# As are bacillus and streptococcus\n",
    "# The lack of correlation between bacillus and staph is a mystery to me\n",
    "many_bacteria = pd.concat([ecoli, pseudomonas, streptococcus, staph, bacillus], axis=1).reset_index()\n",
    "many_bacteria.columns = ['index', 'ecoli', 'pseudomonas', 'streptococcus', 'staph', 'bacillus']\n",
    "many_bacteria.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zswitten/.pyenv/versions/2.7.12/lib/python2.7/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Since ecoli data and pseudomonas data are highly correlated, we will use some pseudomonas data in the model\n",
    "# (With a lower sample weight)\n",
    "ecoli_pseudomonas = many_bacteria.dropna(subset=('ecoli', 'pseudomonas'))\n",
    "x = np.array(ecoli_pseudomonas['pseudomonas']).reshape(-1, 1)\n",
    "y = np.array(ecoli_pseudomonas['ecoli']).reshape(-1, 1)\n",
    "pseudomonas_to_ecoli_model = LinearRegression().fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bacterium_df(bacterium, df):\n",
    "    bdf = df.loc[(df.bacterium.str.contains(bacterium))].groupby(['sequence', 'bacterium'])\n",
    "    return bdf.mean().reset_index().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_bad_amino_acids(df, bad_amino_acids=('U', 'X', 'Z')):\n",
    "    for b in bad_amino_acids:\n",
    "        df = df.loc[~df.sequence.str.contains(b)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli_df = get_bacterium_df('E. coli', df)\n",
    "ecoli_df = strip_bad_amino_acids(ecoli_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10d3b6790>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAD7tJREFUeJzt3X+s3XV9x/Hna1QUZaMI5o60zS6J\nxIXY+GM3iCFZruAMP5aVLWo0RIvp0n/Q4Wgyuu0Psu0fTIZMk8WkGUxMDMiYC42QGYacGJPRCY5R\noBoaVmybAv4A9Oqcudt7f9wP5tq1FO6555zb83k+kpv7/X6+n3M+7096T1/n++N8T6oKSVJ/fmXS\nBUiSJsMAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHVq3aQLeDlnn312zc7OTrqM\nVfWTn/yEN7zhDZMuY2x6mq9znU4n41wffvjh71fVm07Ub00HwOzsLA899NCky1hVg8GA+fn5SZcx\nNj3N17lOp5NxrkmefiX9PAQkSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmd\nWtOfBJbWstmd9/zS+o7Ni1x9VNsoHLjxipGPoT64ByBJnTphACS5NclzSR5b1vbGJPclebL9PrO1\nJ8lnk+xP8miSdy57zNbW/8kkW0czHUnSK/VK9gA+D1x6VNtO4P6qOg+4v60DXAac1362A5+DpcAA\nbgDeBVwA3PBSaEiSJuOE5wCq6utJZo9q3gLMt+XbgAFwfWv/QlUV8GCS9UnOaX3vq6ofAiS5j6VQ\nuX3oGahrRx+Hl/TKrfQk8ExVHWnLzwAzbXkDcHBZv0Ot7Xjt/0+S7SztPTAzM8NgMFhhiWvTwsLC\n1M3p5Yx6vjs2L47suV+tmdPGU89a+Pvp6e94muc69FVAVVVJajWKac+3C9gFMDc3VyfbfbhP5GS8\nt/gwRj3fcVx180rt2LzITXtHf2HdgavmRz7GifT0dzzNc13pVUDPtkM7tN/PtfbDwKZl/Ta2tuO1\nS5ImZKUBsBt46UqercDdy9o/2q4GuhB4sR0q+irwviRntpO/72ttkqQJOeH+apLbWTqJe3aSQyxd\nzXMjcGeSbcDTwAdb93uBy4H9wE+BjwFU1Q+T/BXwzdbvL186ISxJmoxXchXQh4+z6ZJj9C3gmuM8\nz63Ara+qOknSyPhJYEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmd\nMgAkqVMGgCR1ygCQpE4ZAJLUqdF/f52kVTU7wa/BPHDjFRMbW6vPPQBJ6pQBIEmdMgAkqVMGgCR1\nygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqeG\nCoAkf5zk8SSPJbk9yeuSnJtkT5L9Sb6U5NTW97VtfX/bPrsaE5AkrcyKAyDJBuCPgLmqeitwCvAh\n4FPAzVX1ZuB5YFt7yDbg+dZ+c+snSZqQYQ8BrQNOS7IOeD1wBLgYuKttvw24si1vaeu07ZckyZDj\nS5JWaMUBUFWHgb8GvsvSf/wvAg8DL1TVYut2CNjQljcAB9tjF1v/s1Y6viRpOCv+UvgkZ7L0rv5c\n4AXgH4BLhy0oyXZgO8DMzAyDwWDYp1xTFhYWpm5OL2fU892xefHEncZk5rS1Vc8ovPRv2dPf8TTP\ndcUBALwX+M+q+h5Aki8DFwHrk6xr7/I3Aodb/8PAJuBQO2R0BvCDo5+0qnYBuwDm5uZqfn5+iBLX\nnsFgwLTN6eWMer5X77xnZM/9au3YvMhNe4d5Sa19B66aB/r6O57muQ5zDuC7wIVJXt+O5V8CPAE8\nALy/9dkK3N2Wd7d12vavVVUNMb4kaQjDnAPYw9LJ3G8Be9tz7QKuB65Lsp+lY/y3tIfcApzV2q8D\ndg5RtyRpSEPtr1bVDcANRzU/BVxwjL4/Az4wzHiSpNXjJ4ElqVMGgCR1ygCQpE4ZAJLUKQNAkjpl\nAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaA\nJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhS\npwwASerUUAGQZH2Su5J8O8m+JO9O8sYk9yV5sv0+s/VNks8m2Z/k0STvXJ0pSJJWYtg9gM8A/1xV\nvwm8DdgH7ATur6rzgPvbOsBlwHntZzvwuSHHliQNYcUBkOQM4LeBWwCq6udV9QKwBbitdbsNuLIt\nbwG+UEseBNYnOWfFlUuShrJuiMeeC3wP+PskbwMeBq4FZqrqSOvzDDDTljcAB5c9/lBrO4Kkk8Ls\nznsA2LF5kavb8jgcuPGKsY3Vk1TVyh6YzAEPAhdV1Z4knwF+BHyiqtYv6/d8VZ2Z5CvAjVX1jdZ+\nP3B9VT101PNuZ+kQETMzM791xx13rKi+tWphYYHTTz990mWsur2HXzxm+8xp8Ox/jbmYCXGuo7N5\nwxnjG+woJ+Nr9j3vec/DVTV3on7D7AEcAg5V1Z62fhdLx/ufTXJOVR1ph3iea9sPA5uWPX5ja/sl\nVbUL2AUwNzdX8/PzQ5S49gwGA6ZtTsBx3w3u2LzITXuH+TM7eTjX0Tlw1fzYxjratL5mYYhzAFX1\nDHAwyVta0yXAE8BuYGtr2wrc3ZZ3Ax9tVwNdCLy47FCRJGnMho3wTwBfTHIq8BTwMZZC5c4k24Cn\ngQ+2vvcClwP7gZ+2vpKkCRkqAKrqEeBYx5kuOUbfAq4ZZjxJ0urxk8CS1CkDQJI6ZQBIUqcMAEnq\nlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4Z\nAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEg\nSZ0yACSpUwaAJHXKAJCkTg0dAElOSfLvSb7S1s9NsifJ/iRfSnJqa39tW9/fts8OO7YkaeVWYw/g\nWmDfsvVPATdX1ZuB54FtrX0b8Hxrv7n1kyRNyFABkGQjcAXwd209wMXAXa3LbcCVbXlLW6dtv6T1\nlyRNwLohH/83wJ8Av9rWzwJeqKrFtn4I2NCWNwAHAapqMcmLrf/3lz9hku3AdoCZmRkGg8GQJa4t\nCwsLUzcngB2bF4/ZPnPa8bdNG+c6OpN8zUzraxaGCIAkvws8V1UPJ5lfrYKqahewC2Bubq7m51ft\nqdeEwWDAtM0J4Oqd9xyzfcfmRW7aO+z7jJODcx2dA1fNj22so03raxaG2wO4CPi9JJcDrwN+DfgM\nsD7JurYXsBE43PofBjYBh5KsA84AfjDE+JKkIaz4HEBV/WlVbayqWeBDwNeq6irgAeD9rdtW4O62\nvLut07Z/rapqpeNLkoYzis8BXA9cl2Q/S8f4b2nttwBntfbrgJ0jGFuS9AqtykG8qhoAg7b8FHDB\nMfr8DPjAaownSRqenwSWpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS\n1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmd\nMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTKw6AJJuSPJDkiSSPJ7m2\ntb8xyX1Jnmy/z2ztSfLZJPuTPJrknas1CUnSqzfMHsAisKOqzgcuBK5Jcj6wE7i/qs4D7m/rAJcB\n57Wf7cDnhhhbkjSkFQdAVR2pqm+15R8D+4ANwBbgttbtNuDKtrwF+EIteRBYn+ScFVcuSRrKqpwD\nSDILvAPYA8xU1ZG26Rlgpi1vAA4ue9ih1iZJmoB1wz5BktOBfwQ+WVU/SvKLbVVVSepVPt92lg4R\nMTMzw2AwGLbENWVhYWHq5gSwY/PiMdtnTjv+tmnjXEdnkq+ZaX3NwpABkOQ1LP3n/8Wq+nJrfjbJ\nOVV1pB3iea61HwY2LXv4xtb2S6pqF7ALYG5urubn54cpcc0ZDAZM25wArt55zzHbd2xe5Ka9Q7/P\nOCk419E5cNX82MY62rS+ZmG4q4AC3ALsq6pPL9u0G9jalrcCdy9r/2i7GuhC4MVlh4okSWM2TIRf\nBHwE2Jvkkdb2Z8CNwJ1JtgFPAx9s2+4FLgf2Az8FPjbE2JKkIa04AKrqG0COs/mSY/Qv4JqVjidJ\nWl19HLDsxOxxjsNL0rF4KwhJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqU\nASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlN8IJmnNm9S33R248YqJjDsu7gFI\nUqcMAEnqlAEgSZ0yACSpUwaAJHXKq4BG4OWuWNixeZGrJ3RFgyQtZwBI0nHM7rxnYm/axnEJqoeA\nJKlTBoAkdcoAkKROGQCS1CkDQJI6NfYASHJpku8k2Z9k57jHlyQtGetloElOAf4W+B3gEPDNJLur\n6olRjDepOwhK0slg3HsAFwD7q+qpqvo5cAewZcw1SJIYfwBsAA4uWz/U2iRJY5aqGt9gyfuBS6vq\nD9v6R4B3VdXHl/XZDmxvq28BvjO2AsfjbOD7ky5ijHqar3OdTifjXH+jqt50ok7jvhXEYWDTsvWN\nre0XqmoXsGucRY1Tkoeqam7SdYxLT/N1rtNpmuc67kNA3wTOS3JuklOBDwG7x1yDJIkx7wFU1WKS\njwNfBU4Bbq2qx8dZgyRpydjvBlpV9wL3jnvcNWRqD28dR0/zda7TaWrnOtaTwJKktcNbQUhSpwyA\nCUjygSSPJ/nfJFN5dUFPt/xIcmuS55I8NulaRinJpiQPJHmi/f1eO+maRinJ65L8W5L/aPP9i0nX\ntNoMgMl4DPgD4OuTLmQUlt3y4zLgfODDSc6fbFUj9Xng0kkXMQaLwI6qOh+4ELhmyv9d/xu4uKre\nBrwduDTJhROuaVUZABNQVfuqato+4LZcV7f8qKqvAz+cdB2jVlVHqupbbfnHwD6m+JP8tWShrb6m\n/UzVSVMDQKPgLT+mXJJZ4B3AnslWMlpJTknyCPAccF9VTdV8/VL4EUnyL8CvH2PTn1fV3eOuR1ot\nSU4H/hH4ZFX9aNL1jFJV/Q/w9iTrgX9K8taqmppzPQbAiFTVeyddwwSd8JYfOjkleQ1L//l/saq+\nPOl6xqWqXkjyAEvneqYmADwEpFHwlh9TKEmAW4B9VfXpSdczakne1N75k+Q0lr7H5NuTrWp1GQAT\nkOT3kxwC3g3ck+Srk65pNVXVIvDSLT/2AXdO8y0/ktwO/CvwliSHkmybdE0jchHwEeDiJI+0n8sn\nXdQInQM8kORRlt7U3FdVX5lwTavKTwJLUqfcA5CkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAk\nqVMGgCR16v8Az1VCVFIwO5QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MIC on E. coli is normally distributed-ish\n",
    "ecoli_df.value.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the input vectors for our model\n",
    "# Each vector is two dimensional\n",
    "# The first dimension represents the number of characters in the sequence (46 characters)\n",
    "# Each character is a vector of length equal to the number of groupings of amino acids\n",
    "# This grouping can be 1-1 (each amino acid gets its own group), or coarser\n",
    "SHUFFLE_SEQUENCE = False\n",
    "cterminal_amidation = np.array(ecoli_df.has_cterminal_modification)\n",
    "\n",
    "vectors = []\n",
    "for row in ecoli_df.iterrows():\n",
    "    vectors.append(row_to_vector(row[1], shuffle_sequence=SHUFFLE_SEQUENCE))\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "labels = np.array(ecoli_df.value)\n",
    "sample_weights = np.full(len(labels), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sequence(alphabet, length_of_sequence_min=25, length_of_sequence_max=40):\n",
    "        sequence = ''\n",
    "        for _ in range(random.choice(range(length_of_sequence_min, length_of_sequence_max))):\n",
    "            sequence += random.choice(list(alphabet))\n",
    "        has_cterminal_modification = random.choice([0, 1])\n",
    "\n",
    "        return {\n",
    "            'sequence': sequence,\n",
    "            'has_cterminal_modification': has_cterminal_modification\n",
    "        }\n",
    "\n",
    "def add_random_negative_examples(vectors, labels, sample_weights, ratio, max_mic = None):\n",
    "    if not max_mic:\n",
    "        max_mic = max(labels)\n",
    "    # We will add randomly chosen sequences as negative examples\n",
    "    # We will double the length of our training set\n",
    "\n",
    "    len_vectors = ratio * len(vectors)\n",
    "    negative_rows = []\n",
    "    for i in range(len_vectors):\n",
    "        negative_rows.append(row_to_vector(generate_random_sequence(list(CHARACTER_DICT))))\n",
    "    negative_vectors = np.array(negative_rows)\n",
    "    vectors = np.concatenate((vectors, negative_vectors))\n",
    "    negative_labels = np.full(len_vectors, max_mic)\n",
    "    labels = np.concatenate((labels, negative_labels))\n",
    "    # Weight all samples equally\n",
    "    sample_weights = np.concatenate((sample_weights, np.full(len_vectors, 1)))\n",
    "    return vectors, labels, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MIC = 3.5\n",
    "vectors, labels, sample_weights = add_random_negative_examples(vectors, labels, sample_weights, ratio=1, max_mic=MAX_MIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape after adding negative examples\n",
      "(9016, 46, 21)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector shape after adding negative examples\")\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simulated ecoli data from pseudomonas data\n",
    "bacteria_name = 'P. aeruginosa'\n",
    "pa_df = df.loc[(df.bacterium.str.contains(bacteria_name))].groupby(['sequence', 'bacterium'])\n",
    "pa_df = pa_df.mean().reset_index().dropna()\n",
    "\n",
    "pa_cterminal_amidation = np.array(pa_df.has_cterminal_modification)\n",
    "\n",
    "pa_vectors = []\n",
    "for row in pa_df.iterrows():\n",
    "    pa_vectors.append(row_to_vector(row[1], shuffle_sequence=SHUFFLE_SEQUENCE))\n",
    "\n",
    "pa_vectors = np.array(pa_vectors)\n",
    "\n",
    "pa_labels = np.array(pa_df.value.apply(pseudomonas_to_ecoli_model.predict))  # Interpolate using the linear model\n",
    "pa_sample_weights = np.array([0.5] * len(pa_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline error:\n",
      "1.6979921723869722\n",
      "Baseline error on measured examples only\n",
      "0.5995982451676932\n"
     ]
    }
   ],
   "source": [
    "average = np.mean(labels)\n",
    "squared_errors = sum([(label - average) ** 2 for label in labels])\n",
    "baseline_error = squared_errors/len(labels)\n",
    "print(\"Baseline error:\")\n",
    "print(baseline_error)\n",
    "measured_labels = [l for l in labels if l < MAX_MIC]\n",
    "average = np.mean(measured_labels)\n",
    "squared_errors = sum([(label - average) ** 2 for label in measured_labels])\n",
    "baseline_error = squared_errors/len(measured_labels)\n",
    "print(\"Baseline error on measured examples only\")\n",
    "print(baseline_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "linear_estimator = LinearRegression()\n",
    "#cross_val_score(linear_estimator, vectors.reshape(vectors.shape[0], -1)[:10], labels[:10])\n",
    "i = 200\n",
    "#cross_val_score(linear_estimator, np.full(i, 1).reshape(-1, 1), random_labels, scoring='neg_mean_squared_error')\n",
    "#linear_estimator.fit(vectors.reshape(vectors.shape[0], -1)[:95], labels[:95])\n",
    "linear_estimator.fit(np.full(shape=(len(labels)), fill_value=1).reshape(-1, 1), labels)\n",
    "#cross_val_score(linear_estimator, vectors.reshape(vectors.shape[0], -1), labels, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, LSTM, Conv2D, Conv1D, MaxPooling1D, MaxPooling2D, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_splits(\n",
    "        vectors, labels,\n",
    "        extra_training_vectors=[], extra_training_labels=[], extra_sample_weights=[],\n",
    "        cutoff=0.85\n",
    "):\n",
    "    cutoff = int(cutoff * len(labels))\n",
    "    idx = range(len(vectors))\n",
    "    random.shuffle(idx)\n",
    "    reordered_vectors = vectors[idx]\n",
    "    reordered_labels = labels[idx]\n",
    "    reordered_sample_weights = sample_weights[idx]\n",
    "    if len(extra_training_vectors) > 0:\n",
    "        train_x = np.concatenate((reordered_vectors[:cutoff], extra_training_vectors))\n",
    "        train_y = np.concatenate((reordered_labels[:cutoff], extra_training_labels))\n",
    "        train_sample_weights = np.concatenate((reordered_sample_weights[:cutoff], pa_sample_weights))\n",
    "    else:\n",
    "        train_x = reordered_vectors[:cutoff]\n",
    "        train_y = reordered_labels[:cutoff]\n",
    "        train_sample_weights = reordered_sample_weights[:cutoff]\n",
    "    test_x = reordered_vectors[cutoff:]\n",
    "    test_y = reordered_labels[cutoff:]\n",
    "    return train_x, train_y, test_x, test_y, train_sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla LSTM\n",
    "def baseline_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(LSTM(\n",
    "        128,\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, len(character_to_index) + 1),\n",
    "    ))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, kernel_initializer='normal'))\n",
    "    model.add(Dense(20, kernel_initializer='normal'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zswitten/.pyenv/versions/2.7.12/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/100\n",
      "7663/7663 [==============================] - 8s 1ms/step - loss: 2.0667\n",
      "Epoch 2/100\n",
      "7663/7663 [==============================] - 7s 973us/step - loss: 1.7670\n",
      "Epoch 3/100\n",
      "7663/7663 [==============================] - 7s 955us/step - loss: 1.7701\n",
      "Epoch 4/100\n",
      "7663/7663 [==============================] - 7s 973us/step - loss: 1.7568\n",
      "Epoch 5/100\n",
      "7663/7663 [==============================] - 8s 984us/step - loss: 1.7474\n",
      "Epoch 6/100\n",
      "7663/7663 [==============================] - 7s 977us/step - loss: 1.3031\n",
      "Epoch 7/100\n",
      "7663/7663 [==============================] - 7s 973us/step - loss: 0.5760\n",
      "Epoch 8/100\n",
      "7663/7663 [==============================] - 7s 971us/step - loss: 0.4600\n",
      "Epoch 9/100\n",
      "7663/7663 [==============================] - 7s 974us/step - loss: 0.4431\n",
      "Epoch 10/100\n",
      "7663/7663 [==============================] - 7s 966us/step - loss: 0.4087\n",
      "Epoch 11/100\n",
      "7663/7663 [==============================] - 7s 962us/step - loss: 0.3789\n",
      "Epoch 12/100\n",
      "7663/7663 [==============================] - 7s 952us/step - loss: 0.3649\n",
      "Epoch 13/100\n",
      "7663/7663 [==============================] - 7s 949us/step - loss: 0.3520\n",
      "Epoch 14/100\n",
      "7663/7663 [==============================] - 7s 952us/step - loss: 0.3442\n",
      "Epoch 15/100\n",
      "7663/7663 [==============================] - 7s 945us/step - loss: 0.3337\n",
      "Epoch 16/100\n",
      "7663/7663 [==============================] - 7s 944us/step - loss: 0.3248\n",
      "Epoch 17/100\n",
      "7663/7663 [==============================] - 7s 953us/step - loss: 0.3195\n",
      "Epoch 18/100\n",
      "7663/7663 [==============================] - 7s 953us/step - loss: 0.3073\n",
      "Epoch 19/100\n",
      "7663/7663 [==============================] - 7s 963us/step - loss: 0.2982\n",
      "Epoch 20/100\n",
      "7663/7663 [==============================] - 7s 955us/step - loss: 0.2923\n",
      "Epoch 21/100\n",
      "7663/7663 [==============================] - 7s 959us/step - loss: 0.2873\n",
      "Epoch 22/100\n",
      "7663/7663 [==============================] - 8s 999us/step - loss: 0.2955\n",
      "Epoch 23/100\n",
      "7663/7663 [==============================] - 8s 1ms/step - loss: 0.2872\n",
      "Epoch 24/100\n",
      "7663/7663 [==============================] - 8s 1ms/step - loss: 0.2959\n",
      "Epoch 25/100\n",
      "7663/7663 [==============================] - 7s 936us/step - loss: 0.2744\n",
      "Epoch 26/100\n",
      "7663/7663 [==============================] - 7s 942us/step - loss: 0.2581\n",
      "Epoch 27/100\n",
      "7663/7663 [==============================] - 8s 996us/step - loss: 0.2574\n",
      "Epoch 28/100\n",
      "7663/7663 [==============================] - 7s 947us/step - loss: 0.2584\n",
      "Epoch 29/100\n",
      "7663/7663 [==============================] - 7s 943us/step - loss: 0.2489\n",
      "Epoch 30/100\n",
      "7663/7663 [==============================] - 7s 942us/step - loss: 0.2423\n",
      "Epoch 31/100\n",
      "7663/7663 [==============================] - 7s 937us/step - loss: 0.2400\n",
      "Epoch 32/100\n",
      "7663/7663 [==============================] - 7s 937us/step - loss: 0.2495\n",
      "Epoch 33/100\n",
      "7663/7663 [==============================] - 7s 948us/step - loss: 0.2311\n",
      "Epoch 34/100\n",
      "7663/7663 [==============================] - 8s 1ms/step - loss: 0.2263\n",
      "Epoch 35/100\n",
      "7663/7663 [==============================] - 7s 967us/step - loss: 0.2279\n",
      "Epoch 36/100\n",
      "7663/7663 [==============================] - 7s 948us/step - loss: 0.2243\n",
      "Epoch 37/100\n",
      "7663/7663 [==============================] - 7s 965us/step - loss: 0.2172\n",
      "Epoch 38/100\n",
      "7663/7663 [==============================] - 7s 959us/step - loss: 0.2196\n",
      "Epoch 39/100\n",
      "7663/7663 [==============================] - 7s 953us/step - loss: 0.2287\n",
      "Epoch 40/100\n",
      "7663/7663 [==============================] - 7s 953us/step - loss: 0.2088\n",
      "Epoch 41/100\n",
      "7663/7663 [==============================] - 8s 988us/step - loss: 0.2092\n",
      "Epoch 42/100\n",
      "7663/7663 [==============================] - 8s 991us/step - loss: 0.2032\n",
      "Epoch 43/100\n",
      "7663/7663 [==============================] - 8s 982us/step - loss: 0.2033\n",
      "Epoch 44/100\n",
      "7663/7663 [==============================] - 7s 951us/step - loss: 0.2436\n",
      "Epoch 45/100\n",
      "7663/7663 [==============================] - 7s 942us/step - loss: 0.2072\n",
      "Epoch 46/100\n",
      "7663/7663 [==============================] - 7s 959us/step - loss: 0.1900\n",
      "Epoch 47/100\n",
      "7663/7663 [==============================] - 8s 984us/step - loss: 0.1847\n",
      "Epoch 48/100\n",
      "7663/7663 [==============================] - 7s 940us/step - loss: 0.1833\n",
      "Epoch 49/100\n",
      "7663/7663 [==============================] - 7s 966us/step - loss: 0.1821\n",
      "Epoch 50/100\n",
      "7663/7663 [==============================] - 7s 970us/step - loss: 0.1746\n",
      "Epoch 51/100\n",
      "7663/7663 [==============================] - 7s 978us/step - loss: 0.1954\n",
      "Epoch 52/100\n",
      "7663/7663 [==============================] - 7s 967us/step - loss: 0.1737\n",
      "Epoch 53/100\n",
      "7663/7663 [==============================] - 7s 954us/step - loss: 0.1730\n",
      "Epoch 54/100\n",
      "7663/7663 [==============================] - 7s 970us/step - loss: 0.1743\n",
      "Epoch 55/100\n",
      "7663/7663 [==============================] - 7s 976us/step - loss: 0.1715\n",
      "Epoch 56/100\n",
      "7663/7663 [==============================] - 7s 958us/step - loss: 0.1626\n",
      "Epoch 57/100\n",
      "7663/7663 [==============================] - 8s 1ms/step - loss: 0.1586\n",
      "Epoch 58/100\n",
      "7663/7663 [==============================] - 7s 960us/step - loss: 0.1618\n",
      "Epoch 59/100\n",
      "7663/7663 [==============================] - 8s 989us/step - loss: 0.1732\n",
      "Epoch 60/100\n",
      "7663/7663 [==============================] - 8s 981us/step - loss: 0.1575\n",
      "Epoch 61/100\n",
      "7663/7663 [==============================] - 7s 921us/step - loss: 0.1566\n",
      "Epoch 62/100\n",
      "7663/7663 [==============================] - 7s 937us/step - loss: 0.1501\n",
      "Epoch 63/100\n",
      "7663/7663 [==============================] - 8s 985us/step - loss: 0.1458\n",
      "Epoch 64/100\n",
      "7663/7663 [==============================] - 7s 945us/step - loss: 0.1505\n",
      "Epoch 65/100\n",
      "7663/7663 [==============================] - 7s 958us/step - loss: 0.1511\n",
      "Epoch 66/100\n",
      "7663/7663 [==============================] - 8s 994us/step - loss: 0.1418\n",
      "Epoch 67/100\n",
      "7663/7663 [==============================] - 8s 1ms/step - loss: 0.1453\n",
      "Epoch 68/100\n",
      "7663/7663 [==============================] - 8s 981us/step - loss: 0.1382\n",
      "Epoch 69/100\n",
      "7663/7663 [==============================] - 7s 961us/step - loss: 0.1371\n",
      "Epoch 70/100\n",
      "7663/7663 [==============================] - 7s 964us/step - loss: 0.1316\n",
      "Epoch 71/100\n",
      "7663/7663 [==============================] - 8s 1ms/step - loss: 0.1293\n",
      "Epoch 72/100\n",
      "7663/7663 [==============================] - 8s 994us/step - loss: 0.1273\n",
      "Epoch 73/100\n",
      "7663/7663 [==============================] - 8s 994us/step - loss: 0.1280\n",
      "Epoch 74/100\n",
      "7663/7663 [==============================] - 8s 1ms/step - loss: 0.1544\n",
      "Epoch 75/100\n",
      "7663/7663 [==============================] - 8s 984us/step - loss: 0.1271\n",
      "Epoch 76/100\n",
      "7663/7663 [==============================] - 8s 984us/step - loss: 0.1297\n",
      "Epoch 77/100\n",
      "7663/7663 [==============================] - 7s 961us/step - loss: 0.1230\n",
      "Epoch 78/100\n",
      "7663/7663 [==============================] - 8s 980us/step - loss: 0.1182\n",
      "Epoch 79/100\n",
      "7663/7663 [==============================] - 7s 951us/step - loss: 0.1140\n",
      "Epoch 80/100\n",
      "7663/7663 [==============================] - 7s 962us/step - loss: 0.1072\n",
      "Epoch 81/100\n",
      "7663/7663 [==============================] - 7s 971us/step - loss: 0.1068\n",
      "Epoch 82/100\n",
      "7663/7663 [==============================] - 7s 974us/step - loss: 0.1079\n",
      "Epoch 83/100\n",
      "7663/7663 [==============================] - 7s 978us/step - loss: 0.1035\n",
      "Epoch 84/100\n",
      "7663/7663 [==============================] - 7s 946us/step - loss: 0.1096\n",
      "Epoch 85/100\n",
      "7663/7663 [==============================] - 8s 1ms/step - loss: 0.1088\n",
      "Epoch 86/100\n",
      "7663/7663 [==============================] - 8s 986us/step - loss: 0.1031\n",
      "Epoch 87/100\n",
      "7663/7663 [==============================] - 8s 1ms/step - loss: 0.1354\n",
      "Epoch 88/100\n",
      "7663/7663 [==============================] - 7s 971us/step - loss: 0.1174\n",
      "Epoch 89/100\n",
      "7663/7663 [==============================] - 7s 957us/step - loss: 0.0978\n",
      "Epoch 90/100\n",
      "7663/7663 [==============================] - 7s 966us/step - loss: 0.0938\n",
      "Epoch 91/100\n",
      "7663/7663 [==============================] - 8s 997us/step - loss: 0.0914\n",
      "Epoch 92/100\n",
      "7663/7663 [==============================] - 8s 983us/step - loss: 0.0884\n",
      "Epoch 93/100\n",
      "7663/7663 [==============================] - 7s 967us/step - loss: 0.0857\n",
      "Epoch 94/100\n",
      "7663/7663 [==============================] - 7s 957us/step - loss: 0.0858\n",
      "Epoch 95/100\n",
      "7663/7663 [==============================] - 7s 944us/step - loss: 0.0865\n",
      "Epoch 96/100\n",
      "7663/7663 [==============================] - 7s 952us/step - loss: 0.0854\n",
      "Epoch 97/100\n",
      "7663/7663 [==============================] - 7s 958us/step - loss: 0.0838\n",
      "Epoch 98/100\n",
      "7663/7663 [==============================] - 7s 950us/step - loss: 0.0823\n",
      "Epoch 99/100\n",
      "7663/7663 [==============================] - 7s 941us/step - loss: 0.0823\n",
      "Epoch 100/100\n",
      "7663/7663 [==============================] - 7s 960us/step - loss: 0.0825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1177da550>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doesn't work as well as conv model\n",
    "model = baseline_model()\n",
    "train_x, train_y, test_x, test_y, _ = generate_train_test_splits(vectors, labels, pa_vectors, pa_labels, pa_sample_weights)\n",
    "model.fit(train_x, train_y, sample_weight=[], batch_size=40, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM test error, MSE of log MIC\n",
      "1353/1353 [==============================] - 0s 263us/step\n",
      "0.2881708491403618\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM test error, MSE of log MIC\")\n",
    "print(model.evaluate(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48717642"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(\n",
    "    [(actual - predicted) ** 2 \n",
    "     for actual, predicted in zip(test_y, model.predict(test_x)) if actual < MAX_MIC\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional NN\n",
    "def conv_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv1D(\n",
    "        64,\n",
    "        kernel_size = 5,\n",
    "        strides = 1,\n",
    "        activation = 'relu',\n",
    "        input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10156/10156 [==============================] - 2s 154us/step - loss: 0.7864\n",
      "Epoch 2/100\n",
      "10156/10156 [==============================] - 1s 128us/step - loss: 0.5193\n",
      "Epoch 3/100\n",
      "10156/10156 [==============================] - 1s 129us/step - loss: 0.4679\n",
      "Epoch 4/100\n",
      "10156/10156 [==============================] - 1s 125us/step - loss: 0.4086\n",
      "Epoch 5/100\n",
      "10156/10156 [==============================] - 1s 123us/step - loss: 0.3726\n",
      "Epoch 6/100\n",
      "10156/10156 [==============================] - 1s 124us/step - loss: 0.3318\n",
      "Epoch 7/100\n",
      "10156/10156 [==============================] - 1s 124us/step - loss: 0.3210\n",
      "Epoch 8/100\n",
      "10156/10156 [==============================] - 1s 126us/step - loss: 0.2973\n",
      "Epoch 9/100\n",
      "10156/10156 [==============================] - 1s 124us/step - loss: 0.2804\n",
      "Epoch 10/100\n",
      "10156/10156 [==============================] - 1s 125us/step - loss: 0.2676\n",
      "Epoch 11/100\n",
      "10156/10156 [==============================] - 1s 125us/step - loss: 0.2520\n",
      "Epoch 12/100\n",
      "10156/10156 [==============================] - 1s 124us/step - loss: 0.2421\n",
      "Epoch 13/100\n",
      "10156/10156 [==============================] - 1s 125us/step - loss: 0.2306\n",
      "Epoch 14/100\n",
      "10156/10156 [==============================] - 1s 124us/step - loss: 0.2210\n",
      "Epoch 15/100\n",
      "10156/10156 [==============================] - 1s 130us/step - loss: 0.2118\n",
      "Epoch 16/100\n",
      "10156/10156 [==============================] - 1s 125us/step - loss: 0.1977\n",
      "Epoch 17/100\n",
      "10156/10156 [==============================] - 1s 125us/step - loss: 0.1882\n",
      "Epoch 18/100\n",
      "10156/10156 [==============================] - 1s 126us/step - loss: 0.1857\n",
      "Epoch 19/100\n",
      "10156/10156 [==============================] - 1s 125us/step - loss: 0.1775\n",
      "Epoch 20/100\n",
      "10156/10156 [==============================] - 1s 124us/step - loss: 0.1782\n",
      "Epoch 21/100\n",
      "10156/10156 [==============================] - 1s 126us/step - loss: 0.1673\n",
      "Epoch 22/100\n",
      "10156/10156 [==============================] - 1s 124us/step - loss: 0.1641\n",
      "Epoch 23/100\n",
      "10156/10156 [==============================] - 1s 126us/step - loss: 0.1541\n",
      "Epoch 24/100\n",
      "10156/10156 [==============================] - 1s 125us/step - loss: 0.1529\n",
      "Epoch 25/100\n",
      "10156/10156 [==============================] - 1s 124us/step - loss: 0.1438\n",
      "Epoch 26/100\n",
      "10156/10156 [==============================] - 1s 127us/step - loss: 0.1409\n",
      "Epoch 27/100\n",
      "10156/10156 [==============================] - 1s 128us/step - loss: 0.1443\n",
      "Epoch 28/100\n",
      "10156/10156 [==============================] - 1s 128us/step - loss: 0.1402\n",
      "Epoch 29/100\n",
      "10156/10156 [==============================] - 1s 126us/step - loss: 0.1398\n",
      "Epoch 30/100\n",
      "10156/10156 [==============================] - 1s 130us/step - loss: 0.1288\n",
      "Epoch 31/100\n",
      "10156/10156 [==============================] - 1s 132us/step - loss: 0.1249\n",
      "Epoch 32/100\n",
      "10156/10156 [==============================] - 1s 130us/step - loss: 0.1194\n",
      "Epoch 33/100\n",
      "10156/10156 [==============================] - 1s 131us/step - loss: 0.1191\n",
      "Epoch 34/100\n",
      "10156/10156 [==============================] - 1s 131us/step - loss: 0.1150\n",
      "Epoch 35/100\n",
      "10156/10156 [==============================] - 1s 132us/step - loss: 0.1165\n",
      "Epoch 36/100\n",
      "10156/10156 [==============================] - 1s 133us/step - loss: 0.1141\n",
      "Epoch 37/100\n",
      "10156/10156 [==============================] - 1s 133us/step - loss: 0.1111\n",
      "Epoch 38/100\n",
      "10156/10156 [==============================] - 1s 133us/step - loss: 0.1122\n",
      "Epoch 39/100\n",
      "10156/10156 [==============================] - 1s 132us/step - loss: 0.1061\n",
      "Epoch 40/100\n",
      "10156/10156 [==============================] - 1s 132us/step - loss: 0.1086\n",
      "Epoch 41/100\n",
      "10156/10156 [==============================] - 1s 132us/step - loss: 0.1028\n",
      "Epoch 42/100\n",
      "10156/10156 [==============================] - 1s 140us/step - loss: 0.0998\n",
      "Epoch 43/100\n",
      "10156/10156 [==============================] - 1s 144us/step - loss: 0.0981\n",
      "Epoch 44/100\n",
      "10156/10156 [==============================] - 2s 148us/step - loss: 0.0946\n",
      "Epoch 45/100\n",
      "10156/10156 [==============================] - 1s 137us/step - loss: 0.0951\n",
      "Epoch 46/100\n",
      "10156/10156 [==============================] - 1s 142us/step - loss: 0.0960\n",
      "Epoch 47/100\n",
      "10156/10156 [==============================] - 1s 139us/step - loss: 0.0968\n",
      "Epoch 48/100\n",
      "10156/10156 [==============================] - 1s 121us/step - loss: 0.0942\n",
      "Epoch 49/100\n",
      "10156/10156 [==============================] - 1s 133us/step - loss: 0.0993\n",
      "Epoch 50/100\n",
      "10156/10156 [==============================] - 1s 145us/step - loss: 0.0890\n",
      "Epoch 51/100\n",
      "10156/10156 [==============================] - 1s 139us/step - loss: 0.0853\n",
      "Epoch 52/100\n",
      "10156/10156 [==============================] - 1s 144us/step - loss: 0.0873\n",
      "Epoch 53/100\n",
      "10156/10156 [==============================] - 1s 142us/step - loss: 0.0886\n",
      "Epoch 54/100\n",
      "10156/10156 [==============================] - 1s 141us/step - loss: 0.0867\n",
      "Epoch 55/100\n",
      "10156/10156 [==============================] - 1s 140us/step - loss: 0.0798\n",
      "Epoch 56/100\n",
      "10156/10156 [==============================] - 1s 133us/step - loss: 0.0788\n",
      "Epoch 57/100\n",
      "10156/10156 [==============================] - 2s 153us/step - loss: 0.0852\n",
      "Epoch 58/100\n",
      "10156/10156 [==============================] - 2s 158us/step - loss: 0.0852\n",
      "Epoch 59/100\n",
      "10156/10156 [==============================] - 1s 145us/step - loss: 0.0841\n",
      "Epoch 60/100\n",
      "10156/10156 [==============================] - 1s 147us/step - loss: 0.0816\n",
      "Epoch 61/100\n",
      "10156/10156 [==============================] - 1s 117us/step - loss: 0.0788\n",
      "Epoch 62/100\n",
      "10156/10156 [==============================] - 1s 118us/step - loss: 0.0818\n",
      "Epoch 63/100\n",
      "10156/10156 [==============================] - 1s 139us/step - loss: 0.0807\n",
      "Epoch 64/100\n",
      "10156/10156 [==============================] - 2s 149us/step - loss: 0.0825\n",
      "Epoch 65/100\n",
      "10156/10156 [==============================] - 1s 139us/step - loss: 0.0774\n",
      "Epoch 66/100\n",
      "10156/10156 [==============================] - 1s 128us/step - loss: 0.0833\n",
      "Epoch 67/100\n",
      "10156/10156 [==============================] - 1s 119us/step - loss: 0.0793\n",
      "Epoch 68/100\n",
      "10156/10156 [==============================] - 1s 116us/step - loss: 0.0769\n",
      "Epoch 69/100\n",
      "10156/10156 [==============================] - 1s 119us/step - loss: 0.0807\n",
      "Epoch 70/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0752\n",
      "Epoch 71/100\n",
      "10156/10156 [==============================] - 1s 114us/step - loss: 0.0726\n",
      "Epoch 72/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0730\n",
      "Epoch 73/100\n",
      "10156/10156 [==============================] - 1s 120us/step - loss: 0.0782\n",
      "Epoch 74/100\n",
      "10156/10156 [==============================] - 1s 136us/step - loss: 0.0765\n",
      "Epoch 75/100\n",
      "10156/10156 [==============================] - 1s 121us/step - loss: 0.0788\n",
      "Epoch 76/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0695\n",
      "Epoch 77/100\n",
      "10156/10156 [==============================] - 1s 116us/step - loss: 0.0715\n",
      "Epoch 78/100\n",
      "10156/10156 [==============================] - 1s 122us/step - loss: 0.0702\n",
      "Epoch 79/100\n",
      "10156/10156 [==============================] - 1s 122us/step - loss: 0.0722\n",
      "Epoch 80/100\n",
      "10156/10156 [==============================] - 1s 116us/step - loss: 0.0655\n",
      "Epoch 81/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0668\n",
      "Epoch 82/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0675\n",
      "Epoch 83/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0708\n",
      "Epoch 84/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0662\n",
      "Epoch 85/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0697\n",
      "Epoch 86/100\n",
      "10156/10156 [==============================] - 1s 114us/step - loss: 0.0714\n",
      "Epoch 87/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0677\n",
      "Epoch 88/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0767\n",
      "Epoch 89/100\n",
      "10156/10156 [==============================] - 1s 128us/step - loss: 0.0705\n",
      "Epoch 90/100\n",
      "10156/10156 [==============================] - 1s 117us/step - loss: 0.0660\n",
      "Epoch 91/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0716\n",
      "Epoch 92/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0636\n",
      "Epoch 93/100\n",
      "10156/10156 [==============================] - 1s 117us/step - loss: 0.0608\n",
      "Epoch 94/100\n",
      "10156/10156 [==============================] - 1s 115us/step - loss: 0.0636\n",
      "Epoch 95/100\n",
      "10156/10156 [==============================] - 1s 116us/step - loss: 0.0615\n",
      "Epoch 96/100\n",
      "10156/10156 [==============================] - 1s 117us/step - loss: 0.0644\n",
      "Epoch 97/100\n",
      "10156/10156 [==============================] - 1s 118us/step - loss: 0.0676\n",
      "Epoch 98/100\n",
      "10156/10156 [==============================] - 1s 117us/step - loss: 0.0707\n",
      "Epoch 99/100\n",
      "10156/10156 [==============================] - 1s 123us/step - loss: 0.0741\n",
      "Epoch 100/100\n",
      "10156/10156 [==============================] - 1s 122us/step - loss: 0.0670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10ee0bad0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convmodel = conv_model()\n",
    "train_x, train_y, test_x, test_y, _ = generate_train_test_splits(vectors, labels, pa_vectors, pa_labels, pa_sample_weights)\n",
    "convmodel.fit(train_x, train_y, batch_size=40, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN test error, MSE of log MIC\n",
      "1353/1353 [==============================] - 0s 90us/step\n",
      "0.20619859033748122\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN test error, MSE of log MIC\")\n",
    "print(convmodel.evaluate(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN test error on measured examples only\n",
      "0.35855284\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN test error on measured examples only\")\n",
    "print(np.mean(\n",
    "    [(actual - predicted) ** 2 \n",
    "     for actual, predicted in zip(test_y, convmodel.predict(test_x)) if actual < MAX_MIC\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN test error, MSE of log MIC (RANDOM SHUFFLED SEQUENCE)\n",
      "1353/1353 [==============================] - 0s 101us/step\n",
      "0.42354093437514656\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN test error, MSE of log MIC (RANDOM SHUFFLED SEQUENCE)\")\n",
    "print(convmodel.evaluate(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN test error on measured examples only (RANDOM SHUFFLED SEQUENCE)\n",
      "0.7356594\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN test error on measured examples only (RANDOM SHUFFLED SEQUENCE)\")\n",
    "print(np.mean(\n",
    "    [(actual - predicted) ** 2 \n",
    "     for actual, predicted in zip(test_y, convmodel.predict(test_x)) if actual < MAX_MIC\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales={'Eisenberg':{'A':  0.25, 'R': -1.80, 'N': -0.64,'D': -0.72, 'C':  0.04, 'Q': -0.69,'E': -0.62, 'G':  0.16, 'H': -0.40,'I':  0.73, 'L':  0.53, 'K': -1.10,'M':  0.26, 'F':  0.61, 'P': -0.07,'S': -0.26, 'T': -0.18, 'W':  0.37,'Y':  0.02, 'V':  0.54},\n",
    "'Normalized_consensus':{'A':0.62,'C':0.29,'D':-0.9,'E':-0.74,'F':1.19,'G':0.48,'H':-0.4,'I':1.38,'K':-1.5,'L':1.06,'M':0.64,'N':-0.78,'P':0.12,'Q':-0.85,'R':-2.53,'S':-0.18,'T':-.05,'V':1.08,'W':0.81,'Y':0.26}}\n",
    "\n",
    "def hydrophobic_moment(sequence,scale='Normalized_consensus',angle=0,is_in_degrees=True,normalize=True):\n",
    "    # Angle should be 100 for alpha helix, 180 for beta sheet\n",
    "    hscale=scales[scale]\n",
    "    sin_sum = 0\n",
    "    cos_sum = 0\n",
    "    moment=0\n",
    "    for i in range(len(sequence)):\n",
    "        hp=hscale[sequence[i]]\n",
    "        angle_in_radians=i*angle\n",
    "        if is_in_degrees:\n",
    "            angle_in_radians = (i*angle)*math.pi/180.0\n",
    "        sin_sum += hp*math.sin(angle_in_radians)\n",
    "        cos_sum += hp*math.cos(angle_in_radians)\n",
    "    moment = math.sqrt(sin_sum**2+cos_sum**2)\n",
    "    if normalize:\n",
    "        moment = moment/len(sequence)\n",
    "    return moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli_amps_sequence = ecoli_df['sequence']\n",
    "amp_hydrophobic_moments = ecoli_amps_sequence.apply(hydrophobic_moment)\n",
    "\n",
    "shuffled_ecoli_amps = ecoli_amps_sequence.apply(lambda x: ''.join(random.sample(x, len(x))))\n",
    "# Randomly shuffled AMP sequences\n",
    "shuffled_amp_hydrophobic_moments = shuffled_ecoli_amps.apply(hydrophobic_moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seqs = [generate_random_sequence(character_dict) for i in range(len(amp_hydrophobic_moments))]\n",
    "random_sequences = pd.Series([s['sequence'] for s in random_seqs])\n",
    "random_hydrophobic_moments = random_sequences.apply(hydrophobic_moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrophobic_moments = pd.concat(\n",
    "    [amp_hydrophobic_moments, shuffled_amp_hydrophobic_moments, random_hydrophobic_moments],\n",
    "    axis=1\n",
    ")\n",
    "hydrophobic_moments.columns = ['AMPs', 'Shuffled AMPs', 'Random Peptides']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMPs</th>\n",
       "      <th>Shuffled AMPs</th>\n",
       "      <th>Random Peptides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4508.00</td>\n",
       "      <td>4508.00</td>\n",
       "      <td>4508.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.84</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AMPs  Shuffled AMPs  Random Peptides\n",
       "count 4508.00        4508.00          4508.00\n",
       "mean     0.29           0.29             0.14\n",
       "std      0.23           0.23             0.10\n",
       "min      0.00           0.00             0.00\n",
       "25%      0.12           0.12             0.06\n",
       "50%      0.24           0.24             0.12\n",
       "75%      0.40           0.40             0.20\n",
       "max      1.84           1.84             0.66"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrophobic_moments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  12.,   44.,  154.,  550.,  939., 1014.,  960.,  676.,  100.,\n",
       "          57.]),\n",
       " array([-1.64059182, -1.13673863, -0.63288545, -0.12903227,  0.37482091,\n",
       "         0.8786741 ,  1.38252728,  1.88638046,  2.39023364,  2.89408683,\n",
       "         3.39794001]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([l for l in labels if l < 3.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_peptide(peptide, model):\n",
    "    sequence = peptide['sequence']\n",
    "    cterm = peptide['has_cterminal_modification']\n",
    "    return model.predict(row_to_vector(\n",
    "        {'sequence': sequence, 'has_cterminal_modification': int(cterm)}\n",
    "    ).reshape(-1, MAX_SEQUENCE_LENGTH, len(character_to_index) + 1))\n",
    "\n",
    "\n",
    "def find_nearby_sequences(sequence, old_sequences=None, character_dict=CHARACTER_DICT):\n",
    "    new_sequences = set()\n",
    "    if old_sequences == None:\n",
    "        old_sequences = set()\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "        for c1 in character_dict:\n",
    "            for j in range(i + 1, len(sequence)):\n",
    "                for c2 in character_dict:\n",
    "                    new_sequence = sequence[:i] + c1 + sequence[i+1:j] + c2 + sequence[j+1:]\n",
    "                    for cterm in (True, False):\n",
    "                        ns_dict = {'sequence': new_sequence, 'has_cterminal_modification': cterm}\n",
    "                        new_sequences.add(frozenset(ns_dict.items()))\n",
    "    return old_sequences | new_sequences\n",
    "\n",
    "def evaluate_peptides(peptides, model):\n",
    "    return model.predict(\n",
    "        np.array(\n",
    "            [row_to_vector(dict(p)) for p in peptides]\n",
    "        ).reshape(\n",
    "            -1, MAX_SEQUENCE_LENGTH, len(character_to_index) + 1\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sequence = generate_random_sequence(alphabet=list(CHARACTER_DICT))['sequence']\n",
    "nearbys = find_nearby_sequences(random_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset([('has_cterminal_modification', False), ('sequence', u'MKFSYTNAPPCCTMEHWFYMSCCQNNE')])\n"
     ]
    }
   ],
   "source": [
    "print(min(nearbys, key=lambda x: evaluate_peptide(dict(x), convmodel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.4584532]], dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_peptide({'has_cterminal_modification': False, 'sequence': u'MKFSYTNAPPCCTMEHWFYMSCCQNNE'}, convmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.4827764],\n",
       "       [3.4851692],\n",
       "       [3.489354 ],\n",
       "       ...,\n",
       "       [3.4858   ],\n",
       "       [3.4814298],\n",
       "       [3.480552 ]], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_peptides(nearbys, convmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearby_peptide_vectors(peptide_vector):\n",
    "    nearby_vectors = []\n",
    "    for i in range(len(peptide_vector)):\n",
    "        for j in range(len(peptide_vector[i]) - 1):  # - 1 because of amidation\n",
    "            v = np.zeros(len(peptide_vector[i]))\n",
    "            v[-1] = peptide_vector[0][-1]\n",
    "            v[j] = 1\n",
    "            new_vector = np.concatenate([\n",
    "                peptide_vector[:i],\n",
    "                v.reshape(-1, len(peptide_vector[i])),\n",
    "                peptide_vector[i+1:]\n",
    "            ])\n",
    "            nearby_vectors.append(new_vector)\n",
    "            cterm_flipped = deepcopy(new_vector)\n",
    "            reverse_cterm = (new_vector[0][-1] + 1) % 2\n",
    "            for c in cterm_flipped:\n",
    "                c[-1] = reverse_cterm\n",
    "            nearby_vectors.append(cterm_flipped)\n",
    "    return nearby_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = vectors[0]\n",
    "(np.concatenate([v[:9], v[10:11], v[10:]]) == v).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = nearby_peptide_vectors(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = convmodel.predict(np.array(vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'cterminal_amidation': False, 'sequence': u'ERGAGECCANVWPFTVMAVERKLQDWI'}, 2.4110796)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGAGECCANVWPFTVMAVERKLQDWI'}, 0.7822163)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGAGECCANVWPFTVMAKERKLQDWI'}, 0.38010553)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGAGECCANVWPFTVMAKERKLQNWI'}, 0.00746274)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGAGECCANVKPFTVMAKERKLQNWI'}, -0.25730115)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGAGECCANVKPFTVMAKERKLQNWI'}, -0.39485604)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGAGECCANVKPFTVLAKERKLQNWI'}, -0.49316806)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGAGECCKNVKPFTVLAKERKLQNWI'}, -0.61264855)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGAGECCKNVKPFTVLAKERKLQNWI'}, -0.7046004)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGAGECCKNVKPFTVLAKERKLQKWI'}, -0.81727046)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGACECCKNVKPFTVLAKERKLQKWI'}, -0.924792)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGACECCKNVKPFTVLAKERKLQKWIK'}, -1.0256279)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGACECCKNVKPFTVLAKERKLQKWIKR'}, -1.1634789)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGACECCKFVKPFTVLAKERKLQKWIKR'}, -1.2560542)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGACECIKFVKPFTVLAKERKLQKWIKR'}, -1.3353255)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGACECIKFVKPFTVLAKERKLQKWIKR'}, -1.4077747)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGACECIKFVKPFTVLRKERKLQKWIKR'}, -1.4709682)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGACECIKFVKPFTVLRKERKLQKWIKR'}, -1.5215874)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGACECIKFVKPFTVLRKERKLWKWIKR'}, -1.5661011)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGACECIKFVKPFTVLRKESKLWKWIKR'}, -1.6504989)\n",
      "({'cterminal_amidation': False, 'sequence': u'ERGACECIKFVKPFTVLRGESKLWKWIKR'}, -1.7214439)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRGACECIKFVKPFTVLRGESKLWKWIKR'}, -1.7619109)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRGACECIKFVKPFTVLRGESKLWKWIKRP'}, -1.7971137)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRGACECIKFVKPFTVLRGESKLWKWIKRPI'}, -1.889756)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRGACECIKFVKPFTVLRGESKLWKWIKRPI'}, -1.9276261)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRGACECIKFWKPFTVLRGESKLWKWIKRPI'}, -1.9442887)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACECIKFWKPFTVLRGESKLWKWIKRPI'}, -1.9806907)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0558393)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n",
      "({'cterminal_amidation': False, 'sequence': u'GRSACQCIKFWKPFTVLRGESKLWKWIKRPI'}, -2.0695653)\n"
     ]
    }
   ],
   "source": [
    "s = generate_random_sequence(character_dict)\n",
    "v = row_to_vector(s)\n",
    "for i in range(100):\n",
    "    vs = nearby_peptide_vectors(v)\n",
    "    ps = convmodel.predict(np.array(vs))\n",
    "    best_i = min(range(len(ps)), key=lambda x: ps[x])\n",
    "    v = vs[best_i]\n",
    "    print(vector_to_amp(v), ps[best_i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in v:\n",
    "    a[-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1.])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
