{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "%matplotlib inline\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.manifold import TSNE\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from Bio import pairwise2\n",
    "from Bio.SubsMat import MatrixInfo as matlist\n",
    "import seaborn as sns\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, LSTM, Conv2D, Conv1D, MaxPooling1D, MaxPooling2D, Flatten, ZeroPadding1D, SimpleRNN, Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import json\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CHARACTER_LIST = [u'A', u'C', u'E', u'D', u'G', u'F', u'I', u'H', u'K', u'M', u'L', u'N', u'Q', u'P', u'S', u'R', u'T', u'W', u'V', u'Y']\n",
    "nchar = len(CHARACTER_LIST)\n",
    "CHARACTER_LIST.sort()\n",
    "CHARACTER_DICT = set([u'A', u'C', u'E', u'D', u'G', u'F', u'I', u'H', u'K', u'M', u'L', u'N', u'Q', u'P', u'S', u'R', u'T', u'W', u'V', u'Y'])\n",
    "MAX_SEQUENCE_LENGTH=46\n",
    "MAX_MIC = 4\n",
    "FONT_TO_USE = 'Arial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_embedding(matrix_dict,min_corr=0.99):\n",
    "    distance_matrix = np.zeros([len(CHARACTER_LIST),len(CHARACTER_LIST)])\n",
    "    for i,char1 in enumerate(CHARACTER_LIST):\n",
    "        for j,char2 in enumerate(CHARACTER_LIST):\n",
    "            if (char1,char2) in matrix_dict.keys():\n",
    "                distance_matrix[i,j] = (matrix_dict[(char1,char1)]+matrix_dict[(char2,char2)]-2*matrix_dict[(char1,char2)]+0.)\n",
    "            else:\n",
    "                distance_matrix[i,j] = (matrix_dict[(char1,char1)]+matrix_dict[(char2,char2)]-2*matrix_dict[(char2,char1)]+0.)\n",
    "    G_matrix = np.zeros([nchar,nchar])\n",
    "    for i in range(nchar):\n",
    "        for j in range(nchar):\n",
    "            G_matrix[i,i] += distance_matrix[i,j]**2/nchar\n",
    "            for k in range(nchar):\n",
    "                G_matrix[i,i] -= distance_matrix[j,k]**2/(2*(nchar**2))\n",
    "\n",
    "    for i in range(nchar):\n",
    "        for j in range(nchar):\n",
    "            G_matrix[i,j] = (G_matrix[i,i]+G_matrix[j,j]-distance_matrix[i,j]**2)/2\n",
    "    values,vectors = np.linalg.eigh(G_matrix)\n",
    "    corr = 0\n",
    "    n_dimensions = 0\n",
    "    while corr<min_corr:\n",
    "        n_dimensions += 1\n",
    "        sqrt_lambda_matrix = np.zeros([n_dimensions,n_dimensions])\n",
    "        for i in range(n_dimensions):\n",
    "            sqrt_lambda_matrix[i,i] = np.sqrt(values[i-n_dimensions])\n",
    "        u_matrix = vectors[:,nchar-n_dimensions:nchar]\n",
    "        product = np.matmul(sqrt_lambda_matrix,np.transpose(u_matrix))\n",
    "        embedding_matrix = np.zeros([n_dimensions,nchar])\n",
    "        for i in range(nchar):\n",
    "            for j in range(n_dimensions):\n",
    "                embedding_matrix[j,i] = product[n_dimensions-1-j,i]\n",
    "        embedding_matrix = np.transpose(embedding_matrix)\n",
    "        reconst_dist_matrix = np.zeros([nchar,nchar])\n",
    "        for i in range(nchar):\n",
    "            for j in range(nchar):\n",
    "                to_set = np.linalg.norm(embedding_matrix[i,:]-embedding_matrix[j,:])\n",
    "                reconst_dist_matrix[i,j] = to_set\n",
    "        old=[]\n",
    "        reconst=[]\n",
    "        for i in range(nchar-1):\n",
    "            for j in range(i+1,nchar):\n",
    "                old.append(distance_matrix[i,j])\n",
    "                reconst.append(reconst_dist_matrix[i,j])\n",
    "        corr = np.corrcoef(old,reconst)[0,1]\n",
    "    embedding_dict = {}\n",
    "    for i,char in enumerate(CHARACTER_LIST):\n",
    "        embedding_dict[char] = embedding_matrix[i].tolist()\n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the input vectors for our model\n",
    "# Each vector is two dimensional\n",
    "# The first dimension represents the number of characters in the sequence (46 characters)\n",
    "# Each character is a vector of length equal to the number of groupings of amino acids\n",
    "# This grouping can be 1-1 (each amino acid gets its own group), or coarser\n",
    "def df_to_input_vec(df,shuffle = False,embed_dict=None):\n",
    "    cterminal_amidation = np.array(df.has_cterminal_amidation)\n",
    "\n",
    "    vectors = []\n",
    "    for row in df.iterrows():\n",
    "        vectors.append(row_to_vector(row[1],embed_dict=embed_dict, shuffle_sequence=shuffle))\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "    labels = np.array(df.value)\n",
    "    sample_weights = np.full(len(labels), 1)\n",
    "    return vectors, labels, sample_weights\n",
    "\n",
    "def generate_random_sequence(alphabet, length_of_sequence_min=0, length_of_sequence_max=MAX_SEQUENCE_LENGTH, include_C = True):\n",
    "        sequence = ''\n",
    "        choices = [char for char in alphabet if (include_C or char !='C')]\n",
    "        counter = 0\n",
    "        length_to_use = -10\n",
    "        while counter<20 and (length_to_use < length_of_sequence_min or length_to_use > length_of_sequence_max):\n",
    "            length_to_use = random.choice(SEQ_LENGTHS)\n",
    "            counter += 1\n",
    "        for _ in range(length_to_use):\n",
    "            sequence += random.choice(choices)\n",
    "        has_cterminal_amidation = random.uniform(0, 1)\n",
    "\n",
    "        return {\n",
    "            'sequence': sequence,\n",
    "            'has_cterminal_amidation': has_cterminal_amidation>0.5\n",
    "#             Used to be 0.5 but only 34% of positive data has amidation OOPS\n",
    "        }\n",
    "\n",
    "def add_random_negative_examples(vectors, labels, sample_weights, ratio, max_mic = None, include_cysteine = True):\n",
    "    if not max_mic:\n",
    "        max_mic = max(labels)\n",
    "    # We will add randomly chosen sequences as negative examples\n",
    "    # We will double the length of our training set\n",
    "\n",
    "    len_vectors = ratio * len(vectors)\n",
    "    negative_rows = []\n",
    "    for i in range(len_vectors):\n",
    "        negative_rows.append(row_to_vector(generate_random_sequence(list(CHARACTER_DICT),include_C = include_cysteine)))\n",
    "    negative_vectors = np.array(negative_rows)\n",
    "    vectors = np.concatenate((vectors, negative_vectors))\n",
    "    negative_labels = np.full(len_vectors, max_mic)\n",
    "    labels = np.concatenate((labels, negative_labels))\n",
    "    # Weight all samples equally\n",
    "    sample_weights = np.concatenate((sample_weights, np.full(len_vectors, 1)))\n",
    "    return vectors, labels, sample_weights\n",
    "\n",
    "def generate_train_test_splits(\n",
    "        vectors, labels,\n",
    "        extra_training_vectors=[], extra_training_labels=[], extra_sample_weights=[],\n",
    "        cutoff=0.85\n",
    "):\n",
    "    cutoff = int(cutoff * len(labels))\n",
    "    idx = range(len(vectors))\n",
    "    random.shuffle(idx)\n",
    "    reordered_vectors = vectors[idx]\n",
    "    reordered_labels = labels[idx]\n",
    "    reordered_sample_weights = sample_weights[idx]\n",
    "    if len(extra_training_vectors) > 0:\n",
    "        train_x = np.concatenate((reordered_vectors[:cutoff], extra_training_vectors))\n",
    "        train_y = np.concatenate((reordered_labels[:cutoff], extra_training_labels))\n",
    "        train_sample_weights = np.concatenate((reordered_sample_weights[:cutoff], pa_sample_weights))\n",
    "    else:\n",
    "        train_x = reordered_vectors[:cutoff]\n",
    "        train_y = reordered_labels[:cutoff]\n",
    "        train_sample_weights = reordered_sample_weights[:cutoff]\n",
    "    test_x = reordered_vectors[cutoff:]\n",
    "    test_y = reordered_labels[cutoff:]\n",
    "    return train_x, train_y, test_x, test_y, train_sample_weights\n",
    "\n",
    "# Convolutional NN\n",
    "def conv_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(ZeroPadding1D(\n",
    "        5, input_shape = (MAX_SEQUENCE_LENGTH, embed_length)\n",
    "    ))\n",
    "    model.add(Conv1D(\n",
    "        64,\n",
    "        kernel_size = 5,\n",
    "        strides = 1,\n",
    "        activation = 'relu',\n",
    "        #input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Convolutional NN\n",
    "def conv_model_set_dropout(dropout_level=0.5,embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(ZeroPadding1D(\n",
    "        5, input_shape = (MAX_SEQUENCE_LENGTH, embed_length)\n",
    "    ))\n",
    "    model.add(Conv1D(\n",
    "        64,\n",
    "        kernel_size = 5,\n",
    "        strides = 1,\n",
    "        activation = 'relu',\n",
    "        #input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(dropout_level))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Convolutional NN\n",
    "def conv_model_move_dropout(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(ZeroPadding1D(\n",
    "        5, input_shape = (MAX_SEQUENCE_LENGTH, embed_length)\n",
    "    ))\n",
    "    model.add(Conv1D(\n",
    "        64,\n",
    "        kernel_size = 5,\n",
    "        strides = 1,\n",
    "        activation = 'relu',\n",
    "        #input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Convolutional NN\n",
    "def one_layer_conv_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(ZeroPadding1D(\n",
    "        5, input_shape = (MAX_SEQUENCE_LENGTH, embed_length)\n",
    "    ))\n",
    "    model.add(Conv1D(\n",
    "        64,\n",
    "        kernel_size = 5,\n",
    "        strides = 1,\n",
    "        activation = 'relu',\n",
    "        #input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def lstm_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(LSTM(\n",
    "        64,\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, embed_length),\n",
    "    ))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, kernel_initializer='normal'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, kernel_initializer='normal'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def two_layer_recurrent_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(SimpleRNN(\n",
    "        64,\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, embed_length),return_sequences=True\n",
    "    ))\n",
    "    model.add(SimpleRNN(64))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, kernel_initializer='normal'))\n",
    "    model.add(Dense(20, kernel_initializer='normal'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def three_layer_recurrent_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(SimpleRNN(\n",
    "        64,\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, embed_length),return_sequences=True\n",
    "    ))\n",
    "    model.add(SimpleRNN(64,return_sequences=True))\n",
    "    model.add(SimpleRNN(64))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, kernel_initializer='normal'))\n",
    "    model.add(Dense(20, kernel_initializer='normal'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def one_layer_recurrent_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(SimpleRNN(\n",
    "        64,\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, embed_length),return_sequences=False\n",
    "    ))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, kernel_initializer='normal'))\n",
    "    model.add(Dense(20, kernel_initializer='normal'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def bilstm_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        64\n",
    "    ),input_shape=(MAX_SEQUENCE_LENGTH, embed_length)))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, kernel_initializer='normal'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, kernel_initializer='normal'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def birecurrent_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(Bidirectional(SimpleRNN(\n",
    "        64,return_sequences=False\n",
    "    ),input_shape=(MAX_SEQUENCE_LENGTH, embed_length)))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, kernel_initializer='normal'))\n",
    "    model.add(Dense(20, kernel_initializer='normal'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self,models,predict_method,max_mic_buffer=0.1):\n",
    "        self.models = models\n",
    "        self.predict_method = predict_method\n",
    "        self.max_mic_buffer = max_mic_buffer\n",
    "        \n",
    "    def combine_predictions(self,predictions):\n",
    "        if self.predict_method is 'average':\n",
    "            return np.mean(predictions)\n",
    "        elif self.predict_method is 'classify_then_average':\n",
    "            actual_predictions = []\n",
    "            for prediction in predictions:\n",
    "                if prediction < MAX_MIC - self.max_mic_buffer:\n",
    "                    actual_predictions.append(prediction)\n",
    "            if float(len(actual_predictions))/float(len(predictions))>=0.49:\n",
    "                return np.mean(predictions)\n",
    "            else:\n",
    "                return MAX_MIC\n",
    "        else:\n",
    "            print 'predict_method not recognized'\n",
    "            return -100\n",
    "        \n",
    "    def predict(self,test_x):\n",
    "        all_predictions = []\n",
    "        combined_predictions = []\n",
    "        for model in self.models:\n",
    "            all_predictions.append(model.predict(test_x))\n",
    "        for i in range(len(test_x)):\n",
    "            combined_predictions.append(self.combine_predictions([all_predictions[k][i] for k in range(len(self.models))]))\n",
    "        return combined_predictions\n",
    "    \n",
    "    def evaluate(self,test_x,test_y):\n",
    "        predictions = self.predict(test_x)\n",
    "        correctly_classified_error = np.mean([(actual - predicted) ** 2 for actual, predicted in zip(test_y, predictions) if actual < MAX_MIC and predicted < MAX_MIC - self.max_mic_buffer])    \n",
    "        all_error = np.mean([(actual - predicted) ** 2 for actual, predicted in zip(test_y, predictions)])    \n",
    "        all_active_error = np.mean([(actual - predicted) ** 2 for actual, predicted in zip(test_y, predictions) if actual < MAX_MIC])    \n",
    "        return correctly_classified_error,all_active_error, all_error\n",
    "    \n",
    "    def evaluate_as_classifier(self,test_x,test_y):\n",
    "        true_positives=0\n",
    "        true_negatives=0\n",
    "        false_positives=0\n",
    "        false_negatives=0\n",
    "        all_predicted=self.predict(test_x)\n",
    "        for i in range(len(test_y)):\n",
    "            actual=test_y[i]\n",
    "            predicted=all_predicted[i]\n",
    "            if actual<MAX_MIC-0.0001:\n",
    "                if predicted<MAX_MIC - self.max_mic_buffer:\n",
    "                    true_positives+=1\n",
    "                else:\n",
    "                    false_negatives+=1\n",
    "            else:\n",
    "                if predicted<MAX_MIC - self.max_mic_buffer:\n",
    "                    false_positives += 1\n",
    "        #             print vector_to_amp(test_x[i])\n",
    "        #             print 'predicted: '+repr(predicted)+', actual: '+repr(actual)\n",
    "#                     print '>p'+repr(false_positives)+'_'+repr(predicted)\n",
    "#                     print vector_to_amp(test_x[i])['sequence'].replace('_','')\n",
    "                else:\n",
    "                    true_negatives += 1\n",
    "        return true_positives,true_negatives,false_positives,false_negatives\n",
    "        \n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Enter an element of a result dictionary into df-ready row\n",
    "# Standardize units of MIC\n",
    "def standardize_to_uM(concentration, unit, sequence):\n",
    "    concentration = concentration.replace(' ', '')\n",
    "    try:\n",
    "        concentration = float(concentration)\n",
    "    except:\n",
    "        return None\n",
    "    if unit == 'uM' or unit == u'\\xb5M' or unit == u'uM)':\n",
    "        return concentration\n",
    "    elif unit == 'ug/ml' or unit == u'\\xb5g/ml' or unit == u'ug/ml)':\n",
    "        try:\n",
    "            molWt = ProteinAnalysis(sequence).molecular_weight()\n",
    "        except ValueError:\n",
    "            return None\n",
    "        return concentration * 1000/molWt\n",
    "    elif unit == 'nmol/g' or unit == 'pmol/mg':\n",
    "        #1g, at density of 1g/mL, is 1mL, so nmol/g is nmol/mL = umol/L = uM yay!\n",
    "        return concentration\n",
    "    else:\n",
    "        # print 'Unit not recognized: ' + unit\n",
    "        return None\n",
    "    \n",
    "def convert_result_to_rows(sequence, result):\n",
    "    rows = []\n",
    "    if 'bacteria' not in result:\n",
    "        return rows\n",
    "    for bacterium, strain in result['bacteria']:\n",
    "        \n",
    "        rows.append({\n",
    "            'bacterium': bacterium,\n",
    "            'strain': strain,\n",
    "            'sequence': sequence.upper(),\n",
    "            'url_source': result['url_sources'][0],\n",
    "            'value': standardize_to_uM(\n",
    "                result['bacteria'][(bacterium, strain)]['value'],\n",
    "                result['bacteria'][(bacterium, strain)]['unit'],\n",
    "                sequence\n",
    "            ),\n",
    "            'modifications': result['modifications'] if 'modifications' in result else [],\n",
    "            'unit': 'uM'\n",
    "        })\n",
    "        if rows[-1]['value']:\n",
    "            rows[-1]['value'] = np.log10(rows[-1]['value'])\n",
    "    return rows\n",
    "\n",
    "# Remove sequences with amino acids that aren't well-defined\n",
    "def strip_sequences_with_char(df, bad_char):\n",
    "    return df[~df.sequence.str.contains(bad_char)]\n",
    "\n",
    "# We'll want to strip off any sequences with modifications that could be hard to replicate\n",
    "# Their effects are too complex for the model\n",
    "def is_modified(modifications_list):\n",
    "    return len(modifications_list) > 0\n",
    "\n",
    "# However, C-Terminal Amidation is common enough that we make an exception\n",
    "CTERM_AMIDATION_TERMS = ['C-Terminal amidation','C-Terminus: AMD','C-Terminal','C-termianal amidation']\n",
    "\n",
    "def has_non_cterminal_modification(modifications_list):\n",
    "    return any(['C-Term' not in modification for modification in modifications_list])\n",
    "\n",
    "def has_unusual_modification(modifications_list):\n",
    "    return any([is_uncommon_modification(mod) for mod in modifications_list])\n",
    "\n",
    "def has_cterminal_amidation(modifications_list):\n",
    "    return any([is_cterminal_amidation(mod) for mod in modifications_list])\n",
    "\n",
    "def has_disulfide_bonds(modifications_list):\n",
    "    return any([is_disulfide_bond(mod) for mod in modifications_list])\n",
    "\n",
    "def is_cterminal_amidation(mod):\n",
    "    for term in CTERM_AMIDATION_TERMS:\n",
    "        if term in mod:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_disulfide_bond(mod):\n",
    "    return 'disulfide' in mod.lower()\n",
    "\n",
    "def is_uncommon_modification(mod):\n",
    "    return (not is_cterminal_amidation(mod)) and (not is_disulfide_bond(mod))\n",
    "\n",
    "def datasource_has_modifications(cell):\n",
    "    # Everything except CAMP and YADAMP has modification data\n",
    "    return not any([s in cell for s in no_modification_data_sources])\n",
    "\n",
    "def sequence_has_modification_data(cell):\n",
    "    # If the sequence is labeled modifictationless in another database it's OK\n",
    "    return cell in sequences_containing_modifications\n",
    "\n",
    "# Each amino acid its own group\n",
    "character_to_index = {\n",
    "    (character): i\n",
    "    for i, character in enumerate(CHARACTER_DICT)\n",
    "}\n",
    "\n",
    "index2character = {\n",
    "    value: key\n",
    "    for key, value in character_to_index.items()\n",
    "}\n",
    "\n",
    "def sequence_to_vector(sequence, cterminal_amidation,embed_dict=None):\n",
    "# It looks like this truncates any sequence after max_sequence_length (which is length of 95th percentile longest peptide)\n",
    "# I just add cterminal amidation as the amino acid after the last real amino acid (if the amino acid gets truncated\n",
    "# then the cterminal amidation also gets cut off)\n",
    "    if embed_dict==None:\n",
    "        default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "            default[i][character_to_index[character]] = 1\n",
    "        if len(sequence)<MAX_SEQUENCE_LENGTH:\n",
    "            default[len(sequence)][-1]=cterminal_amidation\n",
    "    else:\n",
    "        default = np.zeros([MAX_SEQUENCE_LENGTH,len(embed_dict['A'])+1])\n",
    "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "            embedding = deepcopy(embed_dict[character])\n",
    "            embedding.append(0)\n",
    "            for k,val in enumerate(embedding):\n",
    "                default[i,k] = val\n",
    "            if len(sequence)<MAX_SEQUENCE_LENGTH:\n",
    "                for i in range(len(sequence),MAX_SEQUENCE_LENGTH):\n",
    "                    default[i,-1]=1\n",
    "    return default\n",
    "\n",
    "def find_character(character2index, character):\n",
    "    for key in character2index:\n",
    "        if character in key:\n",
    "            return character2index[key]\n",
    "    return -2\n",
    "\n",
    "def row_to_vector(row, embed_dict=None,shuffle_sequence=False):\n",
    "    sequence = list(row['sequence'])\n",
    "    if shuffle_sequence:\n",
    "        random.shuffle(sequence)\n",
    "    cterminal_amidation = row['has_cterminal_amidation']\n",
    "    return sequence_to_vector(sequence,cterminal_amidation,embed_dict=embed_dict)\n",
    "\n",
    "def old_row_to_vector(row, shuffle_sequence=False):\n",
    "    sequence = list(row['sequence'])\n",
    "    if shuffle_sequence:\n",
    "        random.shuffle(sequence)\n",
    "    cterminal_amidation = row['has_cterminal_amidation']\n",
    "    default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "\n",
    "    for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "        default[i][find_character(character_to_index, character)] = 1\n",
    "        default[i][-1] = cterminal_amidation\n",
    "\n",
    "    return default\n",
    "\n",
    "def vector_to_amp(vector):\n",
    "    sequence = ''\n",
    "    has_cterm = False\n",
    "    for v in vector:\n",
    "        nonzeros = np.argwhere(v[:len(character_to_index)])\n",
    "        if len(nonzeros) > 1:\n",
    "            print(\"?????\")\n",
    "        elif len(nonzeros) == 0:\n",
    "            sequence += '_'\n",
    "        else:\n",
    "            sequence += index2character[np.argwhere(v)[0][0]]  # First one\n",
    "        if v[-1]>0:\n",
    "            has_cterm=True\n",
    "    return {\n",
    "        'sequence': sequence,\n",
    "        'cterminal_amidation': has_cterm\n",
    "    }\n",
    "\n",
    "def bacterium_to_sample_weight(bacterium, intended_bacterium='E. coli'):\n",
    "    if intended_bacterium in bacterium:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.5\n",
    "    \n",
    "def containing_bacterium(bacterium, df):\n",
    "    return df.loc[df.bacterium.str.contains(bacterium)]\n",
    "\n",
    "def average_over_databases(bacterium_df):\n",
    "    return bacterium_df.groupby('sequence')['value'].mean().dropna()\n",
    "\n",
    "def get_bacterium_df(bacterium, df):\n",
    "    bdf = df.loc[(df.bacterium.str.contains(bacterium))].groupby(['sequence', 'bacterium'])\n",
    "    return bdf.mean().reset_index().dropna()\n",
    "\n",
    "def strip_bad_amino_acids(df, bad_amino_acids=('U', 'X', 'Z')):\n",
    "    for b in bad_amino_acids:\n",
    "        df = df.loc[~df.sequence.str.contains(b)]\n",
    "    return df.reset_index()\n",
    "\n",
    "def split_dataframe(df_to_split,cutoff=0.85):\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    for i in range(len(df_to_split)):\n",
    "        if 'C' not in df_to_split['sequence'][i] and random.random()>cutoff:\n",
    "            test_indices.append(i)\n",
    "        else:\n",
    "            train_indices.append(i)\n",
    "    train_df = df_to_split.iloc[train_indices]\n",
    "    test_df = df_to_split.iloc[test_indices]\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ecoli_train_with_c = pd.read_pickle('Saved_variables/ecoli_train_with_c_df.pkl')\n",
    "ecoli_train_no_c = pd.read_pickle('Saved_variables/ecoli_train_no_c_df.pkl')\n",
    "ecoli_test = pd.read_pickle('Saved_variables/ecoli_test_df.pkl')\n",
    "ecoli_df = pd.read_pickle('Saved_variables/ecoli_all_df.pkl')\n",
    "ecoli_df_no_c = pd.read_pickle('Saved_variables/ecoli_all_no_c_df.pkl')\n",
    "all_df = pd.read_pickle('Saved_variables/all_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b62_embedding = generate_embedding(matlist.blosum62,0.997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ecoli_train_input_embed = df_to_input_vec(ecoli_train_no_c,embed_dict=b62_embedding)\n",
    "ecoli_train_input_one_hot = df_to_input_vec(ecoli_train_no_c)\n",
    "ecoli_train_input_one_hot_ridge = [vec.flatten() for vec in ecoli_train_input_one_hot[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ecoli_train_no_c = ecoli_train_no_c.reset_index(drop=True)\n",
    "ecoli_test = ecoli_test.reset_index(drop=True)\n",
    "train_x_aa_counts = []\n",
    "train_x = []\n",
    "train_y = []\n",
    "train_x_ridge=[]\n",
    "test_x = []\n",
    "test_x_aa_counts = []\n",
    "test_y = []\n",
    "test_x_ridge=[]\n",
    "lengths_1=[]\n",
    "lengths_2=[]\n",
    "for i in range(len(ecoli_train_no_c)):\n",
    "    lengths_1.append(len(ecoli_train_no_c.sequence[i]))\n",
    "    if len(ecoli_train_no_c.sequence[i])<MAX_SEQUENCE_LENGTH:\n",
    "        train_x_aa_counts.append(sequence_to_aa_counts(ecoli_train_no_c.sequence[i],ecoli_train_no_c.has_cterminal_amidation[i])) \n",
    "        train_y.append(ecoli_train_no_c.value[i])\n",
    "        train_x.append(sequence_to_vector(ecoli_train_no_c.sequence[i],ecoli_train_no_c.has_cterminal_amidation[i]))\n",
    "        train_x_ridge.append(sequence_to_vector(ecoli_train_no_c.sequence[i],ecoli_train_no_c.has_cterminal_amidation[i]).flatten())\n",
    "        lengths_2.append(len(ecoli_train_no_c.sequence[i]))\n",
    "for i in range(len(ecoli_test)):\n",
    "    if len(ecoli_test.sequence[i])<MAX_SEQUENCE_LENGTH:\n",
    "        test_x_aa_counts.append(sequence_to_aa_counts(ecoli_test.sequence[i],ecoli_test.has_cterminal_amidation[i])) \n",
    "        test_y.append(ecoli_test.value[i])\n",
    "        test_x.append(sequence_to_vector(ecoli_test.sequence[i],ecoli_test.has_cterminal_amidation[i]))\n",
    "        test_x_ridge.append(sequence_to_vector(ecoli_test.sequence[i],ecoli_test.has_cterminal_amidation[i]).flatten())\n",
    "        \n",
    "train_x = np.array(train_x)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression on the whole 21x46 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "alphas_to_test = [0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "ridge_train_one_hot = RidgeCV(alphas=alphas_to_test)\n",
    "ridge_train_one_hot.fit(train_x_ridge,train_y)\n",
    "print ridge_train_one_hot.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n"
     ]
    }
   ],
   "source": [
    "alphas_to_test = [0.0001,2,4,8,16,24,26,27,28,29,30,31,32,36,40,64]\n",
    "ridge_train_one_hot = RidgeCV(alphas=alphas_to_test)\n",
    "ridge_train_one_hot.fit(train_x_ridge,train_y)\n",
    "print ridge_train_one_hot.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_predicted = ridge_train_one_hot.predict(test_x_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8jzbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13d20ded0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6294660347356246\n",
      "0.43212294643519295\n",
      "MSE: 0.3618631143319073\n"
     ]
    }
   ],
   "source": [
    "plt.plot(ridge_predicted,test_y,'.')\n",
    "plt.show()\n",
    "print np.corrcoef(ridge_predicted,test_y)[0,1]\n",
    "tau,pval = kendalltau(ridge_predicted,test_y)\n",
    "print tau\n",
    "print 'MSE: '+repr(float(np.average([(ridge_predicted[i]-test_y[i])**2 for i in range(len(test_y))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using just the amino acid counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_to_aa_counts(sequence,cterm_amidation):\n",
    "    to_return = []\n",
    "    for char in CHARACTER_LIST:\n",
    "        to_return.append(sequence.count(char))\n",
    "    if cterm_amidation>0.5:\n",
    "        to_return.append(1)\n",
    "    else:\n",
    "        to_return.append(0)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "alphas_to_test = [0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "ridge_train_counts = RidgeCV(alphas=alphas_to_test)\n",
    "ridge_train_counts.fit(test_x_aa_counts,test_y)\n",
    "print ridge_train_counts.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.0\n"
     ]
    }
   ],
   "source": [
    "alphas_to_test = [0.0001,20,40,60,70,75,78,80,81,82,83,85,90,100,120,140,160,320,640]\n",
    "ridge_train_counts = RidgeCV(alphas=alphas_to_test)\n",
    "ridge_train_counts.fit(test_x_aa_counts,test_y)\n",
    "print ridge_train_counts.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridge_predicted_counts = ridge_train_counts.predict(test_x_aa_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFHdCCOkgFHdCCOkg/x/swnGyHlg6kwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13e86fad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ridge_predicted_counts,test_y,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.595300022943397\n",
      "0.40678085628613286\n",
      "MSE: 0.3884282175075926\n"
     ]
    }
   ],
   "source": [
    "print np.corrcoef(ridge_predicted_counts,test_y)[0,1]\n",
    "tau,pval = kendalltau(ridge_predicted_counts,test_y)\n",
    "print tau\n",
    "print 'MSE: '+repr(float(np.average([(ridge_predicted_counts[i]-test_y[i])**2 for i in range(len(test_y))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network analysis and comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2840/2840 [==============================] - 10s 4ms/step - loss: 0.8779\n",
      "Epoch 2/300\n",
      "2840/2840 [==============================] - 7s 3ms/step - loss: 0.4895\n",
      "Epoch 3/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.4331\n",
      "Epoch 4/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.4329\n",
      "Epoch 5/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.4306\n",
      "Epoch 6/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.4195\n",
      "Epoch 7/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.4192\n",
      "Epoch 8/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.4109\n",
      "Epoch 9/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.4032\n",
      "Epoch 10/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3985\n",
      "Epoch 11/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3919\n",
      "Epoch 12/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3871\n",
      "Epoch 13/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3935\n",
      "Epoch 14/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3828\n",
      "Epoch 15/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3817\n",
      "Epoch 16/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3751\n",
      "Epoch 17/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3832\n",
      "Epoch 18/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3700\n",
      "Epoch 19/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3793\n",
      "Epoch 20/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3715\n",
      "Epoch 21/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3600\n",
      "Epoch 22/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3543\n",
      "Epoch 23/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3565\n",
      "Epoch 24/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3532\n",
      "Epoch 25/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3458\n",
      "Epoch 26/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3409\n",
      "Epoch 27/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3434\n",
      "Epoch 28/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3400\n",
      "Epoch 29/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3353\n",
      "Epoch 30/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3266\n",
      "Epoch 31/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3309\n",
      "Epoch 32/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3251\n",
      "Epoch 33/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3237\n",
      "Epoch 34/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3119\n",
      "Epoch 35/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3082\n",
      "Epoch 36/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3074\n",
      "Epoch 37/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3023\n",
      "Epoch 38/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3006\n",
      "Epoch 39/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2998\n",
      "Epoch 40/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2968\n",
      "Epoch 41/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.3041\n",
      "Epoch 42/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2909\n",
      "Epoch 43/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2897\n",
      "Epoch 44/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2878\n",
      "Epoch 45/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2830\n",
      "Epoch 46/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2743\n",
      "Epoch 47/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2742\n",
      "Epoch 48/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2717\n",
      "Epoch 49/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2677\n",
      "Epoch 50/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2712\n",
      "Epoch 51/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2613\n",
      "Epoch 52/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2526\n",
      "Epoch 53/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2541\n",
      "Epoch 54/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2510\n",
      "Epoch 55/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2489\n",
      "Epoch 56/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2508\n",
      "Epoch 57/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2414\n",
      "Epoch 58/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2355\n",
      "Epoch 59/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2340\n",
      "Epoch 60/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2283\n",
      "Epoch 61/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2250\n",
      "Epoch 62/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2244\n",
      "Epoch 63/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2266\n",
      "Epoch 64/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2269\n",
      "Epoch 65/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2226\n",
      "Epoch 66/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2150\n",
      "Epoch 67/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2113\n",
      "Epoch 68/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2150\n",
      "Epoch 69/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2108\n",
      "Epoch 70/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2061\n",
      "Epoch 71/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2023\n",
      "Epoch 72/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2018\n",
      "Epoch 73/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.2061\n",
      "Epoch 74/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1970\n",
      "Epoch 75/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1928\n",
      "Epoch 76/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1897\n",
      "Epoch 77/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1952\n",
      "Epoch 78/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1884\n",
      "Epoch 79/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1910\n",
      "Epoch 80/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1821\n",
      "Epoch 81/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1807\n",
      "Epoch 82/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1803\n",
      "Epoch 83/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1767\n",
      "Epoch 84/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1800\n",
      "Epoch 85/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1811\n",
      "Epoch 86/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.1690\n",
      "Epoch 87/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1659\n",
      "Epoch 88/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1716\n",
      "Epoch 89/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1632\n",
      "Epoch 90/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1620\n",
      "Epoch 91/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1678\n",
      "Epoch 92/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1682\n",
      "Epoch 93/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1596\n",
      "Epoch 94/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1577\n",
      "Epoch 95/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1595\n",
      "Epoch 96/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1515\n",
      "Epoch 97/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1500\n",
      "Epoch 98/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1554\n",
      "Epoch 99/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1479\n",
      "Epoch 100/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1531\n",
      "Epoch 101/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1483\n",
      "Epoch 102/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1455\n",
      "Epoch 103/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1420\n",
      "Epoch 104/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1456\n",
      "Epoch 105/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1484\n",
      "Epoch 106/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1403\n",
      "Epoch 107/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1373\n",
      "Epoch 108/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1337\n",
      "Epoch 109/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1373\n",
      "Epoch 110/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1373\n",
      "Epoch 111/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1332\n",
      "Epoch 112/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1292\n",
      "Epoch 113/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1309\n",
      "Epoch 114/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1320\n",
      "Epoch 115/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.1298\n",
      "Epoch 116/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1279\n",
      "Epoch 117/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1273\n",
      "Epoch 118/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1216\n",
      "Epoch 119/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1238\n",
      "Epoch 120/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1215\n",
      "Epoch 121/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1213\n",
      "Epoch 122/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1170\n",
      "Epoch 123/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.1207\n",
      "Epoch 124/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.1194\n",
      "Epoch 125/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1157\n",
      "Epoch 126/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.1160\n",
      "Epoch 127/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1130\n",
      "Epoch 128/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.1102\n",
      "Epoch 129/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.1185\n",
      "Epoch 130/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.1142\n",
      "Epoch 131/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.1139\n",
      "Epoch 132/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.1137\n",
      "Epoch 133/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1069\n",
      "Epoch 134/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1099\n",
      "Epoch 135/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1069\n",
      "Epoch 136/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1041\n",
      "Epoch 137/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0991\n",
      "Epoch 138/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.1061\n",
      "Epoch 139/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.1018\n",
      "Epoch 140/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0990\n",
      "Epoch 141/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0984\n",
      "Epoch 142/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0982\n",
      "Epoch 143/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0999\n",
      "Epoch 144/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0935\n",
      "Epoch 145/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0899\n",
      "Epoch 146/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0986\n",
      "Epoch 147/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0900\n",
      "Epoch 148/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0977\n",
      "Epoch 149/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0914\n",
      "Epoch 150/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0896\n",
      "Epoch 151/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0900\n",
      "Epoch 152/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0907\n",
      "Epoch 153/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0925\n",
      "Epoch 154/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0920\n",
      "Epoch 155/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0900\n",
      "Epoch 156/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0893\n",
      "Epoch 157/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0878\n",
      "Epoch 158/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0876\n",
      "Epoch 159/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0841\n",
      "Epoch 160/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0821\n",
      "Epoch 161/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0841\n",
      "Epoch 162/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0832\n",
      "Epoch 163/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0823\n",
      "Epoch 164/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0879\n",
      "Epoch 165/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0782\n",
      "Epoch 166/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0779\n",
      "Epoch 167/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0797\n",
      "Epoch 168/300\n",
      "2840/2840 [==============================] - 10s 4ms/step - loss: 0.0756\n",
      "Epoch 169/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0767\n",
      "Epoch 170/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0783\n",
      "Epoch 171/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0712\n",
      "Epoch 172/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0739\n",
      "Epoch 173/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0724\n",
      "Epoch 174/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0749\n",
      "Epoch 175/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0718\n",
      "Epoch 176/300\n",
      "2840/2840 [==============================] - 12s 4ms/step - loss: 0.0713\n",
      "Epoch 177/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0711\n",
      "Epoch 178/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0690\n",
      "Epoch 179/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0763\n",
      "Epoch 180/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0744\n",
      "Epoch 181/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0694\n",
      "Epoch 182/300\n",
      "2840/2840 [==============================] - 10s 4ms/step - loss: 0.0723\n",
      "Epoch 183/300\n",
      "2840/2840 [==============================] - 12s 4ms/step - loss: 0.0741\n",
      "Epoch 184/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0707\n",
      "Epoch 185/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0719\n",
      "Epoch 186/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0674\n",
      "Epoch 187/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0662\n",
      "Epoch 188/300\n",
      "2840/2840 [==============================] - 10s 4ms/step - loss: 0.0618\n",
      "Epoch 189/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0611\n",
      "Epoch 190/300\n",
      "2840/2840 [==============================] - 10s 4ms/step - loss: 0.0636\n",
      "Epoch 191/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0628\n",
      "Epoch 192/300\n",
      "2840/2840 [==============================] - 10s 4ms/step - loss: 0.0657\n",
      "Epoch 193/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0651\n",
      "Epoch 194/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0722\n",
      "Epoch 195/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0655\n",
      "Epoch 196/300\n",
      "2840/2840 [==============================] - 13s 5ms/step - loss: 0.0611\n",
      "Epoch 197/300\n",
      "2840/2840 [==============================] - 10s 4ms/step - loss: 0.0608\n",
      "Epoch 198/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0592\n",
      "Epoch 199/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0602\n",
      "Epoch 200/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0631\n",
      "Epoch 201/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0561\n",
      "Epoch 202/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0579\n",
      "Epoch 203/300\n",
      "2840/2840 [==============================] - 12s 4ms/step - loss: 0.0565\n",
      "Epoch 204/300\n",
      "2840/2840 [==============================] - 12s 4ms/step - loss: 0.0563\n",
      "Epoch 205/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0602\n",
      "Epoch 206/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0554\n",
      "Epoch 207/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0548\n",
      "Epoch 208/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0580\n",
      "Epoch 209/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0587\n",
      "Epoch 210/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0610\n",
      "Epoch 211/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0691\n",
      "Epoch 212/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0589\n",
      "Epoch 213/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0551\n",
      "Epoch 214/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0532\n",
      "Epoch 215/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0514\n",
      "Epoch 216/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0512\n",
      "Epoch 217/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0517\n",
      "Epoch 218/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0504\n",
      "Epoch 219/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0500\n",
      "Epoch 220/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0455\n",
      "Epoch 221/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0533\n",
      "Epoch 222/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0501\n",
      "Epoch 223/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0518\n",
      "Epoch 224/300\n",
      "2840/2840 [==============================] - 10s 4ms/step - loss: 0.0516\n",
      "Epoch 225/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0502\n",
      "Epoch 226/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0474\n",
      "Epoch 227/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0505\n",
      "Epoch 228/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0463\n",
      "Epoch 229/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0498\n",
      "Epoch 230/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0448\n",
      "Epoch 231/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0461\n",
      "Epoch 232/300\n",
      "2840/2840 [==============================] - 13s 5ms/step - loss: 0.0461\n",
      "Epoch 233/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0448\n",
      "Epoch 234/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0457\n",
      "Epoch 235/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0463\n",
      "Epoch 236/300\n",
      "2840/2840 [==============================] - 10s 4ms/step - loss: 0.0461\n",
      "Epoch 237/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0477\n",
      "Epoch 238/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0454\n",
      "Epoch 239/300\n",
      "2840/2840 [==============================] - 13s 5ms/step - loss: 0.0507\n",
      "Epoch 240/300\n",
      "2840/2840 [==============================] - 12s 4ms/step - loss: 0.0530\n",
      "Epoch 241/300\n",
      "2840/2840 [==============================] - 10s 4ms/step - loss: 0.0460\n",
      "Epoch 242/300\n",
      "2840/2840 [==============================] - 10s 4ms/step - loss: 0.0442\n",
      "Epoch 243/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0461\n",
      "Epoch 244/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0441\n",
      "Epoch 245/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0435\n",
      "Epoch 246/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0470\n",
      "Epoch 247/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0470\n",
      "Epoch 248/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0439\n",
      "Epoch 249/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0476\n",
      "Epoch 250/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0440\n",
      "Epoch 251/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0441\n",
      "Epoch 252/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0407\n",
      "Epoch 253/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0440\n",
      "Epoch 254/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0421\n",
      "Epoch 255/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0390\n",
      "Epoch 256/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0448\n",
      "Epoch 257/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0501\n",
      "Epoch 258/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0430\n",
      "Epoch 259/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0418\n",
      "Epoch 260/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0404\n",
      "Epoch 261/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0412\n",
      "Epoch 262/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0445\n",
      "Epoch 263/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0435\n",
      "Epoch 264/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0417\n",
      "Epoch 265/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0407\n",
      "Epoch 266/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0389\n",
      "Epoch 267/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0405\n",
      "Epoch 268/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0424\n",
      "Epoch 269/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0395\n",
      "Epoch 270/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0472\n",
      "Epoch 271/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0409\n",
      "Epoch 272/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0369\n",
      "Epoch 273/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0364\n",
      "Epoch 274/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0366\n",
      "Epoch 275/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0380\n",
      "Epoch 276/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0380\n",
      "Epoch 277/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0349\n",
      "Epoch 278/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0381\n",
      "Epoch 279/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0345\n",
      "Epoch 280/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0363\n",
      "Epoch 281/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0377\n",
      "Epoch 282/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0376\n",
      "Epoch 283/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0363\n",
      "Epoch 284/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0388\n",
      "Epoch 285/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0378\n",
      "Epoch 286/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0375\n",
      "Epoch 287/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0370\n",
      "Epoch 288/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0342\n",
      "Epoch 289/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0361\n",
      "Epoch 290/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0345\n",
      "Epoch 291/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0362\n",
      "Epoch 292/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0352\n",
      "Epoch 293/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0363\n",
      "Epoch 294/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0330\n",
      "Epoch 295/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0348\n",
      "Epoch 296/300\n",
      "2840/2840 [==============================] - 8s 3ms/step - loss: 0.0366\n",
      "Epoch 297/300\n",
      "2840/2840 [==============================] - 10s 3ms/step - loss: 0.0335\n",
      "Epoch 298/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0313\n",
      "Epoch 299/300\n",
      "2840/2840 [==============================] - 9s 3ms/step - loss: 0.0319\n",
      "Epoch 300/300\n",
      "2840/2840 [==============================] - 11s 4ms/step - loss: 0.0335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13e86f590>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv_model_normal = conv_model()\n",
    "# conv_model_normal.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_one_layer = one_layer_conv_model()\n",
    "# conv_model_one_layer.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_moved_dropout = conv_model_move_dropout()\n",
    "# conv_model_moved_dropout.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_p9_dropout = conv_model_set_dropout(dropout_level=0.9)\n",
    "# conv_model_p9_dropout.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_p7_dropout = conv_model_set_dropout(dropout_level=0.7)\n",
    "# conv_model_p7_dropout.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_p3_dropout = conv_model_set_dropout(dropout_level=0.3)\n",
    "# conv_model_p3_dropout.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# rnn_one_layer = one_layer_recurrent_model()\n",
    "# rnn_one_layer.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# rnn_two_layers = two_layer_recurrent_model()\n",
    "# rnn_two_layers.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# rnn_three_layers = three_layer_recurrent_model()\n",
    "# rnn_three_layers.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# lstm_mod = lstm_model()\n",
    "# lstm_mod.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# bi_rnn = birecurrent_model()\n",
    "# bi_rnn.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# bi_lstm = bilstm_model()\n",
    "# bi_lstm.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "bi_lstm_300 = bilstm_model()\n",
    "bi_lstm_300.fit(train_x,train_y,batch_size=40,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    predicted = model.predict(test_x)\n",
    "    predicted = [pred[0] for pred in predicted]\n",
    "#     plt.plot(predicted,test_y,'.')\n",
    "#     plt.show()\n",
    "    print 'MSE: '+repr(float(np.average([(predicted[i]-test_y[i])**2 for i in range(len(test_y))])))\n",
    "    print 'Pearson Correlation: '+repr(float(np.corrcoef(predicted,test_y)[1,0]))\n",
    "    tau,pval = kendalltau(predicted,test_y)\n",
    "    print 'Kendall tau Correlation: '+repr(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2630187586654806\n",
      "Pearson Correlation: 0.7627362163345807\n",
      "Kendall tau Correlation: 0.5612715504824447\n",
      "MSE: 0.2584475916238433\n",
      "Pearson Correlation: 0.7576183875498322\n",
      "Kendall tau Correlation: 0.5621919192451926\n",
      "MSE: 0.291193032757182\n",
      "Pearson Correlation: 0.7172390341786713\n",
      "Kendall tau Correlation: 0.5130571447707726\n",
      "MSE: 0.2917004792824526\n",
      "Pearson Correlation: 0.724370655856039\n",
      "Kendall tau Correlation: 0.5228744115734171\n",
      "MSE: 0.26593463689024033\n",
      "Pearson Correlation: 0.748867914454111\n",
      "Kendall tau Correlation: 0.5449794086997665\n",
      "MSE: 0.2623273522133445\n",
      "Pearson Correlation: 0.7536921366086584\n",
      "Kendall tau Correlation: 0.5539893344824567\n",
      "MSE: 0.5466149751475964\n",
      "Pearson Correlation: 0.32500182724698656\n",
      "Kendall tau Correlation: 0.2162947326559626\n",
      "MSE: 0.5999652913986262\n",
      "Pearson Correlation: 0.1057590638773826\n",
      "Kendall tau Correlation: 0.10807681930124084\n",
      "MSE: 0.5993964651540502\n",
      "Pearson Correlation: 0.09598610295951966\n",
      "Kendall tau Correlation: 0.02198220377758971\n",
      "MSE: 0.3408747920881625\n",
      "Pearson Correlation: 0.6625625031919099\n",
      "Kendall tau Correlation: 0.46298585471320547\n",
      "MSE: 0.3455119336724046\n",
      "Pearson Correlation: 0.6850405017148212\n",
      "Kendall tau Correlation: 0.4801983652586316\n",
      "MSE: 0.3157173849810306\n",
      "Pearson Correlation: 0.7026241435942544\n",
      "Kendall tau Correlation: 0.4938101348550615\n",
      "MSE: 0.3220775906229344\n",
      "Pearson Correlation: 0.711561888969169\n",
      "Kendall tau Correlation: 0.5012053785978431\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(conv_model_normal)\n",
    "evaluate_model(conv_model_one_layer)\n",
    "evaluate_model(conv_model_moved_dropout)\n",
    "evaluate_model(conv_model_p9_dropout)\n",
    "evaluate_model(conv_model_p7_dropout)\n",
    "evaluate_model(conv_model_p3_dropout)\n",
    "evaluate_model(rnn_one_layer)\n",
    "evaluate_model(rnn_two_layers)\n",
    "evaluate_model(rnn_three_layers)\n",
    "evaluate_model(lstm_mod)\n",
    "evaluate_model(bi_rnn)\n",
    "evaluate_model(bi_lstm)\n",
    "evaluate_model(bi_lstm_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2895/2895 [==============================] - 1s 418us/step - loss: 0.6799\n",
      "Epoch 2/100\n",
      "2895/2895 [==============================] - 1s 312us/step - loss: 0.4690\n",
      "Epoch 3/100\n",
      "2895/2895 [==============================] - 1s 306us/step - loss: 0.3912\n",
      "Epoch 4/100\n",
      "2895/2895 [==============================] - 1s 305us/step - loss: 0.3743\n",
      "Epoch 5/100\n",
      "2895/2895 [==============================] - 1s 307us/step - loss: 0.3451\n",
      "Epoch 6/100\n",
      "2895/2895 [==============================] - 1s 302us/step - loss: 0.3310\n",
      "Epoch 7/100\n",
      "2895/2895 [==============================] - 1s 308us/step - loss: 0.3065\n",
      "Epoch 8/100\n",
      "2895/2895 [==============================] - 1s 322us/step - loss: 0.2910\n",
      "Epoch 9/100\n",
      "2895/2895 [==============================] - 1s 316us/step - loss: 0.2822\n",
      "Epoch 10/100\n",
      "2895/2895 [==============================] - 1s 424us/step - loss: 0.2745\n",
      "Epoch 11/100\n",
      "2895/2895 [==============================] - 1s 329us/step - loss: 0.2470\n",
      "Epoch 12/100\n",
      "2895/2895 [==============================] - 1s 367us/step - loss: 0.2415\n",
      "Epoch 13/100\n",
      "2895/2895 [==============================] - 1s 348us/step - loss: 0.2357\n",
      "Epoch 14/100\n",
      "2895/2895 [==============================] - 1s 332us/step - loss: 0.2205\n",
      "Epoch 15/100\n",
      "2895/2895 [==============================] - 1s 339us/step - loss: 0.2213\n",
      "Epoch 16/100\n",
      "2895/2895 [==============================] - 1s 399us/step - loss: 0.2137\n",
      "Epoch 17/100\n",
      "2895/2895 [==============================] - 1s 372us/step - loss: 0.2055\n",
      "Epoch 18/100\n",
      "2895/2895 [==============================] - 1s 410us/step - loss: 0.2090\n",
      "Epoch 19/100\n",
      "2895/2895 [==============================] - 1s 354us/step - loss: 0.2063\n",
      "Epoch 20/100\n",
      "2895/2895 [==============================] - 1s 359us/step - loss: 0.1937\n",
      "Epoch 21/100\n",
      "2895/2895 [==============================] - 1s 364us/step - loss: 0.1944\n",
      "Epoch 22/100\n",
      "2895/2895 [==============================] - 1s 346us/step - loss: 0.1917\n",
      "Epoch 23/100\n",
      "2895/2895 [==============================] - 1s 379us/step - loss: 0.1804\n",
      "Epoch 24/100\n",
      "2895/2895 [==============================] - 1s 417us/step - loss: 0.1762\n",
      "Epoch 25/100\n",
      "2895/2895 [==============================] - 1s 406us/step - loss: 0.1721\n",
      "Epoch 26/100\n",
      "2895/2895 [==============================] - 1s 342us/step - loss: 0.1653\n",
      "Epoch 27/100\n",
      "2895/2895 [==============================] - 1s 403us/step - loss: 0.1712\n",
      "Epoch 28/100\n",
      "2895/2895 [==============================] - 1s 347us/step - loss: 0.1687\n",
      "Epoch 29/100\n",
      "2895/2895 [==============================] - 1s 408us/step - loss: 0.1598\n",
      "Epoch 30/100\n",
      "2895/2895 [==============================] - 1s 391us/step - loss: 0.1560\n",
      "Epoch 31/100\n",
      "2895/2895 [==============================] - 1s 364us/step - loss: 0.1512\n",
      "Epoch 32/100\n",
      "2895/2895 [==============================] - 1s 402us/step - loss: 0.1516\n",
      "Epoch 33/100\n",
      "2895/2895 [==============================] - 1s 379us/step - loss: 0.1453\n",
      "Epoch 34/100\n",
      "2895/2895 [==============================] - 1s 370us/step - loss: 0.1484\n",
      "Epoch 35/100\n",
      "2895/2895 [==============================] - 1s 391us/step - loss: 0.1480\n",
      "Epoch 36/100\n",
      "2895/2895 [==============================] - 1s 380us/step - loss: 0.1434\n",
      "Epoch 37/100\n",
      "2895/2895 [==============================] - 1s 381us/step - loss: 0.1326\n",
      "Epoch 38/100\n",
      "2895/2895 [==============================] - 1s 367us/step - loss: 0.1406\n",
      "Epoch 39/100\n",
      "2895/2895 [==============================] - 1s 396us/step - loss: 0.1387\n",
      "Epoch 40/100\n",
      "2895/2895 [==============================] - 1s 390us/step - loss: 0.1331\n",
      "Epoch 41/100\n",
      "2895/2895 [==============================] - 1s 397us/step - loss: 0.1308\n",
      "Epoch 42/100\n",
      "2895/2895 [==============================] - 1s 370us/step - loss: 0.1305\n",
      "Epoch 43/100\n",
      "2895/2895 [==============================] - 1s 374us/step - loss: 0.1263\n",
      "Epoch 44/100\n",
      "2895/2895 [==============================] - 1s 378us/step - loss: 0.1222\n",
      "Epoch 45/100\n",
      "2895/2895 [==============================] - 1s 396us/step - loss: 0.1170\n",
      "Epoch 46/100\n",
      "2895/2895 [==============================] - 1s 383us/step - loss: 0.1183\n",
      "Epoch 47/100\n",
      "2895/2895 [==============================] - 1s 381us/step - loss: 0.1300\n",
      "Epoch 48/100\n",
      "2895/2895 [==============================] - 1s 372us/step - loss: 0.1194\n",
      "Epoch 49/100\n",
      "2895/2895 [==============================] - 1s 402us/step - loss: 0.1178\n",
      "Epoch 50/100\n",
      "2895/2895 [==============================] - 1s 422us/step - loss: 0.1256\n",
      "Epoch 51/100\n",
      "2895/2895 [==============================] - 1s 420us/step - loss: 0.1145\n",
      "Epoch 52/100\n",
      "2895/2895 [==============================] - 1s 401us/step - loss: 0.1192\n",
      "Epoch 53/100\n",
      "2895/2895 [==============================] - 1s 382us/step - loss: 0.1123\n",
      "Epoch 54/100\n",
      "2895/2895 [==============================] - 1s 390us/step - loss: 0.1163\n",
      "Epoch 55/100\n",
      "2895/2895 [==============================] - 1s 381us/step - loss: 0.1071\n",
      "Epoch 56/100\n",
      "2895/2895 [==============================] - 1s 369us/step - loss: 0.1058\n",
      "Epoch 57/100\n",
      "2895/2895 [==============================] - 1s 369us/step - loss: 0.1094\n",
      "Epoch 58/100\n",
      "2895/2895 [==============================] - 1s 368us/step - loss: 0.1029\n",
      "Epoch 59/100\n",
      "2895/2895 [==============================] - 1s 376us/step - loss: 0.1031\n",
      "Epoch 60/100\n",
      "2895/2895 [==============================] - 1s 397us/step - loss: 0.1074\n",
      "Epoch 61/100\n",
      "2895/2895 [==============================] - 1s 392us/step - loss: 0.1046\n",
      "Epoch 62/100\n",
      "2895/2895 [==============================] - 1s 409us/step - loss: 0.1067\n",
      "Epoch 63/100\n",
      "2895/2895 [==============================] - 1s 364us/step - loss: 0.1042\n",
      "Epoch 64/100\n",
      "2895/2895 [==============================] - 1s 379us/step - loss: 0.1045\n",
      "Epoch 65/100\n",
      "2895/2895 [==============================] - 1s 383us/step - loss: 0.1080\n",
      "Epoch 66/100\n",
      "2895/2895 [==============================] - 1s 379us/step - loss: 0.0930\n",
      "Epoch 67/100\n",
      "2895/2895 [==============================] - 1s 381us/step - loss: 0.0989\n",
      "Epoch 68/100\n",
      "2895/2895 [==============================] - 1s 415us/step - loss: 0.0935\n",
      "Epoch 69/100\n",
      "2895/2895 [==============================] - 1s 385us/step - loss: 0.0931\n",
      "Epoch 70/100\n",
      "2895/2895 [==============================] - 1s 415us/step - loss: 0.1005\n",
      "Epoch 71/100\n",
      "2895/2895 [==============================] - 1s 390us/step - loss: 0.0965\n",
      "Epoch 72/100\n",
      "2895/2895 [==============================] - 1s 452us/step - loss: 0.0913\n",
      "Epoch 73/100\n",
      "2895/2895 [==============================] - 1s 415us/step - loss: 0.0913\n",
      "Epoch 74/100\n",
      "2895/2895 [==============================] - 1s 434us/step - loss: 0.0936\n",
      "Epoch 75/100\n",
      "2895/2895 [==============================] - 1s 407us/step - loss: 0.0894\n",
      "Epoch 76/100\n",
      "2895/2895 [==============================] - 1s 395us/step - loss: 0.0931\n",
      "Epoch 77/100\n",
      "2895/2895 [==============================] - 1s 417us/step - loss: 0.0923\n",
      "Epoch 78/100\n",
      "2895/2895 [==============================] - 1s 407us/step - loss: 0.0881\n",
      "Epoch 79/100\n",
      "2895/2895 [==============================] - 1s 416us/step - loss: 0.0902\n",
      "Epoch 80/100\n",
      "2895/2895 [==============================] - 1s 397us/step - loss: 0.0896\n",
      "Epoch 81/100\n",
      "2895/2895 [==============================] - 1s 398us/step - loss: 0.0919\n",
      "Epoch 82/100\n",
      "2895/2895 [==============================] - 1s 387us/step - loss: 0.0866\n",
      "Epoch 83/100\n",
      "2895/2895 [==============================] - 1s 395us/step - loss: 0.0959\n",
      "Epoch 84/100\n",
      "2895/2895 [==============================] - 1s 368us/step - loss: 0.0963\n",
      "Epoch 85/100\n",
      "2895/2895 [==============================] - 1s 401us/step - loss: 0.0868\n",
      "Epoch 86/100\n",
      "2895/2895 [==============================] - 1s 403us/step - loss: 0.0823\n",
      "Epoch 87/100\n",
      "2895/2895 [==============================] - 1s 391us/step - loss: 0.0892\n",
      "Epoch 88/100\n",
      "2895/2895 [==============================] - 1s 408us/step - loss: 0.0831\n",
      "Epoch 89/100\n",
      "2895/2895 [==============================] - 1s 400us/step - loss: 0.0841\n",
      "Epoch 90/100\n",
      "2895/2895 [==============================] - 1s 395us/step - loss: 0.0863\n",
      "Epoch 91/100\n",
      "2895/2895 [==============================] - 1s 390us/step - loss: 0.0797\n",
      "Epoch 92/100\n",
      "2895/2895 [==============================] - 1s 387us/step - loss: 0.0810\n",
      "Epoch 93/100\n",
      "2895/2895 [==============================] - 1s 393us/step - loss: 0.0853\n",
      "Epoch 94/100\n",
      "2895/2895 [==============================] - 1s 378us/step - loss: 0.0824\n",
      "Epoch 95/100\n",
      "2895/2895 [==============================] - 1s 395us/step - loss: 0.0792\n",
      "Epoch 96/100\n",
      "2895/2895 [==============================] - 1s 393us/step - loss: 0.0755\n",
      "Epoch 97/100\n",
      "2895/2895 [==============================] - 1s 395us/step - loss: 0.0796\n",
      "Epoch 98/100\n",
      "2895/2895 [==============================] - 1s 405us/step - loss: 0.0788\n",
      "Epoch 99/100\n",
      "2895/2895 [==============================] - 1s 406us/step - loss: 0.0748\n",
      "Epoch 100/100\n",
      "2895/2895 [==============================] - 1s 402us/step - loss: 0.0794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1223aaa50>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convmodel_embed = conv_model(embed_length = (len(b62_embedding['A'])+1))\n",
    "convmodel_embed.fit(ecoli_train_input_embed[0],ecoli_train_input_embed[1],batch_size=40,epochs=100)\n",
    "convmodel_one_hot = conv_model()\n",
    "convmodel_one_hot.fit(ecoli_train_input_one_hot[0],ecoli_train_input_one_hot[1],batch_size=40,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.7644\n",
      "Epoch 2/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.6333\n",
      "Epoch 3/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6238\n",
      "Epoch 4/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.6278\n",
      "Epoch 5/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6215\n",
      "Epoch 6/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6221\n",
      "Epoch 7/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6224\n",
      "Epoch 8/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6169\n",
      "Epoch 9/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6325\n",
      "Epoch 10/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6218\n",
      "Epoch 11/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6216\n",
      "Epoch 12/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6166\n",
      "Epoch 13/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6257\n",
      "Epoch 14/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6109\n",
      "Epoch 15/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6182\n",
      "Epoch 16/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6177\n",
      "Epoch 17/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6087\n",
      "Epoch 18/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6187\n",
      "Epoch 19/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6092\n",
      "Epoch 20/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5920\n",
      "Epoch 21/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5842\n",
      "Epoch 22/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5915\n",
      "Epoch 23/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.5761\n",
      "Epoch 24/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.5729\n",
      "Epoch 25/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5650\n",
      "Epoch 26/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5481\n",
      "Epoch 27/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6265\n",
      "Epoch 28/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.6441\n",
      "Epoch 29/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5609\n",
      "Epoch 30/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5572\n",
      "Epoch 31/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5362\n",
      "Epoch 32/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5140\n",
      "Epoch 33/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5091\n",
      "Epoch 34/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5328\n",
      "Epoch 35/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5623\n",
      "Epoch 36/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5280\n",
      "Epoch 37/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5188\n",
      "Epoch 38/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.4988\n",
      "Epoch 39/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.5060\n",
      "Epoch 40/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.4752\n",
      "Epoch 41/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.4644\n",
      "Epoch 42/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.4550\n",
      "Epoch 43/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.4705\n",
      "Epoch 44/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4543\n",
      "Epoch 45/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4420\n",
      "Epoch 46/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4309\n",
      "Epoch 47/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.4324\n",
      "Epoch 48/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.4236\n",
      "Epoch 49/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.4425\n",
      "Epoch 50/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4073\n",
      "Epoch 51/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4002\n",
      "Epoch 52/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4034\n",
      "Epoch 53/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.5278\n",
      "Epoch 54/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.5222\n",
      "Epoch 55/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.4729\n",
      "Epoch 56/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4343\n",
      "Epoch 57/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4283\n",
      "Epoch 58/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.4045\n",
      "Epoch 59/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4031\n",
      "Epoch 60/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.3892\n",
      "Epoch 61/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4266\n",
      "Epoch 62/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4075\n",
      "Epoch 63/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.3886\n",
      "Epoch 64/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.3818\n",
      "Epoch 65/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.3720\n",
      "Epoch 66/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.3685\n",
      "Epoch 67/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.3585\n",
      "Epoch 68/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.3571\n",
      "Epoch 69/100\n",
      "2895/2895 [==============================] - 6s 2ms/step - loss: 0.3576\n",
      "Epoch 70/100\n",
      "2895/2895 [==============================] - 6s 2ms/step - loss: 0.3462\n",
      "Epoch 71/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.3699\n",
      "Epoch 72/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.3406\n",
      "Epoch 73/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.3523\n",
      "Epoch 74/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.3573\n",
      "Epoch 75/100\n",
      "2895/2895 [==============================] - 6s 2ms/step - loss: 0.3516\n",
      "Epoch 76/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.3360\n",
      "Epoch 77/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.3435\n",
      "Epoch 78/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.3312\n",
      "Epoch 79/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.3173\n",
      "Epoch 80/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.3171\n",
      "Epoch 81/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.3102\n",
      "Epoch 82/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.3113\n",
      "Epoch 83/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.3119\n",
      "Epoch 84/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.2984\n",
      "Epoch 85/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.3017\n",
      "Epoch 86/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.2906\n",
      "Epoch 87/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.2988\n",
      "Epoch 88/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.2851\n",
      "Epoch 89/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.2962\n",
      "Epoch 90/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.2818\n",
      "Epoch 91/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.2748\n",
      "Epoch 92/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.2978\n",
      "Epoch 93/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.2822\n",
      "Epoch 94/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.2717\n",
      "Epoch 95/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.2815\n",
      "Epoch 96/100\n",
      "2895/2895 [==============================] - 4s 1ms/step - loss: 0.4034\n",
      "Epoch 97/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.3584\n",
      "Epoch 98/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.3085\n",
      "Epoch 99/100\n",
      "2895/2895 [==============================] - 5s 2ms/step - loss: 0.3133\n",
      "Epoch 100/100\n",
      "2895/2895 [==============================] - 4s 2ms/step - loss: 0.2884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128a4cfd0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnnmodel_one_hot = recurrent_model()\n",
    "rnnmodel_one_hot.fit(ecoli_train_input_one_hot[0],ecoli_train_input_one_hot[1],batch_size=40,epochs=100)\n",
    "# convmodel_one_hot = conv_model()\n",
    "# convmodel_one_hot.fit(ecoli_train_input_one_hot[0],ecoli_train_input_one_hot[1],batch_size=40,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 1s 1ms/step\n",
      "0.4065361327413023\n"
     ]
    }
   ],
   "source": [
    "print rnnmodel_one_hot.evaluate(test_x_one_hot,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ecoli_test_input_embed = df_to_input_vec(ecoli_test,embed_dict=b62_embedding)\n",
    "ecoli_test_input_one_hot = df_to_input_vec(ecoli_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 0s 369us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2674829605637459"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x_embed = ecoli_test_input_embed[0]\n",
    "test_x_one_hot = ecoli_test_input_one_hot[0]\n",
    "test_y = ecoli_test_input_embed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 0s 152us/step\n",
      "0.2674829605637459\n",
      "509/509 [==============================] - 0s 150us/step\n",
      "0.27840658735432655\n",
      "509/509 [==============================] - 1s 1ms/step\n",
      "0.32876440074682706\n"
     ]
    }
   ],
   "source": [
    "print convmodel_embed_one_hot.evaluate(test_x_one_hot,test_y)\n",
    "print convmodel_embed.evaluate(test_x_embed,test_y)\n",
    "print rnnmodel_embed.evaluate(test_x_embed,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 0s 327us/step\n",
      "0.2674829605637459\n",
      "509/509 [==============================] - 0s 240us/step\n",
      "0.27840658735432655\n",
      "509/509 [==============================] - 0s 756us/step\n",
      "0.357846174938037\n"
     ]
    }
   ],
   "source": [
    "print convmodel_embed_one_hot.evaluate(test_x_one_hot,test_y)\n",
    "print convmodel_embed.evaluate(test_x_embed,test_y)\n",
    "print rnnmodel_embed.evaluate(test_x_embed,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(509, 1)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convmodel_embed_one_hot.predict(test_x_one_hot).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQUgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a3d2310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(convmodel_embed_one_hot.predict(test_x_one_hot),test_y,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-a4849a744172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconvmodel_embed_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jacobwitten/.local/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36mcorrcoef\u001b[0;34m(x, y, rowvar, bias, ddof)\u001b[0m\n\u001b[1;32m   3173\u001b[0m         warnings.warn('bias and ddof have no effect and are deprecated',\n\u001b[1;32m   3174\u001b[0m                       DeprecationWarning, stacklevel=2)\n\u001b[0;32m-> 3175\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3176\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3177\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jacobwitten/.local/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36mcov\u001b[0;34m(m, y, rowvar, bias, ddof, fweights, aweights)\u001b[0m\n\u001b[1;32m   3042\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrowvar\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3043\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3044\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mddof\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "np.corrcoef(test_y,convmodel_embed_one_hot.predict(test_x_one_hot).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thing=convmodel_embed_one_hot.predict(test_x_one_hot).tolist()\n",
    "pred_y = [thing[i][0] for i in range(len(thing))]\n",
    "testy = [test_y[i] for i in range(len(test_y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509\n",
      "509\n",
      "[[1.         0.75093691]\n",
      " [0.75093691 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print len(pred_y)\n",
    "print len(testy)\n",
    "print np.corrcoef(np.array([pred_y,test_y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00018078088760375977], [0.804699718952179], [0.41527310013771057], [0.65059494972229], [0.2572363615036011], [0.6929062008857727], [0.6024726629257202], [1.3883743286132812], [0.4464077651500702], [2.974808931350708], [0.24348585307598114], [0.8983182311058044], [0.5257531404495239], [0.6965836882591248], [1.5194125175476074], [0.7731612920761108], [1.1550123691558838], [0.9067221283912659], [1.3859975337982178], [1.0199246406555176], [1.1168525218963623], [1.0979042053222656], [1.7597465515136719], [0.26230213046073914], [1.6937675476074219], [0.4183138906955719], [0.38670843839645386], [1.4011256694793701], [0.42569535970687866], [-0.4939499497413635], [0.8234257102012634], [1.7153122425079346], [1.5677518844604492], [2.03965163230896], [0.29985901713371277], [1.457822322845459], [1.2377314567565918], [2.1349802017211914], [0.8958069086074829], [1.537076473236084], [0.6268340945243835], [2.0093462467193604], [1.1671497821807861], [1.6529231071472168], [1.3630664348602295], [1.1016184091567993], [0.3281174600124359], [0.9944605231285095], [0.5011463165283203], [-0.18272927403450012], [1.1745684146881104], [1.1463905572891235], [1.7821683883666992], [1.9451115131378174], [1.5235426425933838], [0.5210536122322083], [0.7486298084259033], [0.6729632019996643], [0.5560232996940613], [1.1927149295806885], [1.8359508514404297], [1.098649501800537], [1.1386756896972656], [1.5589988231658936], [0.8211011290550232], [1.5161938667297363], [1.2799170017242432], [1.7621910572052002], [1.6096620559692383], [1.8141732215881348], [0.7203980684280396], [1.1689801216125488], [2.1942074298858643], [1.0533367395401], [0.7762662768363953], [2.0215020179748535], [1.662395715713501], [1.0761687755584717], [0.8917363286018372], [0.45499396324157715], [1.7907745838165283], [1.821821689605713], [0.8314787745475769], [1.5647532939910889], [1.8968896865844727], [1.619335412979126], [1.4186980724334717], [1.346482753753662], [1.4352836608886719], [1.8019542694091797], [0.6981168985366821], [1.8554790019989014], [1.233443021774292], [1.8639600276947021], [1.9413235187530518], [2.2171504497528076], [0.890221357345581], [0.25217756628990173], [0.5186382532119751], [1.5312821865081787], [1.442197561264038], [1.264542579650879], [1.4806504249572754], [1.5111324787139893], [1.1896650791168213], [1.5505032539367676], [1.4769036769866943], [1.6452391147613525], [1.4222745895385742], [0.6435399651527405], [0.337074339389801], [1.6378443241119385], [0.3944638967514038], [1.2322649955749512], [0.9860270023345947], [1.2796046733856201], [1.2406082153320312], [-0.670314610004425], [-0.08595259487628937], [0.6539418697357178], [0.7763939499855042], [0.5764710903167725], [1.677597999572754], [1.8933203220367432], [1.73756742477417], [1.6473588943481445], [1.3950371742248535], [1.1950044631958008], [0.5226811766624451], [1.174506664276123], [1.4533274173736572], [1.3306350708007812], [0.8063163757324219], [1.0215988159179688], [0.35392093658447266], [0.6308058500289917], [0.4689386487007141], [0.6786109209060669], [0.7517009377479553], [0.5063212513923645], [1.6372158527374268], [1.9168853759765625], [1.6825144290924072], [2.2284915447235107], [1.5471222400665283], [1.5792152881622314], [0.9789133071899414], [1.5188672542572021], [1.503939151763916], [0.8312429785728455], [1.531158208847046], [0.7608324885368347], [1.2609641551971436], [0.9533557891845703], [1.4878840446472168], [1.9013187885284424], [1.2661373615264893], [1.440730094909668], [1.225066900253296], [1.1331138610839844], [1.2245910167694092], [0.5809824466705322], [0.917136013507843], [1.7652814388275146], [1.6679537296295166], [0.9627380967140198], [1.582230567932129], [1.6768641471862793], [0.08684422075748444], [1.1710805892944336], [0.6623997092247009], [1.1771998405456543], [0.42810821533203125], [0.17528602480888367], [0.18638119101524353], [1.3856959342956543], [1.3858973979949951], [2.163431406021118], [0.5403112173080444], [0.6935017704963684], [0.16379880905151367], [0.8146031498908997], [0.7717056274414062], [0.6546413898468018], [0.9474939703941345], [0.8077051639556885], [0.8850446939468384], [0.6191043853759766], [0.8056928515434265], [0.14109370112419128], [1.4854848384857178], [1.2984416484832764], [0.7867262363433838], [0.964419424533844], [0.7230356335639954], [2.269047498703003], [0.9964696764945984], [0.7819361090660095], [1.7412807941436768], [1.0553982257843018], [1.4030671119689941], [1.3263492584228516], [1.685579776763916], [1.5818066596984863], [1.662405014038086], [1.6059236526489258], [1.3034567832946777], [2.079282760620117], [1.2220423221588135], [0.6500633955001831], [1.3363449573516846], [1.6904921531677246], [1.7586781978607178], [0.2144729495048523], [0.9193097352981567], [0.1623724102973938], [0.559642493724823], [0.5252727270126343], [0.532757580280304], [0.984655499458313], [0.7913649082183838], [1.0940523147583008], [0.8742902874946594], [0.8394737839698792], [2.822845220565796], [2.737567663192749], [1.434093713760376], [1.4064598083496094], [0.7383788824081421], [0.6547987461090088], [1.326378345489502], [1.100609540939331], [1.2325520515441895], [1.819338321685791], [0.7851389646530151], [1.5362913608551025], [1.066558599472046], [0.6733260750770569], [1.7791166305541992], [1.7971737384796143], [1.7509877681732178], [1.930494785308838], [1.9381747245788574], [0.9397557377815247], [1.6062736511230469], [2.1090872287750244], [1.0885019302368164], [1.0559163093566895], [0.7024278044700623], [1.1164913177490234], [1.3924789428710938], [0.5500661730766296], [0.7767711281776428], [1.190075397491455], [1.9186692237854004], [1.384662389755249], [0.8683189749717712], [1.1008244752883911], [1.1581518650054932], [1.3998260498046875], [1.9905366897583008], [1.947528600692749], [1.6469645500183105], [0.7102531790733337], [1.7602872848510742], [1.0046979188919067], [1.4584145545959473], [1.243318796157837], [0.33253833651542664], [1.459792137145996], [1.548274278640747], [1.6029455661773682], [0.9809299111366272], [0.5626882910728455], [0.4332731366157532], [0.9948320984840393], [0.6089463233947754], [0.47314637899398804], [1.4975292682647705], [0.5662586688995361], [0.0008024275302886963], [1.8833489418029785], [0.24962972104549408], [1.8793041706085205], [2.067253828048706], [0.41959941387176514], [0.3551916480064392], [1.2900493144989014], [0.8549152612686157], [1.2572250366210938], [0.6637967228889465], [1.0239108800888062], [1.7618706226348877], [0.928898274898529], [0.8497412204742432], [0.8056288957595825], [1.3865852355957031], [0.49852555990219116], [0.8728703856468201], [1.5739350318908691], [1.4783341884613037], [1.1659247875213623], [1.9061052799224854], [0.6552753448486328], [0.6701744198799133], [1.1681139469146729], [0.8295626640319824], [1.5394718647003174], [1.9978995323181152], [1.050023078918457], [1.5331621170043945], [0.9971872568130493], [1.090635061264038], [-0.0015603601932525635], [-0.6975345611572266], [0.570259153842926], [0.24206647276878357], [0.19368945062160492], [1.4277997016906738], [0.8735064268112183], [2.1422433853149414], [0.7862956523895264], [0.9488881230354309], [0.3321835994720459], [1.577664852142334], [1.563950777053833], [1.995591163635254], [1.5772273540496826], [1.468825340270996], [1.2229304313659668], [0.10623680055141449], [-0.06508505344390869], [1.321509838104248], [-0.0650206208229065], [-0.2928019165992737], [-0.1492331624031067], [0.6945260763168335], [0.20146211981773376], [-0.07012835144996643], [-0.14598771929740906], [1.9066643714904785], [1.0929579734802246], [1.0200679302215576], [1.4308664798736572], [1.2681009769439697], [0.9984248280525208], [1.9559874534606934], [0.49623507261276245], [0.8378031253814697], [1.888030767440796], [1.079758644104004], [1.8847980499267578], [1.4231410026550293], [1.8049805164337158], [0.7674166560173035], [1.795325756072998], [2.0684516429901123], [0.5943113565444946], [1.127432107925415], [1.0848444700241089], [1.890411138534546], [0.5788639187812805], [1.5670926570892334], [1.846308708190918], [1.5091023445129395], [1.3753998279571533], [1.24183988571167], [0.9607083201408386], [0.6786017417907715], [1.5528457164764404], [1.1101922988891602], [1.343172550201416], [1.5343990325927734], [1.123786211013794], [0.1844651699066162], [1.005659818649292], [0.6243041753768921], [1.3984456062316895], [1.3042778968811035], [2.148347854614258], [2.038496494293213], [0.23160147666931152], [1.713449239730835], [0.5520842671394348], [0.9262048006057739], [1.4144132137298584], [1.9478065967559814], [1.7842845916748047], [1.9770920276641846], [1.9844343662261963], [2.4488885402679443], [2.272305965423584], [1.6514525413513184], [0.9474400877952576], [1.328881025314331], [0.9487599730491638], [1.319655418395996], [0.5079283714294434], [-0.40549594163894653], [1.4755957126617432], [0.901202380657196], [1.4613122940063477], [0.7067990899085999], [-0.5982083678245544], [2.4324264526367188], [0.5195055603981018], [0.8996080160140991], [0.878807783126831], [1.5848257541656494], [1.0665494203567505], [1.191976547241211], [1.214951515197754], [1.8488116264343262], [1.760735273361206], [0.40923017263412476], [0.0639142245054245], [1.2135977745056152], [0.8076431751251221], [1.766366958618164], [0.4000658392906189], [0.6777170896530151], [0.6057559847831726], [0.7677393555641174], [1.1312227249145508], [1.6733272075653076], [0.4142683446407318], [0.4079529941082001], [0.026053324341773987], [0.7996565103530884], [0.8700143694877625], [1.7333390712738037], [0.9020028710365295], [0.8042541146278381], [0.6434099078178406], [1.115220069885254], [0.7578899264335632], [1.1339340209960938], [0.32406044006347656], [0.9078383445739746], [0.8456533551216125], [0.7296283841133118], [1.856698751449585], [0.1142459362745285], [-0.13247337937355042], [-0.03150629997253418], [0.5316692590713501], [0.7957310080528259], [0.7274839878082275], [1.2419250011444092], [2.0219712257385254], [0.7546358108520508], [0.24829988181591034], [1.8095006942749023], [0.9609720706939697], [1.3256356716156006], [1.3126699924468994], [1.6936914920806885], [1.9547297954559326], [0.844916820526123], [1.253251314163208], [1.0238721370697021], [1.2756834030151367], [1.6688508987426758], [1.3672568798065186], [1.9569759368896484], [0.6979358792304993], [0.7279112935066223], [1.443962574005127], [0.8257660269737244], [1.3508808612823486], [3.019655466079712], [3.1457717418670654], [1.4112529754638672], [0.7513518929481506], [1.6672916412353516], [1.0341620445251465], [2.840277910232544], [2.7574191093444824], [1.6834278106689453], [1.663830280303955], [1.0416498184204102], [1.508000135421753], [1.5918402671813965], [0.8860086798667908], [0.9097087979316711], [0.646779477596283], [1.7141754627227783], [1.212784767150879], [1.2714760303497314], [0.38487571477890015], [1.7218594551086426], [1.6872870922088623], [1.4145801067352295], [2.1589252948760986], [1.0049030780792236], [0.4913071393966675], [1.038332223892212], [1.4116969108581543], [1.5707824230194092], [1.263671636581421], [1.3831236362457275], [1.2454016208648682], [2.9995274543762207], [1.8913328647613525], [0.8778296113014221], [2.065864324569702], [1.8704543113708496], [1.8753681182861328], [1.4237594604492188]]\n"
     ]
    }
   ],
   "source": [
    "print pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQUgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a4632d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thing,test_y,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2674829647269796"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([(thing[i]-test_y[i])**2 for i in range(len(thing))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37463669061385213"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([(ridge_predicted[i]-test_y[i])**2 for i in range(len(thing))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
