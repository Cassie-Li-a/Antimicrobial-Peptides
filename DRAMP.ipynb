{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "DATABASE_NAME = 'DRAMP'\n",
    "ANTIBACTERIA_LINK_PAGES = 326\n",
    "bacteria_regex = r'(?P<bacterium>[A-Z]\\. [a-z]+)(?P<strain>\\s?[A-Z]+\\s?[0-9]+)?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find links to each DRAMP peptide page\n",
    "find_links_base = 'http://dramp.cpu-bioinfor.org/browse/ActivityData.php?order=antibacterial&pageNow='\n",
    "drampids = []\n",
    "for i in range(1, ANTIBACTERIA_LINK_PAGES):\n",
    "    url = find_links_base + str(i)\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    matches = re.finditer('value\\=\\\"(?P<drampid>DRAMP[0-9]+)\\\"', str(soup))\n",
    "    drampids += [m.groupdict()['drampid'] for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of bacteria names\n",
    "bacteria_list_url_base = 'http://www.thelabrat.com/protocols/Bacterialspecies/byname'\n",
    "bacteria_names = []\n",
    "import string\n",
    "for letter in string.ascii_uppercase:\n",
    "    url = bacteria_list_url_base + letter + '.shtml'\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text = soup.text\n",
    "    # start = text.index('Y | Z \\n\\n\\') + len(\"Y | Z \\n\\n\")\n",
    "    end = start + text[start:].index('\\n\\n<!--\\ngoogle_ad_client')\n",
    "    for name in text[start:end].split('\\n'):\n",
    "        if name and name.strip() and all(c.isalpha() or c == ' ' for c in name):\n",
    "            bacteria_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteria_list_url_base_2 = 'https://www.ncbi.nlm.nih.gov/books/NBK'\n",
    "for i in range(818, 844):\n",
    "    url = bacteria_list_url_base_2 + str(i)\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    str_soup = str(soup)\n",
    "    for match in re.finditer('targettype=tax', str_soup):\n",
    "        start = match.start()\n",
    "        while str_soup[start] != '>':\n",
    "            start += 1\n",
    "        start += 1\n",
    "        end = start\n",
    "        while str_soup[end] != '<':\n",
    "            end += 1\n",
    "        if str_soup[start].isupper():\n",
    "            bacteria_names.append(str_soup[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteria_names = list(set(bacteria_names + bnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(bacteria_names, open(\"bacteria_list.txt\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_splits(soup):\n",
    "    text = soup.text\n",
    "    while text != text.replace('\\n\\n', '\\n'):\n",
    "        text = text.replace('\\n\\n', '\\n')\n",
    "    return text.split('\\n')\n",
    "\n",
    "def get_sequence(soup_split_by_line):\n",
    "    return soup_split_by_line[soup_split_by_line.index('Sequence Length') - 1]\n",
    "\n",
    "def get_modifications(soup):\n",
    "    ptm_string = '\"comments-PTM\">PTM</h4></li><li><h5> '\n",
    "    text = str(soup)\n",
    "    if ptm_string not in text:\n",
    "        return []\n",
    "    rest = text[text.index(ptm_string) + len(ptm_string):]\n",
    "    ptm = rest[:rest.index('</')]\n",
    "    if 'erminal amidation' not in rest:\n",
    "        return [\"Unknown modification: \" + ptm]\n",
    "    return ['C-Terminal Amidation']\n",
    "\n",
    "def get_references(soup_split_by_line):\n",
    "    references = []\n",
    "    reference_splits = soup_split_by_line[soup_split_by_line.index('Literature Information'):]\n",
    "    for i, line in enumerate(reference_splits):\n",
    "        if line == 'Reference':\n",
    "            pubmed_id = reference_splits[i - 1]\n",
    "            reference = reference_splits[i + 3]\n",
    "            author = reference_splits[i + 1]\n",
    "            title = reference_splits[i + 5]\n",
    "            references.append(\n",
    "                \"PubMed ID: %s. Reference: %s. Author: %s. Title: %s\"\n",
    "                % (pubmed_id, reference, author, title)\n",
    "            )\n",
    "    return references\n",
    "\n",
    "def find_longest_bacteria_matches(bacteria_names, text_section):\n",
    "    # Find bacteria names that match, using only longest form\n",
    "    # This way we don't keep the one-word abbreviations when two names are present\n",
    "    bacteria_matches = [b for b in bacteria_names if ' ' + b in text_section.lower()]\n",
    "    longest_matches = []\n",
    "    for match in bacteria_matches:\n",
    "        is_sub = False\n",
    "        for other_match in bacteria_matches:\n",
    "            if match in other_match and len(other_match) > len(match):\n",
    "                is_sub = True\n",
    "        if not is_sub:\n",
    "            longest_matches.append(match)\n",
    "    return longest_matches\n",
    "\n",
    "def bacteria_text_to_bacteria_and_strain(bacteria_names, line):\n",
    "    all_matches = []\n",
    "    matches_from_full_names = find_longest_bacteria_matches(bacteria_names, line)\n",
    "\n",
    "    strain = re.search('ATCC[0-9]*', line)\n",
    "    if strain:\n",
    "        strain = strain.group(0)\n",
    "\n",
    "    for match in matches_from_full_names:\n",
    "        if len(match.split()) > 1:\n",
    "            all_matches.append((\n",
    "                match[0].upper() + '. ' + match.split()[1],\n",
    "                strain.strip() if strain else ''\n",
    "            ))\n",
    "        else:\n",
    "            splits = line.lower().split()\n",
    "            for i, split in enumerate(splits):\n",
    "                if split == match:\n",
    "                    all_matches.append((\n",
    "                        match[0].upper() + '. ' + splits[i + 1],\n",
    "                        strain.strip() if strain else ''\n",
    "                    ))\n",
    "            \n",
    "    for regex_match in re.finditer(bacteria_regex, line):\n",
    "        bacterium = regex_match.groupdict()['bacterium']\n",
    "        strain = regex_match.groupdict()['strain']\n",
    "        all_matches.append((\n",
    "            bacterium,\n",
    "            strain.strip() if strain else ''\n",
    "        ))\n",
    "\n",
    "    return all_matches\n",
    "\n",
    "def extract_unit_and_value(line):\n",
    "    value = re.search('[0-9]+\\.?[0-9]*', line)\n",
    "    if not value:\n",
    "        return ('', '')\n",
    "    unit = line[value.end():].strip().replace(')', '')\n",
    "    return (value.group(0), unit)\n",
    "\n",
    "def get_mic_data(soup_split_by_line, _bacteria_names):\n",
    "    bacteria_names_lower = [b.lower() for b in bacteria_names]\n",
    "    all_bacteria = {}\n",
    "    mic_line_index = sorted(\n",
    "        [i for i in range(len(soup_split_by_line)) if soup_split_by_line[i] == 'Target Organism']\n",
    "    )[1] + 1\n",
    "    mic_line = soup_split_by_line[mic_line_index]\n",
    "    bacterium_or_unit_and_value_fields = re.split('(\\(MIC\\s?.*?\\))', mic_line)  # Alternating between unit/values and bacteria\n",
    "    for i, mic_split in enumerate(bacterium_or_unit_and_value_fields):\n",
    "        if re.search('(\\(MIC\\s?.*?\\))', mic_split):\n",
    "            bacteria_and_strains = bacteria_text_to_bacteria_and_strain(bacteria_names_lower, bacterium_or_unit_and_value_fields[i - 1])\n",
    "            unit, value = extract_unit_and_value(mic_split)\n",
    "            for (bacterium, strain) in bacteria_and_strains:\n",
    "                all_bacteria[(bacterium, strain)] = {\n",
    "                    'unit': unit,\n",
    "                    'value': value\n",
    "                }\n",
    "    return all_bacteria\n",
    "\n",
    "def get_hemolysis_data(soup):  # Return the sentence with the hemolytic data, leaving parsing for later\n",
    "    text = soup.text\n",
    "    if 'hemoly' in text:\n",
    "        for sentence in text.split('.'):\n",
    "            if 'hemoly' in sentence:\n",
    "                return sentence\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "amps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_soup(soup):\n",
    "    ssplits = get_line_splits(soup)\n",
    "    sequence = get_sequence(ssplits)\n",
    "    if not sequence:\n",
    "        return None, None\n",
    "    modifications = get_modifications(soup)\n",
    "    references = get_references(ssplits)\n",
    "    mic_data = get_mic_data(ssplits, bacteria_names)\n",
    "    url_sources = [url]\n",
    "    hemolysis_data = get_hemolysis_data(soup)\n",
    "    return (\n",
    "        sequence, \n",
    "        {\n",
    "            'modifications': modifications,\n",
    "            'references': references,\n",
    "            'bacteria': mic_data,\n",
    "            'url_sources': url_sources,\n",
    "            'hemolysis': hemolysis_data,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"url = 'http://dramp.cpu-bioinfor.org/browse/All_Information.php?id=DRAMP01474&dataset='\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "parse_soup(soup)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for i, drampid in enumerate(drampids):\n",
    "    url = url_base + drampid\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    ssplits = get_line_splits(soup)\n",
    "\n",
    "    sequence = get_sequence(ssplits)\n",
    "    if not sequence:\n",
    "        continue\n",
    "\n",
    "    sequence, results = parse_soup(soup)\n",
    "    if sequence:\n",
    "        amps[sequence] = results\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(DATABASE_NAME + \".data\", 'w') as f:\n",
    "    f.write(str(amps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
