{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary stuff\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "%matplotlib inline\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from Bio import pairwise2\n",
    "from Bio import SeqIO\n",
    "from Bio.SubsMat import MatrixInfo as matlist\n",
    "import seaborn as sns\n",
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARACTER_DICT = set([u'A', u'C', u'E', u'D', u'G', u'F', u'I', u'H', u'K', u'M', u'L', u'N', u'Q', u'P', u'S', u'R', u'T', u'W', u'V', u'Y'])\n",
    "MAX_SEQUENCE_LENGTH=46\n",
    "MAX_MIC = 4\n",
    "FONT_TO_USE = 'Arial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter an element of a result dictionary into df-ready row\n",
    "# Standardize units of MIC\n",
    "def standardize_to_uM(concentration, unit, sequence):\n",
    "    concentration = concentration.replace(' ', '')\n",
    "    try:\n",
    "        concentration = float(concentration)\n",
    "    except:\n",
    "        return None\n",
    "    if unit == 'uM' or unit == u'\\xb5M' or unit == u'uM)':\n",
    "        return concentration\n",
    "    elif unit == 'ug/ml' or unit == u'\\xb5g/ml' or unit == u'ug/ml)':\n",
    "        try:\n",
    "            molWt = ProteinAnalysis(sequence).molecular_weight()\n",
    "        except ValueError:\n",
    "            return None\n",
    "        return concentration * 1000/molWt\n",
    "    elif unit == 'nmol/g' or unit == 'pmol/mg':\n",
    "        #1g, at density of 1g/mL, is 1mL, so nmol/g is nmol/mL = umol/L = uM yay!\n",
    "        return concentration\n",
    "    else:\n",
    "        # print 'Unit not recognized: ' + unit\n",
    "        return None\n",
    "    \n",
    "def convert_result_to_rows(sequence, result):\n",
    "    rows = []\n",
    "    if 'bacteria' not in result:\n",
    "        return rows\n",
    "    for bacterium, strain in result['bacteria']:\n",
    "        \n",
    "        rows.append({\n",
    "            'bacterium': bacterium,\n",
    "            'strain': strain,\n",
    "            'sequence': sequence.upper(),\n",
    "            'url_source': result['url_sources'][0],\n",
    "            'value': standardize_to_uM(\n",
    "                result['bacteria'][(bacterium, strain)]['value'],\n",
    "                result['bacteria'][(bacterium, strain)]['unit'],\n",
    "                sequence\n",
    "            ),\n",
    "            'modifications': result['modifications'] if 'modifications' in result else [],\n",
    "            'unit': 'uM'\n",
    "        })\n",
    "        if rows[-1]['value']:\n",
    "            rows[-1]['value'] = np.log10(rows[-1]['value'])\n",
    "    return rows\n",
    "\n",
    "# Remove sequences with amino acids that aren't well-defined\n",
    "def strip_sequences_with_char(df, bad_char):\n",
    "    return df[~df.sequence.str.contains(bad_char)]\n",
    "\n",
    "# We'll want to strip off any sequences with modifications that could be hard to replicate\n",
    "# Their effects are too complex for the model\n",
    "def is_modified(modifications_list):\n",
    "    return len(modifications_list) > 0\n",
    "\n",
    "# However, C-Terminal Amidation is common enough that we make an exception\n",
    "CTERM_AMIDATION_TERMS = ['C-Terminal amidation','C-Terminus: AMD','C-Terminal','C-termianal amidation']\n",
    "\n",
    "def has_non_cterminal_modification(modifications_list):\n",
    "    return any(['C-Term' not in modification for modification in modifications_list])\n",
    "\n",
    "def has_unusual_modification(modifications_list):\n",
    "    return any([is_uncommon_modification(mod) for mod in modifications_list])\n",
    "\n",
    "def has_cterminal_amidation(modifications_list):\n",
    "    return any([is_cterminal_amidation(mod) for mod in modifications_list])\n",
    "\n",
    "def has_disulfide_bonds(modifications_list):\n",
    "    return any([is_disulfide_bond(mod) for mod in modifications_list])\n",
    "\n",
    "def is_cterminal_amidation(mod):\n",
    "    for term in CTERM_AMIDATION_TERMS:\n",
    "        if term in mod:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_disulfide_bond(mod):\n",
    "    return 'disulfide' in mod.lower()\n",
    "\n",
    "def is_uncommon_modification(mod):\n",
    "    return (not is_cterminal_amidation(mod)) and (not is_disulfide_bond(mod))\n",
    "\n",
    "def datasource_has_modifications(cell):\n",
    "    # Everything except CAMP and YADAMP has modification data\n",
    "    NO_MODIFICATION_DATA_SOURCES = ['camp3', 'yadamp']\n",
    "    return not any([s in cell for s in NO_MODIFICATION_DATA_SOURCES])\n",
    "\n",
    "def sequence_has_modification_data(cell):\n",
    "    # If the sequence is labeled modifictationless in another database it's OK\n",
    "    return cell in sequences_containing_modifications\n",
    "\n",
    "# Each amino acid its own group\n",
    "character_to_index = {\n",
    "    (character): i\n",
    "    for i, character in enumerate(CHARACTER_DICT)\n",
    "}\n",
    "\n",
    "index2character = {\n",
    "    value: key\n",
    "    for key, value in character_to_index.items()\n",
    "}\n",
    "\n",
    "def sequence_to_vector(sequence, cterminal_amidation):\n",
    "# It looks like this truncates any sequence after max_sequence_length (which is length of 95th percentile longest peptide)\n",
    "# I just add cterminal amidation as the amino acid after the last real amino acid (if the amino acid gets truncated\n",
    "# then the cterminal amidation also gets cut off)\n",
    "    default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "    for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "        default[i][character_to_index[character]] = 1\n",
    "    if len(sequence)<MAX_SEQUENCE_LENGTH:\n",
    "        default[len(sequence)][-1]=cterminal_amidation\n",
    "    return default\n",
    "\n",
    "def old_sequence_to_vector(sequence, cterminal_amidation):\n",
    "    default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "    for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "        default[i][character_to_index[character]] = 1\n",
    "        default[i][-1] = cterminal_amidation\n",
    "    return default\n",
    "\n",
    "def find_character(character2index, character):\n",
    "    for key in character2index:\n",
    "        if character in key:\n",
    "            return character2index[key]\n",
    "    return -2\n",
    "\n",
    "def row_to_vector(row, shuffle_sequence=False):\n",
    "    sequence = list(row['sequence'])\n",
    "    if shuffle_sequence:\n",
    "        random.shuffle(sequence)\n",
    "    cterminal_amidation = row['has_cterminal_amidation']\n",
    "    return sequence_to_vector(sequence,cterminal_amidation)\n",
    "\n",
    "def old_row_to_vector(row, shuffle_sequence=False):\n",
    "    sequence = list(row['sequence'])\n",
    "    if shuffle_sequence:\n",
    "        random.shuffle(sequence)\n",
    "    cterminal_amidation = row['has_cterminal_amidation']\n",
    "    default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "\n",
    "    for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "        default[i][find_character(character_to_index, character)] = 1\n",
    "        default[i][-1] = cterminal_amidation\n",
    "\n",
    "    return default\n",
    "\n",
    "def vector_to_amp(vector):\n",
    "    sequence = ''\n",
    "    has_cterm = False\n",
    "    for v in vector:\n",
    "        nonzeros = np.argwhere(v[:len(character_to_index)])\n",
    "        if len(nonzeros) > 1:\n",
    "            print(\"?????\")\n",
    "        elif len(nonzeros) == 0:\n",
    "            sequence += '_'\n",
    "        else:\n",
    "            sequence += index2character[np.argwhere(v)[0][0]]  # First one\n",
    "        if v[-1]>0:\n",
    "            has_cterm=True\n",
    "    return {\n",
    "        'sequence': sequence,\n",
    "        'cterminal_amidation': has_cterm\n",
    "    }\n",
    "\n",
    "def bacterium_to_sample_weight(bacterium, intended_bacterium='E. coli'):\n",
    "    if intended_bacterium in bacterium:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.5\n",
    "    \n",
    "def containing_bacterium(bacterium, df):\n",
    "    return df.loc[df.bacterium.str.contains(bacterium)]\n",
    "\n",
    "def average_over_databases(bacterium_df):\n",
    "    return bacterium_df.groupby('sequence')['value'].mean().dropna()\n",
    "\n",
    "def get_bacterium_df(bacterium, df):\n",
    "    bdf = df.loc[(df.bacterium.str.contains(bacterium))].groupby(['sequence', 'bacterium'])\n",
    "    return bdf.mean().reset_index().dropna()\n",
    "\n",
    "def strip_bad_amino_acids(df, bad_amino_acids=('U', 'X', 'Z')):\n",
    "    for b in bad_amino_acids:\n",
    "        df = df.loc[~df.sequence.str.contains(b)]\n",
    "    return df\n",
    "\n",
    "def split_dataframe(df_to_split,cutoff=0.85):\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    for i in range(len(df_to_split)):\n",
    "        if 'C' not in df_to_split['sequence'][i] and random.random()>cutoff:\n",
    "            test_indices.append(i)\n",
    "        else:\n",
    "            train_indices.append(i)\n",
    "    train_df = df_to_split.iloc[train_indices].reset_index(drop=True)\n",
    "    test_df = df_to_split.iloc[test_indices].reset_index(drop=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "def get_database_name(url_source):\n",
    "    if 'dbaasp' in url_source:\n",
    "        return 'DBAASP'\n",
    "    elif 'split4.pmfs' in url_source:\n",
    "        return 'DADP'\n",
    "    elif 'dramp.cpu' in url_source:\n",
    "        return 'DRAMP'\n",
    "    elif 'yadamp.' in url_source:\n",
    "        return 'YADAMP'\n",
    "    elif 'aps.unmc' in url_source:\n",
    "        return 'APD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scripts stored the outputs as dictionaries.\n",
    "all_results = []\n",
    "data_dir = 'data/'\n",
    "for f in os.listdir(data_dir):\n",
    "    if '.data' in f:\n",
    "        with open(data_dir + f, 'r') as g:\n",
    "            all_results.append(ast.literal_eval(g.read()))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the rows into an array         \n",
    "rows = []\n",
    "for result_set in all_results:\n",
    "    for sequence in result_set:\n",
    "        for row in convert_result_to_rows(sequence, result_set[sequence]):\n",
    "            rows.append(row)\n",
    "            \n",
    "# Construct the df\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"Dataframe length before removing bad chars:\", len(df))\n",
    "\n",
    "for bad_char in ['U', 'X', 'Z']:\n",
    "    df = strip_sequences_with_char(df, bad_char)\n",
    "print(\"Dataframe length after removing bad chars:\", len(df))\n",
    "\n",
    "df['is_modified'] = df.modifications.apply(is_modified)\n",
    "df['has_unusual_modification'] = df.modifications.apply(has_unusual_modification)\n",
    "df['has_cterminal_amidation'] = df.modifications.apply(has_cterminal_amidation)\n",
    "\n",
    "# Clean sequences by removing newlines and one improper sequence\n",
    "df.sequence = df.sequence.str.strip()\n",
    "df = df.loc[df.sequence != '/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datasource_has_modifications'] = df['url_source'].apply(datasource_has_modifications)\n",
    "df['database'] = df.url_source.apply(get_database_name)\n",
    "df.to_csv('grampa.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = df.database.unique()\n",
    "for database in databases:\n",
    "    print(database, df[df.database == database].sequence.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude sequences with modifications\n",
    "# Exclude rows from YADAMP and CAMP for having no modification data\n",
    "#     Unless that sequence is in another DB\n",
    "\n",
    "df = df.loc[df.has_unusual_modification == False]\n",
    "\n",
    "print(\"Dataframe length after excluding modifications:\", len(df))\n",
    "\n",
    "df['_datasource_has_modifications'] = df['url_source'].apply(datasource_has_modifications)\n",
    "\n",
    "sequences_containing_modifications = set(df.loc[df._datasource_has_modifications == True, 'sequence'])\n",
    "\n",
    "\n",
    "df['_sequence_has_modifications'] = df['sequence'].apply(sequence_has_modification_data)\n",
    "\n",
    "df['modification_verified'] = df['_sequence_has_modifications'] | df['_datasource_has_modifications']\n",
    "\n",
    "df = df.loc[df.modification_verified == True]\n",
    "\n",
    "print(\"Dataframe length after removing modification unverified:\", len(df))\n",
    "\n",
    "# Correct typos, for example, 'P. aeruginsa'\n",
    "df=df.reset_index()\n",
    "typos=['K. pneumonia','P. aeruginsa','S. aureu','S. a']\n",
    "corrections = ['K. pneumoniae','P. aeruginosa','S. aureus','S. aureus']\n",
    "for k,bacterium in enumerate(df['bacterium']):\n",
    "    for i in range(len(typos)):\n",
    "        if bacterium == typos[i]:\n",
    "            df.at[k,'bacterium']=corrections[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data analysis: making correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between bacteria\n",
    "Ec = df.loc[df.bacterium.str.contains('E. coli')].groupby('sequence')['value'].mean().dropna()\n",
    "Sa = df.loc[df.bacterium.str.contains('S. aureus')].groupby('sequence')['value'].mean().dropna()\n",
    "Pa = df.loc[df.bacterium.str.contains('P. aeruginosa')].groupby('sequence')['value'].mean().dropna()\n",
    "Sm = df.loc[df.bacterium.str.contains('S. mutans')].groupby('sequence')['value'].mean().dropna()\n",
    "Bs = df.loc[df.bacterium.str.contains('B. subtilis')].groupby('sequence')['value'].mean().dropna()\n",
    "Se = df.loc[df.bacterium.str.contains('S. epidermidis')].groupby('sequence')['value'].mean().dropna()\n",
    "Ml = df.loc[df.bacterium.str.contains('M. luteus')].groupby('sequence')['value'].mean().dropna()\n",
    "Kp = df.loc[df.bacterium.str.contains('K. pneumoniae')].groupby('sequence')['value'].mean().dropna()\n",
    "Ef = df.loc[df.bacterium.str.contains('E. faecalis')].groupby('sequence')['value'].mean().dropna()\n",
    "St = df.loc[df.bacterium.str.contains('S. typhimurium')].groupby('sequence')['value'].mean().dropna()\n",
    "Ca = df.loc[df.bacterium.str.contains('C. albicans')].groupby('sequence')['value'].mean().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the correlation between bacteria, with minimum number of shared measurements = 50\n",
    "# Note that Gram-Positivity seems to have a strong effect on correlation\n",
    "# E. coli and pseudomonas are highly correlated\n",
    "# While neither is correlated with streptococcus, staph or bacillus\n",
    "# Meanwhile, staph and streptococcus are strongly correlated as expected\n",
    "# As are bacillus and streptococcus\n",
    "# The lack of correlation between bacillus and staph is a mystery to me\n",
    "many_bacteria = pd.concat([Ca,Pa,Ec,Kp,St,Ml,Ef,Bs,Se,Sa], axis=1).reset_index()\n",
    "many_bacteria.columns = ['index', 'Ca','Pa', 'Ec','Kp','St','Ml','Ef','Bs','Se','Sa']\n",
    "corr=many_bacteria.corr(min_periods=50)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a heatmap of the correlation matrix\n",
    "mask=np.zeros_like(corr,dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "cmap = sns.diverging_palette(255, 10, as_cmap=True)\n",
    "sns.set(font_scale=2,style='white',font=FONT_TO_USE)\n",
    "# sns.set_style({'font.sans-serif': 'arial'})\n",
    "f, ax = plt.subplots(figsize=(11,9))\n",
    "# sns.set_style({'font.sans-serif': ['Arial']})\n",
    "hmp=sns.heatmap(corr,mask=mask,linewidth=2.5,cmap=cmap)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_fontname(FONT_TO_USE)\n",
    "for tick in ax.get_yticklabels():\n",
    "    tick.set_fontname(FONT_TO_USE)\n",
    "# sns.set_style({'font.sans-serif': ['Arial']})\n",
    "fig = hmp.get_figure()\n",
    "fig.savefig('Figures_and_CNN_predictions/Heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Klebsiella pneumoniae vs E. coli, log MICs plotted against each other\n",
    "plt.plot(many_bacteria.Kp,many_bacteria.Ec,'.')\n",
    "plt.show()\n",
    "print len(Ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data analysis looking at some rarer bacteria\n",
    "Msmeg = df.loc[df.bacterium.str.contains('M. smegmatis')].groupby('sequence')['value'].mean().dropna()\n",
    "Mtub = df.loc[df.bacterium.str.contains('M. tuberculosis')].groupby('sequence')['value'].mean().dropna()\n",
    "Bcep = df.loc[df.bacterium.str.contains('B. cepacia')].groupby('sequence')['value'].mean().dropna()\n",
    "Bcen = df.loc[df.bacterium.str.contains('B. cenocepacia')].groupby('sequence')['value'].mean().dropna()\n",
    "Mabs = df.loc[df.bacterium.str.contains('M. abscessus')].groupby('sequence')['value'].mean().dropna()\n",
    "Mav = df.loc[df.bacterium.str.contains('M. avium')].groupby('sequence')['value'].mean().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_bact = pd.concat([Pa,Ec,Kp,St,Bcep,Msmeg,Mtub], axis=1).reset_index()\n",
    "rare_bact.columns = ['index', 'Pa', 'Ec','Kp','St','Bcep','Msmeg','Mtub']\n",
    "corr = rare_bact.corr(min_periods=10)\n",
    "print corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix with some other bacteria; note the color scale is different from what it was\n",
    "mask=np.zeros_like(corr,dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "cmap = sns.diverging_palette(255, 10, as_cmap=True)\n",
    "sns.set(font_scale=2,style='white',font=FONT_TO_USE)\n",
    "# sns.set_style({'font.sans-serif': 'arial'})\n",
    "f, ax = plt.subplots(figsize=(11,9))\n",
    "# sns.set_style({'font.sans-serif': ['Arial']})\n",
    "hmp=sns.heatmap(corr,mask=mask,linewidth=2.5,cmap=cmap)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_fontname(FONT_TO_USE)\n",
    "for tick in ax.get_yticklabels():\n",
    "    tick.set_fontname(FONT_TO_USE)\n",
    "# sns.set_style({'font.sans-serif': ['Arial']})\n",
    "fig = hmp.get_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_interesting_bacteria = pd.concat([Pa,Ec,Kp,St,Ml,Ef,Bs,Se,Sa,Bcep,Msmeg,Mtub], axis=1).reset_index()\n",
    "all_interesting_bacteria.columns = ['Sequence', 'P_aeruginosa', 'E_coli','K_pneumoniae','S_typhimurium','M_luteus','E_faecalis','B_subtilis','S_epidermidis','S_aureus','B_cepacia','M_smegmatis','M_tuberculosis']\n",
    "all_interesting_bacteria.to_csv('Figures_and_CNN_predictions/all_interesting_bacteria.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and pickle the dataframes (don't run this every time if you want to keep the same train-test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli_df = get_bacterium_df('E. coli', df).reset_index(drop=True)\n",
    "ecoli_train,ecoli_test = split_dataframe(ecoli_df)\n",
    "ecoli_train_no_c = strip_bad_amino_acids(ecoli_train,'C')\n",
    "ecoli_df_no_c = strip_bad_amino_acids(ecoli_df,bad_amino_acids=('C'))\n",
    "ecoli_train.to_pickle('data/saved_variables/ecoli_train_with_c_df.pkl')\n",
    "ecoli_train_no_c.to_pickle('data/saved_variables/ecoli_train_no_c_df.pkl')\n",
    "ecoli_test.to_pickle('data/saved_variables/ecoli_test_df.pkl')\n",
    "ecoli_df.to_pickle('data/saved_variables/ecoli_all_df.pkl')\n",
    "ecoli_df_no_c.to_pickle('data/saved_variables/ecoli_all_no_c_df.pkl')\n",
    "df.to_pickle('data/saved_variables/all_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once we have the pickled data, start with training NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, LSTM, Conv2D, Conv1D, MaxPooling1D, MaxPooling2D, Flatten, ZeroPadding1D\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import json\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli_train_with_c = pd.read_pickle('data/saved_variables/ecoli_train_with_c_df.pkl')\n",
    "ecoli_train_no_c = pd.read_pickle('data/saved_variables/ecoli_train_no_c_df.pkl')\n",
    "ecoli_test = pd.read_pickle('data/saved_variables/ecoli_test_df.pkl')\n",
    "ecoli_df = pd.read_pickle('data/saved_variables/ecoli_all_df.pkl')\n",
    "ecoli_df_no_c = pd.read_pickle('data/saved_variables/ecoli_all_no_c_df.pkl')\n",
    "all_df = pd.read_pickle('data/saved_variables/all_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram of lengths just to see what it is\n",
    "seqs = ecoli_df.sequence\n",
    "lengths = [min(MAX_SEQUENCE_LENGTH,len(seq)) for seq in seqs]\n",
    "SEQ_LENGTHS = lengths\n",
    "plt.hist(SEQ_LENGTHS,bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the input vectors for our model\n",
    "# Each vector is two dimensional\n",
    "# The first dimension represents the number of characters in the sequence (46 characters)\n",
    "# Each character is a vector of length equal to the number of groupings of amino acids\n",
    "# This grouping can be 1-1 (each amino acid gets its own group), or coarser\n",
    "def df_to_input_vec(df,shuffle = False):\n",
    "    cterminal_amidation = np.array(df.has_cterminal_amidation)\n",
    "\n",
    "    vectors = []\n",
    "    for row in df.iterrows():\n",
    "        vectors.append(row_to_vector(row[1], shuffle_sequence=shuffle))\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "    labels = np.array(df.value)\n",
    "    sample_weights = np.full(len(labels), 1)\n",
    "    return vectors, labels, sample_weights\n",
    "\n",
    "def generate_random_sequence(alphabet, length_of_sequence_min=0, length_of_sequence_max=MAX_SEQUENCE_LENGTH, include_C = True,fixed_length=-10,just_seq=False):\n",
    "#         Generates a random sequence to match the sequence length distribution of the test dataset\n",
    "        sequence = ''\n",
    "        choices = [char for char in alphabet if (include_C or char !='C')]\n",
    "        counter = 0\n",
    "        length_to_use = -10\n",
    "        if fixed_length<0:\n",
    "            while counter<20 and (length_to_use < length_of_sequence_min or length_to_use > length_of_sequence_max):\n",
    "                length_to_use = random.choice(SEQ_LENGTHS)\n",
    "                counter += 1\n",
    "        else:\n",
    "            length_to_use = fixed_length\n",
    "        for _ in range(length_to_use):\n",
    "            sequence += random.choice(choices)\n",
    "        if just_seq:\n",
    "            return sequence\n",
    "        has_cterminal_amidation = random.uniform(0, 1)\n",
    "        \n",
    "        return {\n",
    "            'sequence': sequence,\n",
    "            'has_cterminal_amidation': has_cterminal_amidation>0.5\n",
    "#             50% C terminal amidation\n",
    "        }\n",
    "\n",
    "def add_random_negative_examples(vectors, labels, sample_weights, ratio, max_mic = None, include_cysteine = True):\n",
    "    if not max_mic:\n",
    "        max_mic = max(labels)\n",
    "    # We will add randomly chosen sequences as negative examples\n",
    "    # We will double the length of our training set\n",
    "\n",
    "    len_vectors = ratio * len(vectors)\n",
    "    negative_rows = []\n",
    "    for i in range(len_vectors):\n",
    "        negative_rows.append(row_to_vector(generate_random_sequence(list(CHARACTER_DICT),include_C = include_cysteine)))\n",
    "    negative_vectors = np.array(negative_rows)\n",
    "    vectors = np.concatenate((vectors, negative_vectors))\n",
    "    negative_labels = np.full(len_vectors, max_mic)\n",
    "    labels = np.concatenate((labels, negative_labels))\n",
    "    # Weight all samples equally\n",
    "    sample_weights = np.concatenate((sample_weights, np.full(len_vectors, 1)))\n",
    "    return vectors, labels, sample_weights\n",
    "\n",
    "def generate_train_test_splits(\n",
    "        vectors, labels,\n",
    "        extra_training_vectors=[], extra_training_labels=[], extra_sample_weights=[],\n",
    "        cutoff=0.85\n",
    "):\n",
    "    cutoff = int(cutoff * len(labels))\n",
    "    idx = range(len(vectors))\n",
    "    random.shuffle(idx)\n",
    "    reordered_vectors = vectors[idx]\n",
    "    reordered_labels = labels[idx]\n",
    "    reordered_sample_weights = sample_weights[idx]\n",
    "    if len(extra_training_vectors) > 0:\n",
    "        train_x = np.concatenate((reordered_vectors[:cutoff], extra_training_vectors))\n",
    "        train_y = np.concatenate((reordered_labels[:cutoff], extra_training_labels))\n",
    "        train_sample_weights = np.concatenate((reordered_sample_weights[:cutoff], pa_sample_weights))\n",
    "    else:\n",
    "        train_x = reordered_vectors[:cutoff]\n",
    "        train_y = reordered_labels[:cutoff]\n",
    "        train_sample_weights = reordered_sample_weights[:cutoff]\n",
    "    test_x = reordered_vectors[cutoff:]\n",
    "    test_y = reordered_labels[cutoff:]\n",
    "    return train_x, train_y, test_x, test_y, train_sample_weights\n",
    "\n",
    "# Convolutional NN\n",
    "def conv_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(ZeroPadding1D(\n",
    "        5, input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(Conv1D(\n",
    "        64,\n",
    "        kernel_size = 5,\n",
    "        strides = 1,\n",
    "        activation = 'relu',\n",
    "        #input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self,models,predict_method,max_mic_buffer=0.1):\n",
    "        self.models = models\n",
    "        self.predict_method = predict_method\n",
    "        self.max_mic_buffer = max_mic_buffer\n",
    "        \n",
    "    def combine_predictions(self,predictions):\n",
    "        if self.predict_method is 'average':\n",
    "            return np.mean(predictions)\n",
    "        elif self.predict_method is 'classify_then_average':\n",
    "            actual_predictions = []\n",
    "            for prediction in predictions:\n",
    "                if prediction < MAX_MIC - self.max_mic_buffer:\n",
    "                    actual_predictions.append(prediction)\n",
    "            if float(len(actual_predictions))/float(len(predictions))>=0.49:\n",
    "                return np.mean(predictions)\n",
    "            else:\n",
    "                return MAX_MIC\n",
    "        else:\n",
    "            print 'predict_method not recognized'\n",
    "            return -100\n",
    "        \n",
    "    def predict(self,test_x):\n",
    "        all_predictions = []\n",
    "        combined_predictions = []\n",
    "        for model in self.models:\n",
    "            all_predictions.append(model.predict(test_x))\n",
    "        for i in range(len(test_x)):\n",
    "            combined_predictions.append(self.combine_predictions([all_predictions[k][i] for k in range(len(self.models))]))\n",
    "        return combined_predictions\n",
    "    \n",
    "    def evaluate(self,test_x,test_y):\n",
    "        predictions = self.predict(test_x)\n",
    "        correctly_classified_error = np.mean([(actual - predicted) ** 2 for actual, predicted in zip(test_y, predictions) if actual < MAX_MIC and predicted < MAX_MIC - self.max_mic_buffer])    \n",
    "        all_error = np.mean([(actual - predicted) ** 2 for actual, predicted in zip(test_y, predictions)])    \n",
    "        all_active_error = np.mean([(actual - predicted) ** 2 for actual, predicted in zip(test_y, predictions) if actual < MAX_MIC])    \n",
    "        return correctly_classified_error,all_active_error, all_error\n",
    "    \n",
    "    def evaluate_as_classifier(self,test_x,test_y):\n",
    "        true_positives=0\n",
    "        true_negatives=0\n",
    "        false_positives=0\n",
    "        false_negatives=0\n",
    "        all_predicted=self.predict(test_x)\n",
    "        for i in range(len(test_y)):\n",
    "            actual=test_y[i]\n",
    "            predicted=all_predicted[i]\n",
    "            if actual<MAX_MIC-0.0001:\n",
    "                if predicted<MAX_MIC - self.max_mic_buffer:\n",
    "                    true_positives+=1\n",
    "                else:\n",
    "                    false_negatives+=1\n",
    "            else:\n",
    "                if predicted<MAX_MIC - self.max_mic_buffer:\n",
    "                    false_positives += 1\n",
    "        #             print vector_to_amp(test_x[i])\n",
    "        #             print 'predicted: '+repr(predicted)+', actual: '+repr(actual)\n",
    "#                     print '>p'+repr(false_positives)+'_'+repr(predicted)\n",
    "#                     print vector_to_amp(test_x[i])['sequence'].replace('_','')\n",
    "                else:\n",
    "                    true_negatives += 1\n",
    "        return true_positives,true_negatives,false_positives,false_negatives\n",
    "        \n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vectors for training and testing\n",
    "ecoli_train_with_c_input = df_to_input_vec(ecoli_train_with_c)\n",
    "ecoli_train_no_c_input = df_to_input_vec(ecoli_train_no_c)\n",
    "ecoli_test_input = df_to_input_vec(ecoli_test)\n",
    "ecoli_df_with_c_input = df_to_input_vec(ecoli_df)\n",
    "ecoli_df_no_c_input = df_to_input_vec(ecoli_df_no_c)\n",
    "\n",
    "seqs = ecoli_df.sequence\n",
    "lengths = [min(MAX_SEQUENCE_LENGTH,len(seq)) for seq in seqs]\n",
    "SEQ_LENGTHS = lengths\n",
    "\n",
    "ecoli_test_input_with_negatives = add_random_negative_examples(ecoli_test_input[0],ecoli_test_input[1],ecoli_test_input[2],ratio=1,max_mic=MAX_MIC,include_cysteine=False)\n",
    "\n",
    "vectors = ecoli_test_input_with_negatives[0]\n",
    "labels = ecoli_test_input_with_negatives[1]\n",
    "average = np.mean(labels)\n",
    "squared_errors = sum([(label - average) ** 2 for label in labels])\n",
    "baseline_error = squared_errors/len(labels)\n",
    "print(\"Baseline error:\")\n",
    "print(baseline_error)\n",
    "measured_labels = [l for l in labels if l < MAX_MIC]\n",
    "average = np.mean(measured_labels)\n",
    "squared_errors = sum([(label - average) ** 2 for label in measured_labels])\n",
    "baseline_error = squared_errors/len(measured_labels)\n",
    "print(\"Baseline error on measured examples only\")\n",
    "print(baseline_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the CNN ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an ensemble of models with cysteines included, generating a new set of negative data each time\n",
    "# Run this for negative ratios of 1, 3, and 10\n",
    "NEGATIVE_RATIO = 10\n",
    "all_conv_models=[]\n",
    "ensemble_size=5\n",
    "done=0\n",
    "for i in range(done,ensemble_size):\n",
    "    ecoli_train_with_c_input_with_negatives = add_random_negative_examples(ecoli_train_with_c_input[0],ecoli_train_with_c_input[1],ecoli_train_with_c_input[2],ratio=NEGATIVE_RATIO,max_mic=MAX_MIC,include_cysteine=True)\n",
    "    weights_fname = 'Saved_models/train_ensemble_weights_withC_'+repr(NEGATIVE_RATIO)+'xneg_'+repr(i)+'.h5'\n",
    "    architecture_fname = 'Saved_models/train_ensemble_architecture_withC_'+repr(NEGATIVE_RATIO)+'xneg_'+repr(i)+'.json'\n",
    "    convmodel = conv_model()\n",
    "    convmodel.fit(ecoli_train_with_c_input_with_negatives[0], ecoli_train_with_c_input_with_negatives[1], batch_size=40, epochs=100)\n",
    "    convmodel.save_weights(weights_fname)\n",
    "    with open(architecture_fname,'w') as f:\n",
    "        f.write(convmodel.to_json())\n",
    "    all_conv_models.append(convmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an ensemble of models without cysteines included, generating a new set of negative data each time\n",
    "# Run this for negative ratios of 1, 3, and 10\n",
    "all_conv_models=[]\n",
    "NEGATIVE_RATIO = 30\n",
    "ensemble_size=5\n",
    "done=0\n",
    "for i in range(done,ensemble_size):\n",
    "    ecoli_train_no_c_input_with_negatives = add_random_negative_examples(ecoli_train_no_c_input[0],ecoli_train_no_c_input[1],ecoli_train_no_c_input[2],ratio=NEGATIVE_RATIO,max_mic=MAX_MIC,include_cysteine=False)\n",
    "    weights_fname = 'Saved_models/train_ensemble_weights_noC_'+repr(NEGATIVE_RATIO)+'xneg_'+repr(i)+'.h5'\n",
    "    architecture_fname = 'Saved_models/train_ensemble_architecture_noC_'+repr(NEGATIVE_RATIO)+'xneg_'+repr(i)+'.json'\n",
    "    convmodel = conv_model()\n",
    "    convmodel.fit(ecoli_train_no_c_input_with_negatives[0], ecoli_train_no_c_input_with_negatives[1], batch_size=40, epochs=100)\n",
    "    convmodel.save_weights(weights_fname)\n",
    "    with open(architecture_fname,'w') as f:\n",
    "        f.write(convmodel.to_json())\n",
    "    all_conv_models.append(convmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an ensemble of models with cysteines included, with all the data\n",
    "# Run this for negative ratios of 1, 3, and 10\n",
    "NEGATIVE_RATIO = 3\n",
    "all_conv_models=[]\n",
    "ensemble_size=5\n",
    "done=0\n",
    "for i in range(done,ensemble_size):\n",
    "    ecoli_df_with_c_input_with_negatives = add_random_negative_examples(ecoli_df_with_c_input[0],ecoli_df_with_c_input[1],ecoli_df_with_c_input[2],ratio=NEGATIVE_RATIO,max_mic=MAX_MIC,include_cysteine=True)\n",
    "    weights_fname = 'Saved_models/all_data_ensemble_weights_withC_'+repr(NEGATIVE_RATIO)+'xneg_'+repr(i)+'.h5'\n",
    "    architecture_fname = 'Saved_models/all_data_ensemble_architecture_withC_'+repr(NEGATIVE_RATIO)+'xneg_'+repr(i)+'.json'\n",
    "    convmodel = conv_model()\n",
    "    convmodel.fit(ecoli_df_with_c_input_with_negatives[0], ecoli_df_with_c_input_with_negatives[1], batch_size=40, epochs=100)\n",
    "    convmodel.save_weights(weights_fname)\n",
    "    with open(architecture_fname,'w') as f:\n",
    "        f.write(convmodel.to_json())\n",
    "    all_conv_models.append(convmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an ensemble of models without cysteines included, with all the data\n",
    "# Run this for negative ratios of 1, 3, and 10\n",
    "NEGATIVE_RATIO = 1\n",
    "all_conv_models=[]\n",
    "ensemble_size=5\n",
    "done=0\n",
    "for i in range(done,ensemble_size):\n",
    "    ecoli_df_no_c_input_with_negatives = add_random_negative_examples(ecoli_df_no_c_input[0],ecoli_df_no_c_input[1],ecoli_df_no_c_input[2],ratio=NEGATIVE_RATIO,max_mic=MAX_MIC,include_cysteine=False)\n",
    "    weights_fname = 'Saved_models/all_data_ensemble_weights_noC_'+repr(NEGATIVE_RATIO)+'xneg_'+repr(i)+'.h5'\n",
    "    architecture_fname = 'Saved_models/all_data_ensemble_architecture_noC_'+repr(NEGATIVE_RATIO)+'xneg_'+repr(i)+'.json'\n",
    "    convmodel = conv_model()\n",
    "    convmodel.fit(ecoli_df_no_c_input_with_negatives[0], ecoli_df_no_c_input_with_negatives[1], batch_size=40, epochs=100)\n",
    "    convmodel.save_weights(weights_fname)\n",
    "    with open(architecture_fname,'w') as f:\n",
    "        f.write(convmodel.to_json())\n",
    "    all_conv_models.append(convmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated annealingfor sequence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences=[sequence for sequence in ecoli_df.sequence]\n",
    "ENSEMBLE_SIZE=5\n",
    "NEG_RATIOS = [1,3,10]\n",
    "with_C_weights = []\n",
    "with_C_architectures = []\n",
    "no_C_weights = []\n",
    "no_C_architectures = []\n",
    "for k in range(len(NEG_RATIOS)):\n",
    "    with_C_weights.append(['all_data_ensemble_weights_withC_'+repr(NEG_RATIOS[k])+'xneg_'+repr(i)+'.h5' for i in range(ENSEMBLE_SIZE)])\n",
    "    with_C_architectures.append(['all_data_ensemble_architecture_withC_'+repr(NEG_RATIOS[k])+'xneg_'+repr(i)+'.json' for i in range(ENSEMBLE_SIZE)])\n",
    "    no_C_weights.append(['all_data_ensemble_weights_noC_'+repr(NEG_RATIOS[k])+'xneg_'+repr(i)+'.h5' for i in range(ENSEMBLE_SIZE)])\n",
    "    no_C_architectures.append(['all_data_ensemble_architecture_noC_'+repr(NEG_RATIOS[k])+'xneg_'+repr(i)+'.json' for i in range(ENSEMBLE_SIZE)])\n",
    "\n",
    "    \n",
    "with_c_ensembles = [[] for i in range(len(NEG_RATIOS))]\n",
    "no_c_ensembles = [[] for i in range(len(NEG_RATIOS))]\n",
    "for k in range(len(NEG_RATIOS)):\n",
    "    for i in range(len(with_C_weights[0])):\n",
    "        with open('models/Saved_models_all_data/'+with_C_architectures[k][i],'r') as f:\n",
    "            reconst_model = model_from_json(f.read())\n",
    "        reconst_model.load_weights('models/Saved_models_all_data/'+with_C_weights[k][i])\n",
    "        reconst_model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "        with_c_ensembles[k].append(reconst_model)    \n",
    "\n",
    "        with open('models/Saved_models_all_data/'+no_C_architectures[k][i],'r') as f:\n",
    "            reconst_model = model_from_json(f.read())\n",
    "        reconst_model.load_weights('models/Saved_models_all_data/'+no_C_weights[k][i])\n",
    "        reconst_model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "        no_c_ensembles[k].append(reconst_model)\n",
    "        \n",
    "everything_ensemble_model = EnsembleModel(no_c_ensembles[0]+with_c_ensembles[0]+no_c_ensembles[1]+with_c_ensembles[1]+no_c_ensembles[2]+with_c_ensembles[2],'average')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales={'Eisenberg':{'A':  0.25, 'R': -1.80, 'N': -0.64,'D': -0.72, 'C':  0.04, 'Q': -0.69,'E': -0.62, 'G':  0.16, 'H': -0.40,'I':  0.73, 'L':  0.53, 'K': -1.10,'M':  0.26, 'F':  0.61, 'P': -0.07,'S': -0.26, 'T': -0.18, 'W':  0.37,'Y':  0.02, 'V':  0.54},\n",
    "'Normalized_consensus':{'A':0.62,'C':0.29,'D':-0.9,'E':-0.74,'F':1.19,'G':0.48,'H':-0.4,'I':1.38,'K':-1.5,'L':1.06,'M':0.64,'N':-0.78,'P':0.12,'Q':-0.85,'R':-2.53,'S':-0.18,'T':-.05,'V':1.08,'W':0.81,'Y':0.26}}\n",
    "\n",
    "def hydrophobic_moment(sequence,scale='Normalized_consensus',angle=0,is_in_degrees=True,normalize=True):\n",
    "    # Angle should be 100 for alpha helix, 180 for beta sheet\n",
    "    hscale=scales[scale]\n",
    "    sin_sum = 0\n",
    "    cos_sum = 0\n",
    "    moment=0\n",
    "    for i in range(len(sequence)):\n",
    "        hp=hscale[sequence[i]]\n",
    "        angle_in_radians=i*angle\n",
    "        if is_in_degrees:\n",
    "            angle_in_radians = (i*angle)*math.pi/180.0\n",
    "        sin_sum += hp*math.sin(angle_in_radians)\n",
    "        cos_sum += hp*math.cos(angle_in_radians)\n",
    "    moment = math.sqrt(sin_sum**2+cos_sum**2)\n",
    "    if normalize:\n",
    "        moment = moment/len(sequence)\n",
    "    return moment\n",
    "\n",
    "def evaluate_peptide(peptide, model):\n",
    "    sequence = peptide['sequence']\n",
    "    cterm = peptide['has_cterminal_amidation']\n",
    "    return model.predict(row_to_vector(\n",
    "        {'sequence': sequence, 'has_cterminal_amidation': int(cterm)}\n",
    "    ).reshape(-1, MAX_SEQUENCE_LENGTH, len(character_to_index) + 1))\n",
    "\n",
    "\n",
    "def find_nearby_sequences(sequence, old_sequences=None, character_dict=CHARACTER_DICT):\n",
    "    new_sequences = set()\n",
    "    if old_sequences == None:\n",
    "        old_sequences = set()\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "        for c1 in character_dict:\n",
    "            for j in range(i + 1, len(sequence)):\n",
    "                for c2 in character_dict:\n",
    "                    new_sequence = sequence[:i] + c1 + sequence[i+1:j] + c2 + sequence[j+1:]\n",
    "                    for cterm in (True, False):\n",
    "                        ns_dict = {'sequence': new_sequence, 'has_cterminal_amidation': cterm}\n",
    "                        new_sequences.add(frozenset(ns_dict.items()))\n",
    "    return old_sequences | new_sequences\n",
    "\n",
    "def evaluate_peptides(peptides, model):\n",
    "    return model.predict(\n",
    "        np.array(\n",
    "            [row_to_vector(dict(p)) for p in peptides]\n",
    "        ).reshape(\n",
    "            -1, MAX_SEQUENCE_LENGTH, len(character_to_index) + 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "def nearby_peptide_vectors(peptide_vector):\n",
    "    nearby_vectors = []\n",
    "    for i in range(len(peptide_vector)):\n",
    "#         print peptide_vector[i]\n",
    "        if np.sum(peptide_vector[i][:len(peptide_vector[i])-1])>0.5:\n",
    "            for j in range(len(peptide_vector[i]) - 1):  # - 1 because of amidation\n",
    "                v = np.zeros(len(peptide_vector[i]))\n",
    "                v[-1] = peptide_vector[0][-1]\n",
    "                v[j] = 1\n",
    "                new_vector = np.concatenate([\n",
    "                    peptide_vector[:i],\n",
    "                    v.reshape(-1, len(peptide_vector[i])),\n",
    "                    peptide_vector[i+1:]\n",
    "                ])\n",
    "                if is_acceptable(vector_to_amp(new_vector)['sequence']):\n",
    "                    nearby_vectors.append(new_vector)\n",
    "                else:\n",
    "                    print 'unacceptable! '+repr(vector_to_amp(new_vector)['sequence'])\n",
    "#                     cterm_flipped = deepcopy(new_vector)\n",
    "#                     reverse_cterm = (new_vector[0][-1] + 1) % 2\n",
    "#                     for c in cterm_flipped:\n",
    "#                         c[-1] = reverse_cterm\n",
    "#                     nearby_vectors.append(cterm_flipped)\n",
    "#         else:\n",
    "#             print 'Nope!'+repr(i)\n",
    "    return nearby_vectors\n",
    "\n",
    "def is_acceptable(sequence_with_padding):\n",
    "    at_underscore=False\n",
    "    for char in sequence_with_padding:\n",
    "        if char == '_':\n",
    "            at_underscore=True\n",
    "        elif at_underscore:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def generate_sequence(cdict=CHARACTER_DICT,min_seq_length=10):\n",
    "    s = generate_random_sequence(cdict,length_of_sequence_min=min_seq_length)\n",
    "    v = row_to_vector(s)\n",
    "    last=''\n",
    "    for i in range(100):\n",
    "        vs = nearby_peptide_vectors(v)\n",
    "        ps = convmodel.predict(np.array(vs))\n",
    "        best_i = min(range(len(ps)), key=lambda x: ps[x])\n",
    "        v = vs[best_i]\n",
    "        if vector_to_amp(v)['sequence']==last:\n",
    "            break\n",
    "        last=vector_to_amp(v)['sequence']\n",
    "    return vector_to_amp(v), ps[best_i][0]\n",
    "\n",
    "def generate_sequence_from_exp_sequences(sequence_pool,max_generated_length=25):\n",
    "    sequence=''\n",
    "    for ind in range(max_generated_length):\n",
    "#         print len(sequence_pool)\n",
    "        which_ind = random.randint(0,len(sequence_pool)-1)\n",
    "#         print 'which ind: '+repr(which_ind)\n",
    "        which = sequence_pool[which_ind]\n",
    "#         print which\n",
    "        if len(sequence)<len(which):\n",
    "#             print which[len(sequence)]\n",
    "            sequence=sequence+which[len(sequence)]\n",
    "        else:\n",
    "            break\n",
    "    has_cterminal_amidation = True\n",
    "#     print len(sequence)\n",
    "    return {\n",
    "        'sequence': sequence,\n",
    "        'has_cterminal_amidation': has_cterminal_amidation\n",
    "    }\n",
    "\n",
    "def generate_move(old_vector,constraint_functions=None,min_length=10,max_length=25):\n",
    "    vector=deepcopy(old_vector)\n",
    "    asc=random.random()\n",
    "    peptide_length=0\n",
    "    for i in range(len(vector)):\n",
    "        if np.sum(vector[i][:len(vector[i])-1])>0.5:\n",
    "            peptide_length=i+1\n",
    "    if asc<0.025 and peptide_length>=min_length:\n",
    "#         Remove from the front 2.5% of the time\n",
    "        for i in range(len(vector)-1):\n",
    "            vector[i]=[k for k in vector[i+1]]\n",
    "        vector[len(vector)-1]=[0 for k in vector[0]]\n",
    "    elif asc < 0.05 and peptide_length>=min_length:\n",
    "#         Remove from the back 2.5% of the time\n",
    "        if peptide_length==len(vector):\n",
    "            vector[len(vector)-1]=[0]*len(vector[0])\n",
    "        else:\n",
    "            vector[peptide_length-1]=[k for k in vector[peptide_length]]\n",
    "            vector[peptide_length]=[0]*len(vector[0])\n",
    "    elif asc < .075 and peptide_length<max_length:\n",
    "#         Add to the front 2.5% of the time\n",
    "        which = random.randint(0,len(vector[0])-2)\n",
    "        blah=0\n",
    "        while which == character_to_index['C'] and blah<10:\n",
    "            which = random.randint(0,len(vector[0])-2)\n",
    "            blah += 1\n",
    "        for i in range(1,len(vector)):\n",
    "            vector[-i]=[k for k in vector[-i-1]]\n",
    "        vector[0]=[0 for k in vector[1]]\n",
    "        vector[0][which]=1\n",
    "    elif asc < .1 and peptide_length<max_length:\n",
    "#         Add to the back 2.5% of the time\n",
    "        which = random.randint(0,len(vector[0])-2)\n",
    "        blah=0\n",
    "        while which == character_to_index['C'] and blah<10:\n",
    "            which = random.randint(0,len(vector[0])-2)\n",
    "            blah += 1\n",
    "        vector[peptide_length][which]=1\n",
    "        if peptide_length<(len(vector)-1):\n",
    "            vector[peptide_length+1][len(vector[0])-1]=vector[peptide_length][len(vector[0])-1]\n",
    "        vector[peptide_length][-1]=0\n",
    "#     elif asc > 0.995 and peptide_length<len(vector):\n",
    "#     Toggle amidation 0.5% of the time\n",
    "#         vector[peptide_length][-1]=(vector[peptide_length][-1]+1)%2\n",
    "    else:\n",
    "#         Swap something in the middle\n",
    "        which_index=random.randint(0,peptide_length-1)\n",
    "        which_residue=random.randint(0,len(vector[0])-2)\n",
    "        blah=0\n",
    "        while which_residue == character_to_index['C'] and blah<10:\n",
    "            which_residue = random.randint(0,len(vector[0])-2)\n",
    "            blah += 1\n",
    "        try:\n",
    "#             print which_index\n",
    "            vector[which_index]=[0 for k in vector[0]]\n",
    "            vector[which_index][which_residue]=1\n",
    "        except:\n",
    "            print 'Trying to change index '+repr(which_index)+' but array is only of length '+repr(len(vector))\n",
    "    old_seq = vector_to_amp(old_vector)['sequence']\n",
    "    seq_to_check = vector_to_amp(vector)['sequence']\n",
    "    if not is_acceptable(seq_to_check):\n",
    "        print 'asc is: '+repr(asc)+' and sequence is '+repr(vector_to_amp(vector)['sequence'])\n",
    "        print 'which_residue: '+repr(which_residue)+', out of '+repr(len(vector[0])-1)\n",
    "        return old_vector\n",
    "    if not constraint_functions is None:\n",
    "        for constraint_function in constraint_functions:\n",
    "            if not constraint_function(seq_to_check,old_seq):\n",
    "                return old_vector\n",
    "    peptide_length=0\n",
    "    for i in range(len(vector)):\n",
    "        if np.sum(vector[i][:len(vector[i])-1])>0.5:\n",
    "            peptide_length=i+1\n",
    "    vector[peptide_length][-1]=1\n",
    "    return vector\n",
    "\n",
    "def accept_move(mic_old,mic_new,temp):\n",
    "    if mic_new<mic_old:\n",
    "        return True\n",
    "    return random.random()<np.exp((mic_old-mic_new)/temp)\n",
    "\n",
    "def generate_sequence_by_simulated_annealing(model,sequence_constraint_functions=None,cdict=CHARACTER_DICT,min_seq_length=10,cooling_schedule = 'Power',max_seq_length=25,nsteps=100000,t0=MAX_MIC/np.log(2),tf=0.00001/np.log(2),all_seq=None):\n",
    "    if all_seq==None:\n",
    "        s = generate_random_sequence(cdict,length_of_sequence_min=min_seq_length,length_of_sequence_max=max_seq_length,include_C=False)\n",
    "    else:\n",
    "        s = generate_sequence_from_exp_sequences(all_seq)\n",
    "    v = row_to_vector(s)\n",
    "    print 'Starting sequence: '+repr(s)\n",
    "    last=''\n",
    "    #So a transition that increases MIC by 1 will have acceptance probability of 0.5 in default\n",
    "    temp = t0\n",
    "    scale=np.power(tf/t0,1./nsteps)\n",
    "    for i in range(nsteps):\n",
    "        move = generate_move(v,constraint_functions=sequence_constraint_functions)\n",
    "        old_and_new = model.predict(np.array([v,move]))\n",
    "        if accept_move(old_and_new[0],old_and_new[1],temp):\n",
    "            v = move\n",
    "        if cooling_schedule is 'Power':\n",
    "            temp=temp*scale\n",
    "        elif cooling_schedule is 'Linear':\n",
    "            temp = temp + (tf - t0)/nsteps\n",
    "        else:\n",
    "            print 'Cooling schedule not recognized: '+cooling_schedule\n",
    "            break\n",
    "#     print vector_to_amp(v)\n",
    "#     print convmodel.predict(np.array([v]))[0][0]\n",
    "    return vector_to_amp(v), model.predict(np.array([v]))[0]\n",
    "\n",
    "def charge_constraint(new_seq,old_seq,max_charge=6):\n",
    "    new_charge = new_seq.count('K')+new_seq.count('R')\n",
    "    old_charge = old_seq.count('K')+old_seq.count('R')\n",
    "    return new_charge <= max_charge or new_charge <= old_charge\n",
    "\n",
    "def net_charge_constraint(new_seq,old_seq,max_net_charge=6):\n",
    "    new_charge = new_seq.count('K')+new_seq.count('R') - new_seq.count('E') - new_seq.count('D')\n",
    "    old_charge = old_seq.count('K')+old_seq.count('R') - old_seq.count('E') - old_seq.count('D')\n",
    "    return new_charge <= max_net_charge or new_charge <= old_charge\n",
    "\n",
    "def charge_density_constraint(new_seq,old_seq,max_charge_density=0.4):\n",
    "    new_charge_density = float(new_seq.count('K')+new_seq.count('R'))/(len(new_seq)-new_seq.count('_'))\n",
    "    old_charge_density = float(old_seq.count('K')+old_seq.count('R'))/(len(old_seq)-old_seq.count('_'))\n",
    "    return new_charge_density <= max_charge_density or new_charge_density <= old_charge_density\n",
    "    \n",
    "def constant_length_constraint(new_seq,old_seq):\n",
    "    return new_seq.count('_') == old_seq.count('_')\n",
    "\n",
    "def generate_random_sequences(cdict=CHARACTER_DICT,min_seq_length=10):\n",
    "    s = generate_random_sequence(cdict,length_of_sequence_min = min_seq_length, include_C = False)\n",
    "    v = row_to_vector(s)\n",
    "    last=''\n",
    "    p=convmodel.predict(np.array([v]))\n",
    "    return vector_to_amp(v), p[0][0]\n",
    "\n",
    "# For FASTA search\n",
    "def print_seqs_for_fasta_search(sequences):\n",
    "    for i,sequence in enumerate(sequences):\n",
    "        print '>s'+repr(i)\n",
    "        print sequence\n",
    "        \n",
    "def seqs_to_fasta(sequences):\n",
    "    to_return = ''\n",
    "    for i,sequence in enumerate(sequences):\n",
    "        to_return += '>s'+repr(i)+'\\n'+sequence+'\\n'\n",
    "    return to_return\n",
    "\n",
    "def fasta_to_seqs(fasta_fname):\n",
    "    to_return=[]\n",
    "    freader = open(fasta_fname,'r')\n",
    "    parity = False\n",
    "    for line in freader:\n",
    "        if parity:\n",
    "            to_return.append(line[0:(len(line)-1)])\n",
    "        parity = ~parity\n",
    "    return to_return\n",
    "        \n",
    "def show_best_n_alignments(test_sequence,sequence_db,nalign,score_matrix=matlist.pam30,gap_open_penalty=-9,gap_extension_penalty=-1):\n",
    "#     blosum62 is another option. Identity matrix is another. I think they're about the same.\n",
    "    alignment_scores=[0]*len(sequence_db)\n",
    "    for i in range(len(sequence_db)):\n",
    "        sequence=sequence_db[i]\n",
    "        alignment_scores[i]=pairwise2.align.localds(test_sequence,sequence,score_matrix,gap_open_penalty,gap_extension_penalty,score_only=True)\n",
    "    argm = np.argmax(alignment_scores)\n",
    "#     alignments=pairwise2.align.localds(test_sequence,all_sequences[argm],score_matrix,gap_open_penalty,gap_extension_penalty)\n",
    "    indices=np.argpartition(alignment_scores,-1*nalign)[(-1*nalign):]\n",
    "    for index in indices:\n",
    "        alignments=pairwise2.align.localds(test_sequence,all_sequences[index],score_matrix,gap_open_penalty,gap_extension_penalty)\n",
    "        print(pairwise2.format_alignment(*alignments[0]))\n",
    "        \n",
    "def hmoment_analysis(test_sequence,angles=[100,140,160,180]):\n",
    "#     100 degrees is alpha helix, 160 degrees is beta sheet (?)\n",
    "    hmoments=[0]*len(angles)\n",
    "    percentiles=[0]*len(angles)\n",
    "    for k in range(len(angles)):\n",
    "        test_angle=angles[k]\n",
    "        hmoments[k] = hydrophobic_moment(test_sequence,angle=test_angle)\n",
    "        other_h_moments=[0]*1000\n",
    "        shuffled=range(len(test_sequence))\n",
    "        perGreater=0\n",
    "        for i in range(1000):\n",
    "            np.random.shuffle(shuffled)\n",
    "            shuffled_seq=[test_sequence[j] for j in shuffled]\n",
    "            other_h_moments[i]=hydrophobic_moment(shuffled_seq,angle=test_angle)\n",
    "            if other_h_moments[i]<hydrophobic_moment(test_sequence,angle=test_angle):\n",
    "                perGreater+=.1\n",
    "        percentiles[k]=perGreater\n",
    "    return hmoments,percentiles\n",
    "\n",
    "def constraints_to_string(constraints):\n",
    "    toReturn = ''\n",
    "    if constraints is None:\n",
    "        return 'No_constraint'\n",
    "    for constraint in constraints:\n",
    "        if constraint is net_charge_constraint:\n",
    "            toReturn += 'Net_charge_constraint_+6,'\n",
    "        elif constraint is charge_constraint:\n",
    "            toReturn += 'Charge_constraint_+6,'\n",
    "        elif constraint is constant_length_constraint:\n",
    "            toReturn += 'Constant_length_constraint,'\n",
    "        elif constraint is charge_density_constraint:\n",
    "            toReturn += 'Charge_density_constraint_0.4',\n",
    "    return toReturn\n",
    "\n",
    "def wipe_out_amidation(test_x_array):\n",
    "    to_return = deepcopy(test_x_array)\n",
    "    for testx in to_return:\n",
    "        for row in testx:\n",
    "            row[-1]=0\n",
    "    return to_return\n",
    "\n",
    "def run_to_row(sequence_with_underscores,pred_log_mic,constraints,nsteps,cooling_function,min_length=10,max_length=25,t0=MAX_MIC/np.log(2),tf=.00001/np.log(2)):\n",
    "    sequence = sequence_with_underscores[:sequence_with_underscores.find('_')]\n",
    "    moments,percentiles=hmoment_analysis(sequence)\n",
    "    alignment_scores=[0]*len(all_sequences)\n",
    "    for i in range(len(all_sequences)):\n",
    "        seq=all_sequences[i]\n",
    "        alignment_scores[i]=pairwise2.align.localds(seq,sequence,matlist.pam30,-9,-1,score_only=True)\n",
    "    argm = np.argmax(alignment_scores)\n",
    "    return {'Sequence':sequence,'Pred_log_MIC':pred_log_mic,\n",
    "                'Best_match':all_sequences[argm],'Constraints':constraints_to_string(constraints),'100_hmoment':moments[0],\n",
    "                 '100_hmoment_percentile':percentiles[0],'Alignment_score':max(alignment_scores),\n",
    "                 'Sim_anneal_steps':nsteps,'t0':t0,'tf':tf,'Cooling_function':cooling_function,\n",
    "                 'Min_length':min_length,'Max_length':max_length\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulated annealing, tracking runtime. You can test different settings\n",
    "start = time.time()\n",
    "nseqs=25\n",
    "rows=[]\n",
    "# constraints=[net_charge_constraint,constant_length_constraint]\n",
    "constraints = None\n",
    "cooling_schedule='Power'\n",
    "niter=10000\n",
    "for i in range(nseqs):\n",
    "    seq,plm=generate_sequence_by_simulated_annealing(everything_ensemble_model,nsteps=niter,cooling_schedule=cooling_schedule,sequence_constraint_functions = constraints,all_seq=None)\n",
    "    rows.append(run_to_row(seq['sequence'],plm,constraints,niter,cooling_schedule))\n",
    "end=time.time()\n",
    "print 'runtime in seconds: '+repr(end-start)\n",
    "\n",
    "start = time.time()\n",
    "nseqs=30\n",
    "# rows=[]\n",
    "constraints=[charge_constraint]\n",
    "constraint = None\n",
    "cooling_schedule='Power'\n",
    "niter=10000\n",
    "for i in range(nseqs):\n",
    "    seq,plm=generate_sequence_by_simulated_annealing(everything_ensemble_model,nsteps=niter,cooling_schedule=cooling_schedule,sequence_constraint_functions = constraints,all_seq=None)\n",
    "    rows.append(run_to_row(seq['sequence'],plm,constraints,niter,cooling_schedule))\n",
    "end=time.time()\n",
    "print 'runtime in seconds: '+repr(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this to add the peptides you just generated to the old peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_peptide_df=pd.read_pickle('models/Saved_variables/new_peptide_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_peptide_df = pd.concat([old_peptide_df,new_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_peptide_df.to_pickle('models/Saved_variables/new_peptide_info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing how good our peptide is against the other peptides in the database\n",
    "my_mic=np.log10(3)\n",
    "better_than=0\n",
    "worse_than=0\n",
    "for val in ecoli_df.value:\n",
    "    if my_mic < val:\n",
    "        better_than+=1\n",
    "    else:\n",
    "        worse_than+=1\n",
    "print better_than\n",
    "print worse_than\n",
    "print 1-float(worse_than)/len(ecoli_df)\n",
    "print float(worse_than)/(better_than+worse_than)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_peptide_df=pd.read_pickle('models/Saved_variables/new_peptide_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_peptide_df = all_peptide_df[all_peptide_df.Cooling_function=='Power'].reset_index(drop=True)\n",
    "# The ones with the linear cooling function tended to be a lot worse so I dropped those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print set(all_peptide_df.Constraints)\n",
    "no_constraint_peptides = all_peptide_df[all_peptide_df.Constraints=='No_constraint'].reset_index(drop=True)\n",
    "charge_constraint_peptides1 = all_peptide_df[all_peptide_df.Constraints=='Charge_constraint_+6'].reset_index(drop=True)\n",
    "charge_constraint_peptides2 = all_peptide_df[all_peptide_df.Constraints=='Charge_constraint_+6,'].reset_index(drop=True)\n",
    "# Combining the ones with the charge constraint with and without a comma, just a quirk in how they were labeled\n",
    "charge_constraint_peptides = pd.concat([charge_constraint_peptides1,charge_constraint_peptides2]).reset_index(drop=True)\n",
    "charge_density_constraint_peptides = all_peptide_df[all_peptide_df.Constraints=='Charge_density_constraint_0.4'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_um_df = all_peptide_df[all_peptide_df['Pred_log_MIC']<0].reset_index(drop=True)\n",
    "sub_um_good_hmoment_df = sub_um_df[sub_um_df['100_hmoment_percentile']>90].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_peptide_df.sort_values(['Pred_log_MIC'])[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_constraint_100 = no_constraint_peptides['100_hmoment_percentile']\n",
    "charge_density_100 = charge_density_constraint_peptides['100_hmoment_percentile']\n",
    "charge_constraint_100 = charge_constraint_peptides['100_hmoment_percentile']\n",
    "no_constraint_hmoments = [hmoment_analysis(seq) for seq in no_constraint_peptides.Sequence]\n",
    "charge_constraint_hmoments = [hmoment_analysis(seq) for seq in charge_constraint_peptides.Sequence]\n",
    "charge_density_constraint_hmoments = [hmoment_analysis(seq) for seq in charge_density_constraint_peptides.Sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_constraint_pred_log_mic = no_constraint_peptides['Pred_log_MIC']\n",
    "charge_constraint_pred_log_mic = charge_constraint_peptides['Pred_log_MIC']\n",
    "charge_density_constraint_pred_log_mic = charge_density_constraint_peptides['Pred_log_MIC']\n",
    "experimental_mic = ecoli_df.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_labels = ['Cationic AAs','Cationic AA density','Experimental']\n",
    "plt.hist([charge_constraint_pred_log_mic,charge_density_constraint_pred_log_mic,experimental_mic],normed=1,label=mic_labels,bins=range(-1,4))\n",
    "plt.xlabel('log MIC')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(range(-1,4))\n",
    "plt.legend(prop={'size':16})\n",
    "plt.savefig('Figures/log_MIC_hist.png',bbox_inches='tight',frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hmoments = [hmoment_analysis(seq) for seq in ecoli_df.sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_constraint_100 = [hmoments[1][0] for hmoments in no_constraint_hmoments]\n",
    "charge_density_constraint_100 = [hmoments[1][0] for hmoments in charge_density_constraint_hmoments]\n",
    "charge_constraint_100 = [hmoments[1][0] for hmoments in charge_constraint_hmoments]\n",
    "dataset_100 = [hmoments[1][0] for hmoments in dataset_hmoments]\n",
    "\n",
    "no_constraint_140 = [hmoments[1][1] for hmoments in no_constraint_hmoments]\n",
    "charge_density_constraint_140 = [hmoments[1][1] for hmoments in charge_density_constraint_hmoments]\n",
    "charge_constraint_140 = [hmoments[1][1] for hmoments in charge_constraint_hmoments]\n",
    "dataset_140 = [hmoments[1][1] for hmoments in dataset_hmoments]\n",
    "\n",
    "no_constraint_180 = [hmoments[1][3] for hmoments in no_constraint_hmoments]\n",
    "charge_density_constraint_180 = [hmoments[1][3] for hmoments in charge_density_constraint_hmoments]\n",
    "charge_constraint_180 = [hmoments[1][3] for hmoments in charge_constraint_hmoments]\n",
    "dataset_180 = [hmoments[1][3] for hmoments in dataset_hmoments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmoment_labels = ['Cationic AAs','Cationic AA density','Experimental']\n",
    "\n",
    "plt.hist([charge_density_constraint_100,charge_constraint_100,dataset_100],bins=10,normed=1,label=hmoment_labels)\n",
    "# plt.hist(,bins=20)\n",
    "plt.xlabel('HM percentile for alpha helix',fontsize=16)\n",
    "plt.ylabel('Frequency',fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks([0.02*i for i in range(5)],fontsize=16)\n",
    "plt.legend()\n",
    "plt.legend(loc='upper left',prop={'size':16})\n",
    "plt.savefig('Figures/alpha_helix_hmoment.png',bbox_inches='tight',frameon=False)\n",
    "# plt.ylabel('')\n",
    "plt.show()\n",
    "\n",
    "plt.hist([charge_density_constraint_140,charge_constraint_140,dataset_140],bins=10,normed=1,label=hmoment_labels)\n",
    "# plt.hist(,bins=20)\n",
    "plt.xlabel('HM percentile for $140^\\circ$ rotation',fontsize=16)\n",
    "plt.yticks([0.01*i for i in range(4)],fontsize=16)\n",
    "plt.ylabel('Frequency',fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.legend()\n",
    "plt.legend(loc='best',prop={'size':14})\n",
    "plt.savefig('Figures/140_degree_hmoment.png',bbox_inches='tight',frameon=False)\n",
    "# plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "plt.hist([charge_density_constraint_180,charge_constraint_180,dataset_180],bins=10,normed=1,label=hmoment_labels)\n",
    "# plt.hist(,bins=20)\n",
    "plt.xlabel('HM percentile for beta sheet',fontsize=16)\n",
    "plt.yticks([0.01*i for i in range(5)],fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.legend()\n",
    "plt.legend(loc='upper left',prop={'size':14})\n",
    "plt.savefig('Figures/beta_sheet_hmoment.png',bbox_inches='tight',frameon=False)\n",
    "# plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# plt.hist(all_peptide_df['100_hmoment'],bins=20)\n",
    "# plt.xlabel('HM for alpha helix')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()\n",
    "# plt.hist(all_peptide_df['Pred_log_MIC'],bins=20)\n",
    "# plt.xlabel('Predicted log MIC')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([no_constraint_100,charge_density_constraint_100,charge_constraint_100],bins=10)\n",
    "plt.xlabel('HM percentile for alpha helix')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at best alignments for peptides of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_seqs = ['GWKVWKKILAKLAKLAK','WRRWWKILKAALAKLAK','WRWWKILKKLATVLKKLITLGTLA']\n",
    "# good_seqs = good_seqs + ['RRWKWRKLAKVLTTLLRGGKRIQRL','WRKFWKKILKKLAKLAKTGLRKPII','GWRWWKKLAKVLAKLAK']\n",
    "# good_seqs = ['WRKFLLKILKLLVQRYR','KWKKFWKVLKKVF']\n",
    "# good_seqs = ['RRWKWRKLAKVLTTLLRGGKRIQRL','WRRWWKILKAALAKLAK','WRRWWKILKAALAKLAK','RKWIWWKLAKVLAKLAK','GFWKKILKKLATKLAKLAK']\n",
    "good_seqs = ['RKWIWWKLAKVLAKLAK','GFWKKILKKLATKLAKLAK','GWKTLAKLAKKLAKLAK','WRKFWKKILKKLAKLAKTGLRKPII']\n",
    "for seq in good_seqs:\n",
    "    print all_peptide_df.loc[all_peptide_df.Sequence.str.contains(seq)]['Best_match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print all_peptide_df.loc[all_peptide_df.Sequence.str.contains('DAHKLAKLAKKLAKLAK')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_matches = ['VNWKKILAKIIKVAK','KWWRWRRWW','KWKSFIKKLTSVLKKVVTTAKPLISS','FKCWRWQWRWKKLGAKVFKRLEKLFSKI','KWKKLLKKLLKLPKKLLKKLKKLLK','FKCWRWQWRWKKLGAKVFKRLEKLFSKI']\n",
    "# seq_matches = ['KWKKLLKKLLKLL','KWKKFKKIGAVLKKL']\n",
    "# seq_matches = ['FKCWRWQWRWKKLGAKVFKRLEKLFSKI','KWWRWRRWW','DAHKLAKLAKKLAKLAK']\n",
    "seq_matches=['DAHKLAKLAKKLAKLAK','DAHKLAKLAKKLAKLAK','DAHKLAKLAKKLAKLAK','KWKKLLKKLLKLPKKLLKKLKKLLK']\n",
    "for index,my_seq in enumerate(good_seqs):\n",
    "    their_seq = seq_matches[index]\n",
    "#     print df.loc[df.sequence.str.contains('KWKSFIKKLTSVLKKVVTTAKPLISS')]\n",
    "    print 'Pam30 matrix:'\n",
    "    alignment=pairwise2.align.localds(my_seq,their_seq,matlist.pam30,-9,-1)\n",
    "    print pairwise2.format_alignment(*alignment[0])\n",
    "#     print 'Identity matrix:'\n",
    "#     alignment=pairwise2.align.localds(my_seq,their_seq,matlist.ident,-9,-1)\n",
    "#     print pairwise2.format_alignment(*alignment[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing model preformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First assemble decoy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decoys = []\n",
    "# fnames = ['Veltri_data/DECOY.eval.fa','Veltri_data/DECOY.te.fa','Veltri_data/DECOY.tr.fa']\n",
    "fnames = ['Fasta_files/Uniprot_negatives.txt']\n",
    "for fname in fnames:\n",
    "    fasta_sequences=SeqIO.parse(fname,'fasta')\n",
    "    for fasta in fasta_sequences:\n",
    "        seq = str(fasta.seq)\n",
    "        all_decoys.append(seq)\n",
    "print len(all_decoys)\n",
    "decoys_no_cysteine = []\n",
    "for seq in all_decoys:\n",
    "    if len(seq)>MAX_SEQUENCE_LENGTH:\n",
    "        tries=0\n",
    "        added = False\n",
    "        while tries<15 and not added:\n",
    "            start = random.randint(0,len(seq)-MAX_SEQUENCE_LENGTH)\n",
    "            to_add = seq[start:(start+MAX_SEQUENCE_LENGTH)]\n",
    "            tries += 1\n",
    "            if 'C' not in to_add:\n",
    "                decoys_no_cysteine.append(to_add)\n",
    "                added = True\n",
    "decoy_set = set(decoys_no_cysteine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniprot_testing_set(seqs,yvals,decoy_set,include_cysteine = True):\n",
    "    deep_decoy_set = deepcopy(decoy_set)\n",
    "#     print len(deep_decoy_set)\n",
    "    lengths = []\n",
    "    output_pos_seqs = []\n",
    "    output_pos_yvals = []\n",
    "    output_neg_seqs = []\n",
    "    output_neg_yvals = []\n",
    "    for i,seq in enumerate(seqs):\n",
    "        if len(seq)<MAX_SEQUENCE_LENGTH:\n",
    "            output_pos_seqs.append(seq)\n",
    "            output_pos_yvals.append(yvals[i])\n",
    "            output_neg_yvals.append(MAX_MIC)\n",
    "#             print i\n",
    "#             done = False\n",
    "            alt_seq = ''\n",
    "            for decoy_seq in deep_decoy_set:\n",
    "#                 if len(decoy_seq) == len(seq):\n",
    "#                     output_neg_seqs.append(decoy_seq)\n",
    "#                     done = True\n",
    "#                     deep_decoy_set.remove(decoy_seq)\n",
    "#                     break\n",
    "                if len(decoy_seq) >= len(seq):\n",
    "                    alt_seq = decoy_seq\n",
    "                    break\n",
    "#             if not done:\n",
    "            start = random.randint(0,len(alt_seq)-len(seq))\n",
    "            output_neg_seqs.append(alt_seq[start:(start+len(seq))])\n",
    "            deep_decoy_set.remove(alt_seq)\n",
    "                \n",
    "    return output_pos_seqs, output_neg_seqs, output_pos_yvals, output_neg_yvals\n",
    "\n",
    "def random_testing_set(seqs,yvals,include_cysteine = True,keep_maxlength_seqs=False):\n",
    "    lengths = []\n",
    "    output_pos_seqs = []\n",
    "    output_pos_yvals = []\n",
    "    output_neg_seqs = []\n",
    "    output_neg_yvals = []\n",
    "    for i,seq in enumerate(seqs):\n",
    "        if len(seq)<MAX_SEQUENCE_LENGTH or keep_maxlength_seqs:\n",
    "            output_pos_seqs.append(seq)\n",
    "            output_pos_yvals.append(yvals[i])\n",
    "            output_neg_yvals.append(MAX_MIC)\n",
    "            output_neg_seqs.append(generate_random_sequence(list(CHARACTER_DICT),include_C=False,fixed_length=len(seq),just_seq=True))\n",
    "                \n",
    "    return output_pos_seqs, output_neg_seqs, output_pos_yvals, output_neg_yvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = ecoli_test.reset_index().sequence\n",
    "vals = ecoli_test.reset_index().value\n",
    "output_pos_seqs, output_neg_seqs, output_pos_yvals, output_neg_yvals = uniprot_testing_set(seqs,vals,decoy_set)\n",
    "all_test_seqs = output_pos_seqs + output_neg_seqs\n",
    "all_test_y_vals = output_pos_yvals + output_neg_yvals\n",
    "\n",
    "p9_y_vals = []\n",
    "p9_seqs = []\n",
    "for i,seq in enumerate(seqs):\n",
    "    if seq in seqs_p9_cutoff:\n",
    "        p9_y_vals.append(vals[i])\n",
    "        p9_seqs.append(seq)\n",
    "\n",
    "p7_y_vals = []\n",
    "p7_seqs = []\n",
    "for i,seq in enumerate(seqs):\n",
    "    if seq in seqs_p7_cutoff:\n",
    "        p7_y_vals.append(vals[i])\n",
    "        p7_seqs.append(seq)\n",
    "\n",
    "output_pos_seqs_p9, output_neg_seqs_p9, output_pos_yvals_p9, output_neg_yvals_p9 = uniprot_testing_set(p9_seqs,p9_y_vals,decoy_set)\n",
    "all_test_seqs_p9 = output_pos_seqs_p9 + output_neg_seqs_p9\n",
    "all_test_y_vals_p9 = output_pos_yvals_p9 + output_neg_yvals_p9\n",
    "# print len(all_test_seqs_p9)\n",
    "# print len(output_pos_seqs_p9)\n",
    "\n",
    "output_pos_seqs_p7, output_neg_seqs_p7, output_pos_yvals_p7, output_neg_yvals_p7 = uniprot_testing_set(p7_seqs,p7_y_vals,decoy_set)\n",
    "all_test_seqs_p7 = output_pos_seqs_p7 + output_neg_seqs_p7\n",
    "all_test_y_vals_p7 = output_pos_yvals_p7 + output_neg_yvals_p7\n",
    "# print len(all_test_seqs_p7)\n",
    "# print len(output_pos_seqs_p7)\n",
    "\n",
    "\n",
    "vectors_with_uniprot = []\n",
    "for i,seq in enumerate(all_test_seqs):\n",
    "    vectors_with_uniprot.append(sequence_to_vector(seq,False))\n",
    "vectors_with_uniprot = np.array(vectors_with_uniprot)\n",
    "\n",
    "vectors_with_uniprot_p9 = []\n",
    "for i,seq in enumerate(all_test_seqs_p9):\n",
    "    vectors_with_uniprot_p9.append(sequence_to_vector(seq,False))\n",
    "vectors_with_uniprot_p9 = np.array(vectors_with_uniprot_p9)\n",
    "\n",
    "vectors_with_uniprot_p7 = []\n",
    "for i,seq in enumerate(all_test_seqs_p7):\n",
    "    vectors_with_uniprot_p7.append(sequence_to_vector(seq,False))\n",
    "vectors_with_uniprot_p7 = np.array(vectors_with_uniprot_p7)\n",
    "\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list:\n",
    "    all_preds.append(emodel.predict(vectors_with_uniprot))\n",
    "output_file = open('Figures/predictions_vs_uniprot.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_uniprot):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "seqs_for_fasta = []\n",
    "for vect in vectors_with_uniprot:\n",
    "    seq = vector_to_amp(vect)['sequence']\n",
    "    if seq.find('_')>-1:\n",
    "        seq = seq[0:seq.find('_')]\n",
    "    seqs_for_fasta.append(seq)\n",
    "for_fasta = seqs_to_fasta(seqs_for_fasta)\n",
    "test_file = open('Fasta_files/test_fasta_with_negatives_vs_uniprot.txt','w')\n",
    "test_file.write(for_fasta)\n",
    "test_file.close()\n",
    "\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list_avgs:\n",
    "    all_preds.append(emodel.predict(vectors_with_uniprot))\n",
    "output_file = open('Figures/predictions_vs_uniprot_avgs.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_uniprot):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list:\n",
    "    all_preds.append(emodel.predict(vectors_with_uniprot_p9))\n",
    "output_file_p9 = open('Figures/predictions_vs_uniprot_p9.csv','w')\n",
    "output_file_p9.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_uniprot_p9):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file_p9.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals_p9[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file_p9.write(repr(all_preds[k][i])+',')\n",
    "    output_file_p9.write('\\n')\n",
    "output_file_p9.close()\n",
    "\n",
    "seqs_for_fasta = []\n",
    "for vect in vectors_with_uniprot_p9:\n",
    "    seq = vector_to_amp(vect)['sequence']\n",
    "    if seq.find('_')>-1:\n",
    "        seq = seq[0:seq.find('_')]\n",
    "    seqs_for_fasta.append(seq)\n",
    "for_fasta = seqs_to_fasta(seqs_for_fasta)\n",
    "test_file = open('Fasta_files/test_fasta_with_negatives_vs_uniprot_p9.txt','w')\n",
    "test_file.write(for_fasta)\n",
    "test_file.close()\n",
    "\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list:\n",
    "    all_preds.append(emodel.predict(vectors_with_uniprot_p7))\n",
    "output_file_p7 = open('Figures/predictions_vs_uniprot_p7.csv','w')\n",
    "output_file_p7.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_uniprot_p7):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file_p7.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals_p7[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file_p7.write(repr(all_preds[k][i])+',')\n",
    "    output_file_p7.write('\\n')\n",
    "output_file_p7.close()\n",
    "\n",
    "seqs_for_fasta = []\n",
    "for vect in vectors_with_uniprot_p7:\n",
    "    seq = vector_to_amp(vect)['sequence']\n",
    "    if seq.find('_')>-1:\n",
    "        seq = seq[0:seq.find('_')]\n",
    "    seqs_for_fasta.append(seq)\n",
    "for_fasta = seqs_to_fasta(seqs_for_fasta)\n",
    "test_file = open('Fasta_files/test_fasta_with_negatives_vs_uniprot_p7.txt','w')\n",
    "test_file.write(for_fasta)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds=[]\n",
    "for emodel in ensemble_model_list_avgs:\n",
    "    all_preds.append(emodel.predict(vectors_with_uniprot_p9))\n",
    "output_file = open('Figures/predictions_vs_uniprot_p9_avgs.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_uniprot_p9):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list_avgs:\n",
    "    all_preds.append(emodel.predict(vectors_with_random_p9))\n",
    "output_file = open('Figures/predictions_vs_random_p9_avgs.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_random_p9):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list_avgs:\n",
    "    all_preds.append(emodel.predict(vectors_with_uniprot_p7))\n",
    "output_file = open('Figures/predictions_vs_uniprot_p7_avgs.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_uniprot_p9):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list_avgs:\n",
    "    all_preds.append(emodel.predict(vectors_with_random_p7))\n",
    "output_file = open('Figures/predictions_vs_random_p7_avgs.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_random_p9):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = ecoli_test.reset_index().sequence\n",
    "vals = ecoli_test.reset_index().value\n",
    "output_pos_seqs, output_neg_seqs, output_pos_yvals, output_neg_yvals = random_testing_set(seqs,vals)\n",
    "all_test_seqs = output_pos_seqs + output_neg_seqs\n",
    "all_test_y_vals = output_pos_yvals + output_neg_yvals\n",
    "\n",
    "p9_y_vals = []\n",
    "p9_seqs = []\n",
    "for i,seq in enumerate(seqs):\n",
    "    if seq in seqs_p9_cutoff:\n",
    "        p9_y_vals.append(vals[i])\n",
    "        p9_seqs.append(seq)\n",
    "\n",
    "p7_y_vals = []\n",
    "p7_seqs = []\n",
    "for i,seq in enumerate(seqs):\n",
    "    if seq in seqs_p7_cutoff:\n",
    "        p7_y_vals.append(vals[i])\n",
    "        p7_seqs.append(seq)\n",
    "\n",
    "output_pos_seqs_p9, output_neg_seqs_p9, output_pos_yvals_p9, output_neg_yvals_p9 = random_testing_set(p9_seqs,p9_y_vals)\n",
    "all_test_seqs_p9 = output_pos_seqs_p9 + output_neg_seqs_p9\n",
    "all_test_y_vals_p9 = output_pos_yvals_p9 + output_neg_yvals_p9\n",
    "# print len(all_test_seqs_p9)\n",
    "# print len(output_pos_seqs_p9)\n",
    "\n",
    "output_pos_seqs_p7, output_neg_seqs_p7, output_pos_yvals_p7, output_neg_yvals_p7 = random_testing_set(p7_seqs,p7_y_vals)\n",
    "all_test_seqs_p7 = output_pos_seqs_p7 + output_neg_seqs_p7\n",
    "all_test_y_vals_p7 = output_pos_yvals_p7 + output_neg_yvals_p7\n",
    "# print len(all_test_seqs_p7)\n",
    "# print len(output_pos_seqs_p7)\n",
    "\n",
    "\n",
    "vectors_with_random = []\n",
    "for i,seq in enumerate(all_test_seqs):\n",
    "    vectors_with_random.append(sequence_to_vector(seq,False))\n",
    "vectors_with_random = np.array(vectors_with_random)\n",
    "\n",
    "vectors_with_random_p9 = []\n",
    "for i,seq in enumerate(all_test_seqs_p9):\n",
    "    vectors_with_random_p9.append(sequence_to_vector(seq,False))\n",
    "vectors_with_random_p9 = np.array(vectors_with_random_p9)\n",
    "\n",
    "vectors_with_random_p7 = []\n",
    "for i,seq in enumerate(all_test_seqs_p7):\n",
    "    vectors_with_random_p7.append(sequence_to_vector(seq,False))\n",
    "vectors_with_random_p7 = np.array(vectors_with_random_p7)\n",
    "\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list:\n",
    "    all_preds.append(emodel.predict(vectors_with_random))\n",
    "output_file = open('Figures/predictions_vs_random.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_random):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "seqs_for_fasta = []\n",
    "for vect in vectors_with_random:\n",
    "    seq = vector_to_amp(vect)['sequence']\n",
    "    if seq.find('_')>-1:\n",
    "        seq = seq[0:seq.find('_')]\n",
    "    seqs_for_fasta.append(seq)\n",
    "for_fasta = seqs_to_fasta(seqs_for_fasta)\n",
    "test_file = open('Fasta_files/test_fasta_with_negatives_vs_random.txt','w')\n",
    "test_file.write(for_fasta)\n",
    "test_file.close()\n",
    "\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list_avgs:\n",
    "    all_preds.append(emodel.predict(vectors_with_random))\n",
    "output_file = open('Figures/predictions_vs_random_avgs.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_random):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list:\n",
    "    all_preds.append(emodel.predict(vectors_with_random_p9))\n",
    "output_file_p9 = open('Figures/predictions_vs_random_p9.csv','w')\n",
    "output_file_p9.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_random_p9):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file_p9.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals_p9[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file_p9.write(repr(all_preds[k][i])+',')\n",
    "    output_file_p9.write('\\n')\n",
    "output_file_p9.close()\n",
    "\n",
    "seqs_for_fasta = []\n",
    "for vect in vectors_with_random_p9:\n",
    "    seq = vector_to_amp(vect)['sequence']\n",
    "    if seq.find('_')>-1:\n",
    "        seq = seq[0:seq.find('_')]\n",
    "    seqs_for_fasta.append(seq)\n",
    "for_fasta = seqs_to_fasta(seqs_for_fasta)\n",
    "test_file = open('Fasta_files/test_fasta_with_negatives_vs_random_p9.txt','w')\n",
    "test_file.write(for_fasta)\n",
    "test_file.close()\n",
    "\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list:\n",
    "    all_preds.append(emodel.predict(vectors_with_random_p7))\n",
    "output_file_p7 = open('Figures/predictions_vs_random_p7.csv','w')\n",
    "output_file_p7.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(vectors_with_random_p7):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file_p7.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(all_test_y_vals_p7[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file_p7.write(repr(all_preds[k][i])+',')\n",
    "    output_file_p7.write('\\n')\n",
    "output_file_p7.close()\n",
    "\n",
    "seqs_for_fasta = []\n",
    "for vect in vectors_with_random_p7:\n",
    "    seq = vector_to_amp(vect)['sequence']\n",
    "    if seq.find('_')>-1:\n",
    "        seq = seq[0:seq.find('_')]\n",
    "    seqs_for_fasta.append(seq)\n",
    "for_fasta = seqs_to_fasta(seqs_for_fasta)\n",
    "test_file = open('Fasta_files/test_fasta_with_negatives_vs_random_p7.txt','w')\n",
    "test_file.write(for_fasta)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN and OCNN (one-class nearest neighbors) based classification (OCNN performed worse and we didn't include it in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli_train_train_no_c = pd.read_pickle('Saved_variables/ecoli_train_train_no_c_df.pkl')\n",
    "ecoli_validate_no_c = pd.read_pickle('Saved_variables/ecoli_validate_no_c_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "def nn_distance(seq,seq_db):\n",
    "    min_dist = 100\n",
    "    for seq_in_db in seq_db:\n",
    "        dist = levenshtein(seq,seq_in_db)\n",
    "        min_dist = min(dist,min_dist)\n",
    "    return min_dist\n",
    "\n",
    "def knn_distances(seq, seq_db, k):\n",
    "    distances = [levenshtein(seq, seq_in_db) for seq_in_db in seq_db]\n",
    "    return sorted(distances)[:k]\n",
    "\n",
    "def avg_knn_distance(seq,seq_db,k):\n",
    "    return np.mean(knn_distances(seq,seq_db,k))\n",
    "\n",
    "def kjnn_distance_ratio(seq,seq_db,j,k):\n",
    "    min_dists = [100]*j\n",
    "    min_dist_seqs = [' ']*j\n",
    "    for i,seq_in_db in enumerate(seq_db):\n",
    "        dist = levenshtein(seq,seq_in_db)\n",
    "        for ind,min_dist in enumerate(min_dists):\n",
    "            if dist<min_dist:\n",
    "                min_dists[ind]=dist\n",
    "                min_dist_seqs[ind]=seq_in_db\n",
    "                break\n",
    "    second_neighbor_dists = []\n",
    "    for second_seq in min_dist_seqs:\n",
    "        second_neighbor_dists.append(avg_knn_distance(second_seq,seq_db,k+1)*float(k+1)/float(k))\n",
    "    return np.mean(min_dists),np.mean(second_neighbor_dists),np.mean(min_dists)/np.mean(second_neighbor_dists)\n",
    "\n",
    "def knn_all_distances_with_negatives(query_seqs,pos_seq_db,neg_seq_db,kmax):\n",
    "    pos_distances = [knn_distances(seq,pos_seq_db,k) for seq in query_seqs]\n",
    "    neg_distances = [knn_distances(seq,neg_seq_db,k) for seq in query_seqs]\n",
    "    return pos_distances,neg_distances\n",
    "\n",
    "def vote_fractions_by_k(pos_distances,neg_distances,kmax):\n",
    "    to_return=[]\n",
    "    for k in range(1,kmax+1):\n",
    "        kfrac=0.\n",
    "        pos_ind_on=0\n",
    "        neg_ind_on=0\n",
    "        for i in range(0,k):\n",
    "            if pos_distances[pos_ind_on]<neg_distances[neg_ind_on]:\n",
    "                pos_ind_on +=1\n",
    "                kfrac += 1.0/k\n",
    "            else:\n",
    "                neg_ind_on += 1\n",
    "        to_return.append(kfrac)\n",
    "    return to_return\n",
    "\n",
    "def vote_fraction(seq,pos_seq_db,neg_seq_db,k):\n",
    "    pos_distances = knn_distances(seq,pos_seq_db,k)\n",
    "    neg_distances = knn_distances(seq,neg_seq_db,k)\n",
    "    frac=0.\n",
    "    pos_ind_on=0\n",
    "    neg_ind_on=0\n",
    "    for i in range(0,k):\n",
    "        if pos_distances[pos_ind_on]<neg_distances[neg_ind_on]:\n",
    "            pos_ind_on +=1\n",
    "            frac += 1.0/k\n",
    "        else:\n",
    "            neg_ind_on += 1\n",
    "    return frac\n",
    "\n",
    "def vote_fractions_matrix(all_distances_to_pos,all_distances_to_neg,kmax):\n",
    "    return [vote_fractions_by_k(all_distances_to_pos[i],all_distances_to_neg[i],kmax) for i in range(len(all_distances_to_pos))]\n",
    "    \n",
    "def mcc_matrix(vote_fracs_pos_queries,vote_fracs_neg_queries,vote_cutoffs=[0.05+0.05*i for i in range(20)]):\n",
    "    mat_to_return = [[0]*len(vote_cutoffs) for i in range(len(vote_fracs_pos_queries[0]))]\n",
    "    print 'mat size: '+repr(len(mat_to_return))+' x '+repr(len(mat_to_return[0]))\n",
    "    y_real=[1]*len(vote_fracs_pos_queries)+[0]*len(vote_fracs_neg_queries)\n",
    "    for k_ind in range(len(vote_fracs_pos_queries[0])):\n",
    "        for cutoff_ind,cutoff in enumerate(vote_cutoffs):\n",
    "            y_pred=[]\n",
    "            for row in vote_fracs_pos_queries:\n",
    "                if row[k_ind]>cutoff:\n",
    "                    y_pred.append(1)\n",
    "                else:\n",
    "                    y_pred.append(0)\n",
    "            for row in vote_fracs_neg_queries:\n",
    "                if row[k_ind]>cutoff:\n",
    "                    y_pred.append(1)\n",
    "                else:\n",
    "                    y_pred.append(0)\n",
    "            mat_to_return[k_ind][cutoff_ind]=matthews_corrcoef(y_real,y_pred)\n",
    "    return mat_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = ecoli_validate_no_c.reset_index(drop=True).sequence\n",
    "vals = ecoli_validate_no_c.reset_index(drop=True).value\n",
    "nn_seq_db = ecoli_train_train_no_c.reset_index(drop=True).sequence\n",
    "# output_pos_seqs, output_neg_seqs, output_pos_yvals, output_neg_yvals = random_testing_set(seqs,vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = ecoli_train_train_no_c.reset_index(drop=True).sequence\n",
    "train_vals = ecoli_train_train_no_c.reset_index(drop=True).value\n",
    "# train_pos_seqs, train_neg_seqs, train_pos_yvals, train_neg_yvals = random_testing_set(train_seqs,train_vals,keep_maxlength_seqs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_seqs = ecoli_train_no_c.reset_index(drop=True).sequence\n",
    "# all_train_vals = ecoli_train_no_c.reset_index(drop=True).value\n",
    "# train_pos_seqs, train_neg_seqs, train_pos_yvals, train_neg_yvals = random_testing_set(train_seqs,train_vals,keep_maxlength_seqs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_negs_file = open('Fasta_files/ocnn_tuning_negs.txt','w')\n",
    "train_negs_file.write(seqs_to_fasta(train_neg_seqs))\n",
    "train_negs_file.close()\n",
    "train_negs_file = open('Fasta_files/ocnn_validation_negs.txt','w')\n",
    "train_negs_file.write(seqs_to_fasta(output_neg_seqs))\n",
    "train_negs_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_distances_with_negatives_for_pos_seqs = knn_all_distances_with_negatives(output_pos_seqs,train_pos_seqs,train_neg_seqs,10)\n",
    "knn_distances_with_negatives_for_neg_seqs = knn_all_distances_with_negatives(output_neg_seqs,train_pos_seqs,train_neg_seqs,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_to_pos_for_pos=knn_distances_with_negatives_for_pos_seqs[0]\n",
    "knn_to_neg_for_pos=knn_distances_with_negatives_for_pos_seqs[1]\n",
    "knn_to_pos_for_neg=knn_distances_with_negatives_for_neg_seqs[0]\n",
    "knn_to_neg_for_neg=knn_distances_with_negatives_for_neg_seqs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_frac_matrix_pos_queries=vote_fractions_matrix(knn_to_pos_for_pos,knn_to_neg_for_pos,10)\n",
    "vote_frac_matrix_neg_queries=vote_fractions_matrix(knn_to_pos_for_neg,knn_to_neg_for_neg,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_mat = mcc_matrix(vote_frac_matrix_pos_queries,vote_frac_matrix_neg_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in mcc_mat:\n",
    "    print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dists = []\n",
    "for seq in output_pos_seqs:\n",
    "    pos_dists.append(nn_distance(seq,nn_seq_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dists = []\n",
    "for seq in output_neg_seqs:\n",
    "    neg_dists.append(nn_distance(seq,nn_seq_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open('Figures/ocnn_validation_distances.csv','w')\n",
    "output_file.write('Sequence,Is_AMP,Min_distance\\n')\n",
    "for i,seq in enumerate(output_pos_seqs):\n",
    "    output_file.write(seq+',1,'+repr(pos_dists[i])+'\\n')\n",
    "for i,seq in enumerate(output_neg_seqs):\n",
    "    output_file.write(seq+',0,'+repr(neg_dists[i])+'\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MCC as a function of just nearest neighbor distances (a simper version, basically a KNN approach as opposed to a JKNN approach)\n",
    "distance_cutoffs = [i+0.5 for i in range(30)]\n",
    "actual_y = [1]*len(pos_dists)+[0]*len(neg_dists)\n",
    "for cutoff in distance_cutoffs:\n",
    "    pred_y = []\n",
    "    for dist in pos_dists:\n",
    "        dist=float(dist)\n",
    "        if dist>cutoff:\n",
    "            pred_y.append(0)\n",
    "        else:\n",
    "            pred_y.append(1)\n",
    "    for dist in neg_dists:\n",
    "        dist=float(dist)\n",
    "        if dist>cutoff:\n",
    "            pred_y.append(0)\n",
    "        else:\n",
    "            pred_y.append(1)\n",
    "    print repr(cutoff)+','+repr(matthews_corrcoef(actual_y,pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "pos_dists = []\n",
    "for seq in output_pos_seqs:\n",
    "    pos_dists.append(avg_knn_distance(seq,nn_seq_db,k))\n",
    "neg_dists = []\n",
    "for seq in output_neg_seqs:\n",
    "    neg_dists.append(avg_knn_distance(seq,nn_seq_db,k))\n",
    "output_file = open('Figures/ocnn_validation_distances_k_'+repr(k)+'.csv','w')\n",
    "output_file.write('Sequence,Is_AMP,Avg_distance\\n')\n",
    "for i,seq in enumerate(output_pos_seqs):\n",
    "    output_file.write(seq+',1,'+repr(pos_dists[i])+'\\n')\n",
    "for i,seq in enumerate(output_neg_seqs):\n",
    "    output_file.write(seq+',0,'+repr(neg_dists[i])+'\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MCC as a function of just nearest neighbor distances (a simper version, basically a KNN approach as opposed to a JKNN approach)\n",
    "distance_cutoffs = [i+0.5 for i in range(30)]\n",
    "actual_y = [1]*len(pos_dists)+[0]*len(neg_dists)\n",
    "for cutoff in distance_cutoffs:\n",
    "    pred_y = []\n",
    "    for dist in pos_dists:\n",
    "        dist=float(dist)\n",
    "        if dist>cutoff:\n",
    "            pred_y.append(0)\n",
    "        else:\n",
    "            pred_y.append(1)\n",
    "    for dist in neg_dists:\n",
    "        dist=float(dist)\n",
    "        if dist>cutoff:\n",
    "            pred_y.append(0)\n",
    "        else:\n",
    "            pred_y.append(1)\n",
    "    print repr(cutoff)+','+repr(matthews_corrcoef(actual_y,pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MCC as a function of distance ratio cutoffs, used for the general JKNN approach described in Khan & Ahmad\n",
    "# https://arxiv.org/pdf/1604.01686.pdf\n",
    "# Relationship between Variants of One-Class Nearest Neighbours and Creating their Accurate Ensembles\n",
    "# Note: Khan and Ahmad say \"In this section, we will show that varying\n",
    "# decision threshold (θ) with 11NN is similar to other OCNN\n",
    "# methods discussed in Section 3.\" So there's not too much point going beyond the 11NN approach we use here\n",
    "k=1\n",
    "j=1\n",
    "pos_ratios = []\n",
    "pos_dists = []\n",
    "pos_second_order_dists=[]\n",
    "for seq in output_pos_seqs:\n",
    "    first,second,ratio=kjnn_distance_ratio(seq,nn_seq_db,k,j)\n",
    "    pos_dists.append(first)\n",
    "    pos_second_order_dists.append(second)\n",
    "    pos_ratios.append(ratio)\n",
    "neg_dists = []\n",
    "neg_ratios=[]\n",
    "neg_second_order_dists=[]\n",
    "for seq in output_neg_seqs:\n",
    "    first,second,ratio=kjnn_distance_ratio(seq,nn_seq_db,k,j)\n",
    "    neg_dists.append(first)\n",
    "    neg_second_order_dists.append(second)\n",
    "    neg_ratios.append(ratio)\n",
    "output_file = open('Figures/ocnn_validation_distances_k_'+repr(k)+'_j_'+repr(j)+'.csv','w')\n",
    "output_file.write('Sequence,Is_AMP,First_order_distance,Second_order_distance,Ratio\\n')\n",
    "for i,seq in enumerate(output_pos_seqs):\n",
    "    output_file.write(seq+',1,'+repr(pos_dists[i])+','+repr(pos_second_order_dists[i])+','+repr(pos_ratios[i])+'\\n')\n",
    "for i,seq in enumerate(output_neg_seqs):\n",
    "    output_file.write(seq+',0,'+repr(neg_dists[i])+','+repr(neg_second_order_dists[i])+','+repr(neg_ratios[i])+'\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MCC as a function of distance ratio cutoffs, used for the general JKNN approach described in Khan & Ahmad\n",
    "ratio_cutoffs = [float(i)*0.1 for i in range(50)]\n",
    "actual_y = [1]*len(pos_ratios)+[0]*len(neg_ratios)\n",
    "for cutoff in ratio_cutoffs:\n",
    "    pred_y = []\n",
    "    for ratio in pos_ratios:\n",
    "        if ratio>cutoff:\n",
    "            pred_y.append(0)\n",
    "        else:\n",
    "            pred_y.append(1)\n",
    "    for ratio in neg_ratios:\n",
    "        if ratio>cutoff:\n",
    "            pred_y.append(0)\n",
    "        else:\n",
    "            pred_y.append(1)\n",
    "    print repr(cutoff)+','+repr(matthews_corrcoef(actual_y,pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now running kNN, not OCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_fnames_to_test = ['test_fasta_with_negatives_vs_random_p7.txt']\n",
    "fasta_fnames_to_test.append('test_fasta_with_negatives_vs_random_p9.txt')\n",
    "fasta_fnames_to_test.append('test_fasta_with_negatives_vs_random.txt')\n",
    "fasta_fnames_to_test.append('test_fasta_with_negatives_vs_uniprot_p7.txt')\n",
    "fasta_fnames_to_test.append('test_fasta_with_negatives_vs_uniprot_p9.txt')\n",
    "fasta_fnames_to_test.append('test_fasta_with_negatives_vs_uniprot.txt')\n",
    "output_fnames = ['knn_random_p7_7nn_p8cutoff.csv']\n",
    "output_fnames.append('knn_random_p9_7nn_p8cutoff.csv')\n",
    "output_fnames.append('knn_random_all_7nn_p8cutoff.csv')\n",
    "output_fnames.append('knn_uniprot_p7_7nn_p8cutoff.csv')\n",
    "output_fnames.append('knn_uniprot_p9_7nn_p8cutoff.csv')\n",
    "output_fnames.append('knn_uniprot_all_7nn_p8cutoff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_db = ecoli_train_with_c.reset_index(drop=True).sequence\n",
    "train_vals=ecoli_train_with_c.reset_index(drop=True).value\n",
    "train_pos_seqs, train_neg_seqs, train_pos_yvals, train_neg_yvals = random_testing_set(train_seq_db,train_vals,keep_maxlength_seqs=True)\n",
    "train_negs_file = open('Fasta_files/ocnn_train_negs.txt','w')\n",
    "train_negs_file.write(seqs_to_fasta(train_neg_seqs))\n",
    "train_negs_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_frac_threshold=0.8\n",
    "k=7\n",
    "for i,name in enumerate(fasta_fnames_to_test):\n",
    "    seqs_to_test = fasta_to_seqs('Fasta_files/Good_fasta_files_with_negatives/'+name)\n",
    "    output_file = open('Figures/'+output_fnames[i],'w')\n",
    "    output_file.write('Sequence,Vote_fraction,Is_pred_AMP\\n')\n",
    "    for j,seq in enumerate(seqs_to_test):\n",
    "        vote_frac = vote_fraction(seq,train_pos_seqs,train_neg_seqs,k)\n",
    "        is_pred_amp = 0\n",
    "        if vote_frac>vote_frac_threshold:\n",
    "            is_pred_amp=1\n",
    "        output_file.write(seq+','+repr(vote_frac)+','+repr(is_pred_amp)+'\\n')\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=5.5\n",
    "for i,name in enumerate(fasta_fnames_to_test):\n",
    "    seqs_to_test = fasta_to_seqs('Fasta_files/Good_fasta_files_with_negatives/'+name)\n",
    "    output_file = open('Figures/'+output_fnames[i],'w')\n",
    "    output_file.write('Sequence,Min_distance,Is_pred_AMP\\n')\n",
    "    for seq in seqs_to_test:\n",
    "        dist = nn_distance(seq,train_seq_db)\n",
    "        is_pred_amp = 0\n",
    "        if dist<threshold:\n",
    "            is_pred_amp=1\n",
    "        output_file.write(seq+','+repr(dist)+','+repr(is_pred_amp)+'\\n')\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_names = ['Our model','7NN','AMP scanner v2','iAMPpred','CAMP SVM','CAMP RF','CAMP ANN','CAMP DA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_fname = 'Figures/Saved_predictions/Uniprot_combined_p9_predictions.xlsx'\n",
    "uniprot_wkbk = xlrd.open_workbook(uniprot_fname)\n",
    "uniprot_sheet = uniprot_wkbk.sheet_by_name('All_predictions')\n",
    "y_true=[]\n",
    "y_guesses = [[] for i in range(1,uniprot_sheet.ncols)]\n",
    "\n",
    "for row_ind in range(1,uniprot_sheet.nrows):\n",
    "    row = uniprot_sheet.row_values(row_ind)\n",
    "    y_true_log_mic = row[0]\n",
    "    if y_true_log_mic > 3.99:\n",
    "        y_true.append(0)\n",
    "    else:\n",
    "        y_true.append(1)\n",
    "    for col_ind in range(1,uniprot_sheet.ncols):\n",
    "        if col_ind==1:\n",
    "            y_guesses[col_ind-1].append(-row[col_ind])\n",
    "        else:\n",
    "            y_guesses[col_ind-1].append(row[col_ind])\n",
    "\n",
    "uniprot_rocs = [roc_curve(y_true,y_guesses[i]) for i in range(len(y_guesses))]\n",
    "uniprot_roc_aucs = [roc_auc_score(y_true,y_guesses[i]) for i in range(len(y_guesses))]\n",
    "print (uniprot_roc_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [algorithm_names[i]+', AUC = '+'%.3f'%uniprot_roc_aucs[i] for i in range(len(uniprot_roc_aucs))]\n",
    "colors = ['k','b','g','r','orange','yellow','gray','pink']\n",
    "plt.figure(figsize=[5.5,5])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "# plt.plot([0,1],[0,1],'--',color='gray')\n",
    "for i in range(len(uniprot_rocs)):\n",
    "    plt.plot(uniprot_rocs[i][0],uniprot_rocs[i][1],'-',color=colors[i],label=labels[i])\n",
    "for i in reversed(range(len(uniprot_rocs))):\n",
    "    plt.plot(uniprot_rocs[i][0],uniprot_rocs[i][1],'-',color=colors[i])\n",
    "plt.legend(loc=4,prop={'size':14})\n",
    "plt.savefig('Figures/Uniprot_p9_roc_curves.png',bbox_inches='tight',frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_fname = 'Figures/Saved_predictions/Uniprot_combined_predictions.xlsx'\n",
    "uniprot_wkbk = xlrd.open_workbook(uniprot_fname)\n",
    "uniprot_sheet = uniprot_wkbk.sheet_by_name('All_predictions')\n",
    "y_true=[]\n",
    "y_guesses = [[] for i in range(1,uniprot_sheet.ncols)]\n",
    "\n",
    "for row_ind in range(1,uniprot_sheet.nrows):\n",
    "    row = uniprot_sheet.row_values(row_ind)\n",
    "    y_true_log_mic = row[0]\n",
    "    if y_true_log_mic > 3.99:\n",
    "        y_true.append(0)\n",
    "    else:\n",
    "        y_true.append(1)\n",
    "    for col_ind in range(1,uniprot_sheet.ncols):\n",
    "        if col_ind==1:\n",
    "            y_guesses[col_ind-1].append(-row[col_ind])\n",
    "        else:\n",
    "            y_guesses[col_ind-1].append(row[col_ind])\n",
    "\n",
    "uniprot_rocs = [roc_curve(y_true,y_guesses[i]) for i in range(len(y_guesses))]\n",
    "uniprot_roc_aucs = [roc_auc_score(y_true,y_guesses[i]) for i in range(len(y_guesses))]\n",
    "print (uniprot_roc_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_fname = 'Figures/Saved_predictions/Random_combined_p9_predictions.xlsx'\n",
    "random_wkbk = xlrd.open_workbook(random_fname)\n",
    "random_sheet = random_wkbk.sheet_by_name('All_predictions')\n",
    "y_true=[]\n",
    "y_guesses = [[] for i in range(1,random_sheet.ncols)]\n",
    "\n",
    "for row_ind in range(1,random_sheet.nrows):\n",
    "    row = random_sheet.row_values(row_ind)\n",
    "    y_true_log_mic = row[0]\n",
    "    if y_true_log_mic > 3.99:\n",
    "        y_true.append(0)\n",
    "    else:\n",
    "        y_true.append(1)\n",
    "    for col_ind in range(1,random_sheet.ncols):\n",
    "        if col_ind==1:\n",
    "            y_guesses[col_ind-1].append(-row[col_ind])\n",
    "        else:\n",
    "            y_guesses[col_ind-1].append(row[col_ind])\n",
    "\n",
    "random_rocs = [roc_curve(y_true,y_guesses[i]) for i in range(len(y_guesses))]\n",
    "random_roc_aucs = [roc_auc_score(y_true,y_guesses[i]) for i in range(len(y_guesses))]\n",
    "print (random_roc_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [algorithm_names[i]+', AUC = '+'%.3f'%random_roc_aucs[i] for i in range(len(random_roc_aucs))]\n",
    "colors = ['k','b','g','r','orange','yellow','gray','pink']\n",
    "plt.figure(figsize=[5.5,5])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "# plt.plot([0,1],[0,1],'--',color='gray')\n",
    "for i in range(len(random_rocs)):\n",
    "    plt.plot(random_rocs[i][0],random_rocs[i][1],'-',color=colors[i],label=labels[i])\n",
    "for i in reversed(range(len(random_rocs))):\n",
    "    plt.plot(random_rocs[i][0],random_rocs[i][1],'-',color=colors[i])\n",
    "plt.legend(loc=4,prop={'size':14})\n",
    "plt.savefig('Figures/Random_p9_roc_curves.png',bbox_inches='tight',frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_fname = 'Figures/Saved_predictions/Random_combined_predictions.xlsx'\n",
    "random_wkbk = xlrd.open_workbook(random_fname)\n",
    "random_sheet = random_wkbk.sheet_by_name('All_predictions')\n",
    "y_true=[]\n",
    "y_guesses = [[] for i in range(1,random_sheet.ncols)]\n",
    "\n",
    "for row_ind in range(1,random_sheet.nrows):\n",
    "    row = random_sheet.row_values(row_ind)\n",
    "    y_true_log_mic = row[0]\n",
    "    if y_true_log_mic > 3.99:\n",
    "        y_true.append(0)\n",
    "    else:\n",
    "        y_true.append(1)\n",
    "    for col_ind in range(1,random_sheet.ncols):\n",
    "        if col_ind==1 or col_ind==2:\n",
    "            y_guesses[col_ind-1].append(-row[col_ind])\n",
    "        else:\n",
    "            y_guesses[col_ind-1].append(row[col_ind])\n",
    "\n",
    "random_rocs = [roc_curve(y_true,y_guesses[i]) for i in range(len(y_guesses))]\n",
    "random_roc_aucs = [roc_auc_score(y_true,y_guesses[i]) for i in range(len(y_guesses))]\n",
    "print (random_roc_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [algorithm_names[i]+', AUC = '+'%.3f'%random_roc_aucs[i] for i in range(len(random_roc_aucs))]\n",
    "colors = ['k','b','g','r','orange','yellow','gray']\n",
    "plt.figure(figsize=[5.5,5])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "# plt.plot([0,1],[0,1],'--',color='gray')\n",
    "for i in range(len(random_rocs)):\n",
    "    plt.plot(random_rocs[i][0],random_rocs[i][1],'-',color=colors[i],label=labels[i])\n",
    "for i in reversed(range(len(random_rocs))):\n",
    "    plt.plot(random_rocs[i][0],random_rocs[i][1],'-',color=colors[i])\n",
    "plt.legend(loc=4,prop={'size':14})\n",
    "plt.savefig('Figures/Random_p9_roc_curves.png',bbox_inches='tight',frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate fasta files with test sets for all data, 90% and 70% identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First generate training and test set fasta files, and assemble the training ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences=[sequence for sequence in ecoli_df.sequence]\n",
    "ENSEMBLE_SIZE=5\n",
    "NEG_RATIOS = [1,3,10]\n",
    "with_C_weights = []\n",
    "with_C_architectures = []\n",
    "no_C_weights = []\n",
    "no_C_architectures = []\n",
    "for k in range(len(NEG_RATIOS)):\n",
    "    with_C_weights.append(['train_ensemble_weights_withC_'+repr(NEG_RATIOS[k])+'xneg_'+repr(i)+'.h5' for i in range(ENSEMBLE_SIZE)])\n",
    "    with_C_architectures.append(['train_ensemble_architecture_withC_'+repr(NEG_RATIOS[k])+'xneg_'+repr(i)+'.json' for i in range(ENSEMBLE_SIZE)])\n",
    "    no_C_weights.append(['train_ensemble_weights_noC_'+repr(NEG_RATIOS[k])+'xneg_'+repr(i)+'.h5' for i in range(ENSEMBLE_SIZE)])\n",
    "    no_C_architectures.append(['train_ensemble_architecture_noC_'+repr(NEG_RATIOS[k])+'xneg_'+repr(i)+'.json' for i in range(ENSEMBLE_SIZE)])\n",
    "\n",
    "    \n",
    "with_c_ensembles = [[] for i in range(len(NEG_RATIOS))]\n",
    "no_c_ensembles = [[] for i in range(len(NEG_RATIOS))]\n",
    "for k in range(len(NEG_RATIOS)):\n",
    "    for i in range(len(with_C_weights[0])):\n",
    "        with open('Saved_models/'+with_C_architectures[k][i],'r') as f:\n",
    "            reconst_model = model_from_json(f.read())\n",
    "        reconst_model.load_weights('Saved_models/'+with_C_weights[k][i])\n",
    "        reconst_model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "        with_c_ensembles[k].append(reconst_model)    \n",
    "\n",
    "        with open('Saved_models/'+no_C_architectures[k][i],'r') as f:\n",
    "            reconst_model = model_from_json(f.read())\n",
    "        reconst_model.load_weights('Saved_models/'+no_C_weights[k][i])\n",
    "        reconst_model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "        no_c_ensembles[k].append(reconst_model)\n",
    "        \n",
    "# everything_ensemble_model = EnsembleModel(no_c_ensembles[0]+with_c_ensembles[0]+no_c_ensembles[1]+with_c_ensembles[1]+no_c_ensembles[2]+with_c_ensembles[2],'average')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_c_ensemble_models = [EnsembleModel(no_c_ensembles[k],'classify_then_average') for k in range(len(NEG_RATIOS))]\n",
    "with_c_ensemble_models = [EnsembleModel(with_c_ensembles[k],'classify_then_average') for k in range(len(NEG_RATIOS))]\n",
    "no_c_ensemble_model_avgs = [EnsembleModel(no_c_ensembles[k],'average') for k in range(len(NEG_RATIOS))]\n",
    "with_c_ensemble_model_avgs = [EnsembleModel(with_c_ensembles[k],'average') for k in range(len(NEG_RATIOS))]\n",
    "all_ensemble_models = [EnsembleModel(no_c_ensembles[k]+with_c_ensembles[k],'classify_then_average') for k in range(len(NEG_RATIOS))]\n",
    "all_ensemble_model_avgs = [EnsembleModel(no_c_ensembles[k]+with_c_ensembles[k],'average') for k in range(len(NEG_RATIOS))]\n",
    "\n",
    "everything_ensemble_model = EnsembleModel(no_c_ensembles[0]+with_c_ensembles[0]+no_c_ensembles[1]+with_c_ensembles[1]+no_c_ensembles[2]+with_c_ensembles[2],'classify_then_average')  \n",
    "everything_ensemble_model_avg = EnsembleModel(no_c_ensembles[0]+with_c_ensembles[0]+no_c_ensembles[1]+with_c_ensembles[1]+no_c_ensembles[2]+with_c_ensembles[2],'average')\n",
    "\n",
    "ensemble_model_list = no_c_ensemble_models+with_c_ensemble_models+[everything_ensemble_model]\n",
    "ensemble_model_list_avgs = no_c_ensemble_model_avgs + with_c_ensemble_model_avgs + [everything_ensemble_model_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fasta = seqs_to_fasta(ecoli_train_with_c['sequence'])\n",
    "test_fasta = seqs_to_fasta(ecoli_test['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open('Fasta_files/train_fasta.txt','w')\n",
    "train_file.write(train_fasta)\n",
    "train_file.close()\n",
    "test_file = open('Fasta_files/test_fasta.txt','w')\n",
    "test_file.write(test_fasta)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now remove sequences at http://weizhong-lab.ucsd.edu/cdhit-web-server/cgi-bin/index.cgi?cmd=cd-hit-2d\n",
    "\n",
    "### Sequence identity cutoffs to run with: 0.9 and 0.7\n",
    "\n",
    "### In all other settings: just use the defaults\n",
    "\n",
    "### Then make a file containing the sequences along with the predictions from the various ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_p9_cutoff = fasta_to_seqs('Fasta_files/p9_identity.txt')\n",
    "seqs_p7_cutoff = fasta_to_seqs('Fasta_files/p7_identity.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i,seq in enumerate(ecoli_test['sequence']):\n",
    "    if len(seq)<MAX_SEQUENCE_LENGTH:\n",
    "        indices.append(i)\n",
    "ecoli_test_not_too_long = ecoli_test.iloc[indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i,seq in enumerate(ecoli_test['sequence']):\n",
    "    if seq in seqs_p9_cutoff and len(seq)<MAX_SEQUENCE_LENGTH:\n",
    "        indices.append(i)\n",
    "ecoli_test_p9 = ecoli_test.iloc[indices]\n",
    "\n",
    "indices = []\n",
    "for i,seq in enumerate(ecoli_test['sequence']):\n",
    "    if seq in seqs_p7_cutoff and len(seq)<MAX_SEQUENCE_LENGTH:\n",
    "        indices.append(i)\n",
    "ecoli_test_p7 = ecoli_test.iloc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = ecoli_df.sequence\n",
    "lengths = [min(MAX_SEQUENCE_LENGTH,len(seq)) for seq in seqs]\n",
    "SEQ_LENGTHS = lengths\n",
    "ecoli_test_input = df_to_input_vec(ecoli_test)\n",
    "ecoli_test_input_with_negatives = add_random_negative_examples(ecoli_test_input[0],ecoli_test_input[1],ecoli_test_input[2],ratio=1,max_mic=MAX_MIC,include_cysteine=False)\n",
    "ecoli_test_input_p9 = df_to_input_vec(ecoli_test_p9)\n",
    "ecoli_test_input_with_negatives_p9 = add_random_negative_examples(ecoli_test_input_p9[0],ecoli_test_input_p9[1],ecoli_test_input_p9[2],ratio=1,max_mic=MAX_MIC,include_cysteine=False)\n",
    "ecoli_test_input_p7 = df_to_input_vec(ecoli_test_p7)\n",
    "ecoli_test_input_with_negatives_p7 = add_random_negative_examples(ecoli_test_input_p7[0],ecoli_test_input_p7[1],ecoli_test_input_p7[2],ratio=1,max_mic=MAX_MIC,include_cysteine=False)\n",
    "\n",
    "\n",
    "test_x_with_negatives = ecoli_test_input_with_negatives[0]\n",
    "test_y_with_negatives = ecoli_test_input_with_negatives[1]\n",
    "test_x_p9_with_negatives = ecoli_test_input_with_negatives_p9[0]\n",
    "test_y_p9_with_negatives = ecoli_test_input_with_negatives_p9[1]\n",
    "test_x_p7_with_negatives = ecoli_test_input_with_negatives_p7[0]\n",
    "test_y_p7_with_negatives = ecoli_test_input_with_negatives_p7[1]\n",
    "\n",
    "ecoli_test_input = df_to_input_vec(ecoli_test_not_too_long)\n",
    "test_x = ecoli_test_input[0]\n",
    "test_y = ecoli_test_input[1]\n",
    "test_x_p9 = ecoli_test_input_p9[0]\n",
    "test_y_p9 = ecoli_test_input_p9[1]\n",
    "test_x_p7 = ecoli_test_input_p7[0]\n",
    "test_y_p7 = ecoli_test_input_p7[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds=[]\n",
    "for emodel in ensemble_model_list:\n",
    "    all_preds.append(emodel.predict(test_x_with_negatives))\n",
    "output_file = open('Figures/predictions.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(test_x_with_negatives):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(test_y_with_negatives[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "seqs_for_fasta = []\n",
    "for vect in test_x_with_negatives:\n",
    "    seq = vector_to_amp(vect)['sequence']\n",
    "    if seq.find('_')>-1:\n",
    "        seq = seq[0:seq.find('_')]\n",
    "    seqs_for_fasta.append(seq)\n",
    "for_fasta = seqs_to_fasta(seqs_for_fasta)\n",
    "test_file = open('Fasta_files/test_fasta_with_negatives.txt','w')\n",
    "test_file.write(for_fasta)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_with_negatives_no_amidation = wipe_out_amidation(test_x_with_negatives)\n",
    "all_preds=[]\n",
    "for emodel in ensemble_model_list:\n",
    "    all_preds.append(emodel.predict(test_x_with_negatives_no_amidation))\n",
    "output_file = open('Figures/predictions_no_amidation.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(test_x_with_negatives_no_amidation):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(test_y_with_negatives[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds=[]\n",
    "for emodel in ensemble_model_list:\n",
    "    all_preds.append(emodel.predict(test_x_p9_with_negatives))\n",
    "output_file = open('Figures/predictions_p9.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(test_x_p9_with_negatives):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(test_y_p9_with_negatives[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "seqs_for_fasta = []\n",
    "for vect in test_x_p9_with_negatives:\n",
    "    seq = vector_to_amp(vect)['sequence']\n",
    "    if seq.find('_')>-1:\n",
    "        seq = seq[0:seq.find('_')]\n",
    "    seqs_for_fasta.append(seq)\n",
    "for_fasta = seqs_to_fasta(seqs_for_fasta)\n",
    "test_file = open('Fasta_files/test_fasta_with_negatives_p9.txt','w')\n",
    "test_file.write(for_fasta)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds=[]\n",
    "for emodel in ensemble_model_list:\n",
    "    all_preds.append(emodel.predict(test_x_p7_with_negatives))\n",
    "output_file = open('Figures/predictions_p7.csv','w')\n",
    "output_file.write('Sequence,Amidation,Actual,No_C_1,No_C_3,No_C_10,With_C_1,With_C_3,With_C_10,All\\n')\n",
    "for i,testx in enumerate(test_x_p7_with_negatives):\n",
    "    amp = vector_to_amp(testx)\n",
    "    output_file.write(repr(amp['sequence'])+','+repr(amp['cterminal_amidation'])+','+repr(test_y_p7_with_negatives[i])+',')\n",
    "    for k in range(len(ensemble_model_list)):\n",
    "#         print all_preds\n",
    "        output_file.write(repr(all_preds[k][i])+',')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "seqs_for_fasta = []\n",
    "for vect in test_x_p7_with_negatives:\n",
    "    seq = vector_to_amp(vect)['sequence']\n",
    "    if seq.find('_')>-1:\n",
    "        seq = seq[0:seq.find('_')]\n",
    "    seqs_for_fasta.append(seq)\n",
    "for_fasta = seqs_to_fasta(seqs_for_fasta)\n",
    "test_file = open('Fasta_files/test_fasta_with_negatives_p7.txt','w')\n",
    "test_file.write(for_fasta)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted = everything_ensemble_model_avg.predict(test_x)\n",
    "p7_predicted = everything_ensemble_model_avg.predict(test_x_p7)\n",
    "p9_predicted = everything_ensemble_model_avg.predict(test_x_p9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predicted,test_y):\n",
    "    tau,pval = kendalltau(predicted,test_y)\n",
    "    mse = 'MSE,'+repr(float(np.average([(predicted[i]-test_y[i])**2 for i in range(len(test_y))])))\n",
    "    pearson = 'Pearson,'+repr(float(np.corrcoef(predicted,test_y)[1,0]))\n",
    "    tau = 'Kendall tau,'+repr(tau)\n",
    "    print mse+','+pearson+','+tau+','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(all_predicted,test_y)\n",
    "evaluate_predictions(p9_predicted,test_y_p9)\n",
    "evaluate_predictions(p7_predicted,test_y_p7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.corrcoef(all_predicted,test_y)\n",
    "print np.corrcoef(p9_predicted,test_y_p9)\n",
    "print np.corrcoef(p7_predicted,test_y_p7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.set(font_scale=1,style='white',font='Arial')\n",
    "font_to_use = {'fontname':FONT_TO_USE}\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "plt.plot(all_predicted,test_y,'.',color='k')\n",
    "plt.plot([-2,4.2],[-2,4.2],'k-')\n",
    "plt.ylim(-1.5,4.2)\n",
    "plt.xlim(-1.5,4.2)\n",
    "plt.xlabel('Predicted log MIC',**font_to_use)\n",
    "plt.ylabel('Actual log MIC',**font_to_use)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_fontname(FONT_TO_USE)\n",
    "for tick in ax.get_yticklabels():\n",
    "    tick.set_fontname(FONT_TO_USE)\n",
    "plt.tick_params(top='off',right='off')\n",
    "plt.savefig('Figures/pred_vs_actual_all.png',bbox_inches='tight',frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_to_use = {'fontname':FONT_TO_USE}\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "plt.plot(p7_predicted,test_y_p7,'k.')\n",
    "plt.plot([-2,4.2],[-2,4.2],'k-')\n",
    "plt.xlabel('Predicted log MIC',**font_to_use)\n",
    "plt.ylabel('Actual log MIC',**font_to_use)\n",
    "plt.ylim(-1.5,4.2)\n",
    "plt.xlim(-1.5,4.2)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_fontname(FONT_TO_USE)\n",
    "for tick in ax.get_yticklabels():\n",
    "    tick.set_fontname(FONT_TO_USE)\n",
    "plt.tick_params(top='off',right='off')\n",
    "ax.set_aspect('equal')\n",
    "plt.savefig('Figures/pred_vs_actual_p7.png',bbox_inches='tight',frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p7_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_to_use = {'fontname':FONT_TO_USE}\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "plt.plot(p9_predicted,test_y_p9,'k.')\n",
    "plt.plot([-2,4.2],[-2,4.2],'k-')\n",
    "plt.xlabel('Predicted log MIC',**font_to_use)\n",
    "plt.ylabel('Actual log MIC',**font_to_use)\n",
    "plt.ylim(-1.5,4.2)\n",
    "plt.xlim(-1.5,4.2)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_fontname(FONT_TO_USE)\n",
    "for tick in ax.get_yticklabels():\n",
    "    tick.set_fontname(FONT_TO_USE)\n",
    "plt.tick_params(top='off',right='off')\n",
    "ax.set_aspect('equal')\n",
    "plt.savefig('Figures/pred_vs_actual_p9.png',bbox_inches='tight',frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
